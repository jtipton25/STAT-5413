# Day 20

```{r, message = FALSE}
library(tidyverse)
library(STRbook)
library(fields)
library(mvnfast)
library(fda)
library(spBayes)
library(coda)
library(mgcv)
library(glmnet)
library(microbenchmark)
```

## Announcements


## spBayes example

Fitting a spatial model using the `NOAA_df_1990` data.frame from the `STRbook` package. We will focus on the average max temperature for July 1990.



```{r}
data("NOAA_df_1990")
## add a factor variable for left_join
NOAA_df_1990$id_factor <- factor(NOAA_df_1990$id)
dat <- NOAA_df_1990 %>%
    subset(year == 1990 & month == 7 & proc == "Tmax") %>%
    group_by(id_factor) %>% 
    summarize(mean_Tmax = mean(z)) #

## add back in the lat/lon variables
dat <- NOAA_df_1990 %>%
    subset(year == 1990 & month == 7 & proc == "Tmax" & day == 1) %>%
    left_join(dat, by = "id_factor") 
```


```{r, cache = TRUE}
n.samples <- 5000

starting <- list(phi = 3/100, sigma.sq = 2, tau.sq = 2)
tuning <- list(phi = 2.5, sigma.sq = 0.1, tau.sq = 0.1)
p <- 3 ## use lat and lon as predictors (plus an intercept)
coords <- as.matrix(cbind(dat$lon, dat$lat))
max_dist <- max(rdist(coords))
priors <- list(
    beta.Norm   = list(rep(0, p), diag(1e+06, p)),
    phi.Unif    = c(0.01, 100),
    sigma.sq.IG = c(2, 2), 
    tau.sq.IG   = c(2, 2)
)
cov.model <- "exponential"

fit_e <- spLM(
    dat$z ~ dat$lon + dat$lat,
    coords    = coords,
    starting  = starting,
    tuning    = tuning, 
    priors    = priors, 
    cov.model = cov.model, 
    n.samples = n.samples,
    n.report  = 1000
)
```

The above fits the model. However, sometimes it is hard to "tune" the model to get the desired Metropolis acceptance rate. `spLM` has an option for adaptive MCMC tuning. To enable this option, we add an `amcmc` option. We are also going to change to a Matern covarince function in the next code as well.

```{r, cache = TRUE}
n.samples    <- 5000
batch.length <- 50
n.batch      <- n.samples / batch.length

## must add in values for the smoothness paramter nu
starting <- list(phi = 3/100, sigma.sq = 2, tau.sq = 2, nu = 1)
tuning <- list(phi = 2.5, sigma.sq = 0.1, tau.sq = 0.1, nu = 0.1)
p <- 3 ## use lat and lon as predictors (plus an intercept)
coords <- as.matrix(cbind(dat$lon, dat$lat))
max_dist <- max(rdist(coords))
priors <- list(
    beta.Norm   = list(rep(0, p), diag(1e+06, p)),
    phi.Unif    = c(0.01, 100),
    nu.Unif     = c(0.01, 100),
    sigma.sq.IG = c(2, 2), 
    tau.sq.IG   = c(2, 2)
)
cov.model <- "matern"

fit_m <- spLM(
    dat$z ~ dat$lon + dat$lat,
    coords    = coords,
    starting  = starting,
    tuning    = tuning, 
    priors    = priors, 
    cov.model = cov.model, 
    n.samples = n.samples,
    amcmc     = list(
        n.batch = n.batch, 
        batch.length = batch.length,
        accept.rate = 0.44
    ),
    n.report  = 10 ## report every 10 adaptive batches
)
## can update the initial tuning parameters given the adapted values
```

```{r, cache = TRUE}
tuning <- list(phi = 0.5, sigma.sq = 0.38, tau.sq = 0.6, nu = 0.3)
fit_m <- spLM(
    dat$z ~ dat$lon + dat$lat,
    coords    = coords,
    starting  = starting,
    tuning    = tuning, 
    priors    = priors, 
    cov.model = cov.model, 
    n.samples = n.samples,
    amcmc     = list(
        n.batch = n.batch, 
        batch.length = batch.length,
        accept.rate = 0.44
    ),
    n.report  = 10 ## report every 10 adaptive batches
)
## can update the initial tuning parameters given the adapted values
```


To get the fitted parameters, we can recover these with composition sampling.

```{r, cache = TRUE}
## discard the first half of the samples as burn-in
burn.in <- 0.5 * n.samples
## trace plots for the exponential model
fit_e <- spRecover(fit_e, start = burn.in, thin = 2, verbose = FALSE)
theta.samps <- mcmc.list(fit_e$p.theta.samples, fit_e$p.theta.samples)
plot(theta.samps, density = FALSE)
beta.samps <- mcmc.list(fit_e$p.beta.recover.samples, fit_e$p.beta.recover.samples)  
plot(beta.samps, density = FALSE)
```

```{r, cache = TRUE}
## trace plots for the matern model
fit_m <- spRecover(fit_m, start = burn.in, thin = 2, verbose = FALSE)
theta.samps <- mcmc.list(fit_m$p.theta.samples, fit_m$p.theta.samples)
plot(theta.samps, density = FALSE)
beta.samps <- mcmc.list(fit_m$p.beta.recover.samples, fit_m$p.beta.recover.samples)  
plot(beta.samps, density = FALSE)
round(summary(theta.samps)$quantiles, 3)
```

```{r}
## recover the spatial random effects
fit_e_w <- fit_e$p.w.recover.samples
fit_m_w <- fit_m$p.w.recover.samples

## recover the spatial random effects (eta's in our class notation)
w.samps <- cbind(fit_e_w, fit_m_w)
w.summary <- apply(w.samps, 1, function(x) {
    quantile(x, prob = c(0.025, 0.5, 0.975))
})
```

Next we can genereate predictions for the exponential model

```{r, cache = TRUE}
states <- map_data("state")

pred_coords <- as.data.frame(
    expand.grid(
        seq(min(dat$lon), max(dat$lon), length = 100), 
        seq(min(dat$lat), max(dat$lat), length = 100)
    )
)
colnames(pred_coords) <- c("lon", "lat")

## predict using the tuned Matern fit
preds <- spPredict(
    fit_m, 
    start = burn.in, 
    thin = 10, 
    pred.coords = cbind(pred_coords$lon, pred_coords$lat), 
    pred.covars = cbind(1, pred_coords$lon, pred_coords$lat)
)

## Calculate the prediction means and variances
pred_coords$pred_mean <- apply(preds$p.y.predictive.samples, 1, mean)
pred_coords$pred_var <- apply(preds$p.y.predictive.samples, 1, var)

## calculate 95% credible intervals for the predictions
pred.summary <- apply(preds$p.y.predictive.samples, 1, function(x) {
    quantile(x, prob = c(0.025, 0.5, 0.975))
})

ggplot(data = pred_coords, aes(x = lon, y = lat, fill = pred_mean)) +
    geom_raster() +
    geom_polygon(data = states, aes(x = long, y = lat, group = group), 
                 colour = "black", fill = NA) +
    scale_fill_viridis_c(option = "plasma") +
  ggtitle("Predicted Average July max Temperature in 1990")  +
  coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3)


ggplot(data = pred_coords, aes(x = lon, y = lat, fill = pred_var)) +
    geom_raster() +
    geom_polygon(data = states, aes(x = long, y = lat, group = group), 
                 colour = "black", fill = NA) +
    scale_fill_viridis_c(option = "plasma") +
  ggtitle("Average July max Temperature prediction variance in 1990")  +
  coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3)
```

Note these variances are higher than the MLE estimates (potentially by a lot -- link to MLE fits day 11)


## Global vs. Local Basis function

* simulate some data

```{r}
set.seed(44)
n <- 1000
sigma <- 0.2
x <- seq(0, 1, length.out = n)
Sigma <- exp( - rdist(x)^2 / 0.02)

## Gaussian covariance with a small amount of "error"
mu <- c(rmvn(1, rep(0, n), Sigma + 1e-10 * diag(n))) 
y <- mu + rnorm(n, 0, sigma)
dat <- data.frame(
    x  = x,
    y  = y, 
    mu = mu
)
ggplot(data = dat, aes(x = x, y = mu)) +
    geom_line(color = "red") +
    geom_point(aes(x = x, y = y))
```


### An example global basis: Fourier Basis

```{r}
make_fourier_basis <- function(x, num_freq) {
    # if (!is.integer(num_freq)) {
    #     stop("num_freq must be an odd integer")
    # }
    # if (num_freq %% 2 == 0) {
    #     stop("num_freq must be an odd integer")
    # }
    
    X_fourier <- cbind(
        1, 
        do.call(
            cbind, 
            sapply(
                1:((num_freq - 1) / 2), function(i) { 
                    cbind(sin(2 * i * pi * x), cos(2 * i * pi * x))
                }, 
            simplify = FALSE
            )
        )
    )
    
return(X_fourier)
}

X_fourier <- make_fourier_basis(x, num_freq = 5)
dat <- data.frame(
    x         = x, 
    y         = y,
    mu        = mu,
    X_fourier = c(X_fourier),
    basis     = factor(rep(1:ncol(X_fourier), each = n))
)

ggplot(data = dat, aes(x = x, y = y)) +
    # geom_point() +
    geom_line(aes(x = x, y = X_fourier, group = basis, color = basis)) +
    scale_color_viridis_d(option = "magma") +
    theme_dark()
```


```{r}
## fit a model using the Fourier basis
fit <- lm(y ~ X_fourier - 1)
```

```{r}
## plot the basis with fitted coefficients
dat$fitted_basis <- c(
    sapply(1:ncol(X_fourier), function(i) X_fourier[, i] * fit$coefficients[i])
)

ggplot(data = dat, aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = fitted_basis, color = basis, group = basis)) +
    ggtitle("Weighted Fourier Basis functions")
```


```{r}
## calculate the fitted function -- the sum of the bases
dat$mu_hat <- apply(sapply(1:ncol(X_fourier), function(i) X_fourier[, i] * fit$coefficients[i]), 1, sum)

ggplot(data = dat, aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = mu_hat), color = "red", lwd  = 2) +
    geom_line(aes(x = x, y = mu), color = "blue", lwd = 2) +
    ggtitle("Fitted function vs. simulated function")
```

- The Fourier basis is global

```{r}
## only a small percentage of these values are zero
mean(X_fourier == 0)
```

## example local basis

- B-splines are piecewise polynomial functions

- degrees of freedom are the number of functions

    - More degrees of freedom -- more "wiggly" fit -- potential for overfitting
    
```{r}
## B-splines
X_bs <- bs(x, intercept = TRUE, df = 6)
dat <- data.frame(
    x     = x, 
    y     = y,
    mu    = mu,
    X_bs  = c(X_bs),
    basis = factor(rep(1:ncol(X_bs), each = n))
)

ggplot(data = dat, aes(x = x, y = y)) +
    # geom_point() +
    geom_line(aes(x = x, y = X_bs, group = basis, color = basis)) +
    scale_color_viridis_d(option = "magma") +
    theme_dark()
```


```{r}
## fit a model using the B-spline basis
fit <- lm(y ~ X_bs - 1)
```

```{r}
## plot the basis with fitted coefficients
dat$fitted_basis <- c(
    sapply(1:ncol(X_bs), function(i) X_bs[, i] * fit$coefficients[i])
)

ggplot(data = dat, aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = fitted_basis, color = basis, group = basis)) +
    ggtitle("Weighted B-spline Basis functions")
```


```{r}
## calculate the fitted function -- the sum of the bases
dat$mu_hat <- apply(sapply(1:ncol(X_bs), function(i) X_bs[, i] * fit$coefficients[i]), 1, sum)

ggplot(data = dat, aes(x = x, y = y)) +
    geom_point() +
    geom_line(aes(x = x, y = mu_hat), color = "red", lwd  = 2) +
    geom_line(aes(x = x, y = mu), color = "blue", lwd = 2) +
    ggtitle("Fitted function vs. simulated function")
```

- The B-spline basis is local

```{r}
## a much larger percentage of these values are zero -- can use sparse matrix
##     routines to solve these equations much faster
mean(X_bs == 0)
```


## Fitting spatial models many ways

## Empirical Orthogonal Functions

- Recall the Karhunen-Lo&egrave;ve expansion

- Analog for PCA for spatio-temporal data

- Use the sea surface temperature (SST) data in @wikle2019spatio

```{r}
data("SSTlandmask", package = "STRbook")
data("SSTlonlat", package = "STRbook") 
data("SSTdata", package = "STRbook")
```

Delete the values of SST that are over land (e.g. `SSTlandmask` is 1)

```{r}
delete_rows <- which(SSTlandmask == 1)
SSTdata <- SSTdata[-delete_rows, 1:396]
```

The eigen decomposition of the sample covariance of the data is equivalent to the singular value decomposition of the scaled and detrended data $\mathbf{y}$.

\begin{align*}
\tilde{\mathbf{y}} & \equiv \frac{1}{\sqrt{T - 1}} \left( \right)
\end{align*}


To get a spatial EOF, we need to transform the data into space-wide format

```{r}
y <- t(SSTdata)
dim(y)
```

To use the equation above, we need to calculate the spatial mean and the number of spatial replicates and scale and detrend the data

```{r}
spatial_mean <- apply(SSTdata, 1, mean)
nT <- ncol(SSTdata)
y_tilde <- 1 / sqrt(nT - 1) * (y - outer(rep(1, nT), spatial_mean))
```

We carry out the SVD on this scaled and detrended data

```{r}
svd_y <- svd(y_tilde)
```

which returns a list with three elements $\mathbf{U}$ $\mathbf{D}$ $\mathbf{V}$

\begin{align*}
\tilde{\mathbf{y}} & = \mathbf{U} \mathbf{D} \mathbf{V}
\end{align*}

```{r}
V <- svd_y$v
colnames(V) <- paste0("EOF", 1:ncol(SSTdata))
EOFs <- cbind(SSTlonlat[ - delete_rows, ], V)
glimpse(EOFs[, 1:6, ])
```

Plot the EOFs

```{r}
ggplot(data = EOFs, aes(x = lon, y = lat, fill = EOF1)) +
    geom_raster() +
    scale_fill_viridis_c(option = "magma") +
    coord_fixed(ratio = 1.3) +
    theme_bw() +
    xlab("Longitude (deg)") + 
    ylab("Latitude (deg)") +
    ggtitle("First EOF")
```

```{r}
ggplot(data = EOFs, aes(x = lon, y = lat, fill = EOF2)) +
    geom_raster() +
    scale_fill_viridis_c(option = "magma") +
    coord_fixed(ratio = 1.3) +
    theme_bw() +
    xlab("Longitude (deg)") + 
    ylab("Latitude (deg)") +
    ggtitle("Second EOF")
```

* Let's assume I have data for January 1970. 

```{r}
dat <- SST_df %>%
    subset(Year == 1970 & Month == "Jan")
EOFs$y <- dat$sst[ - delete_rows]
```

Model the data using the first few EOFs as basis functions

```{r}
mod <- lm(y ~ EOF1, data = EOFs)
summary(mod)
plot(EOFs$y, predict(mod), main = "Fitted vs. Predicted")
abline(0, 1, col = "red")
```


```{r}
mod <- lm(y ~ EOF1 + EOF2 + EOF3 + EOF4 + EOF5, data = EOFs)
summary(mod)
plot(EOFs$y, predict(mod), main = "Fitted vs. Predicted")
abline(0, 1, col = "red")
```

```{r}
mod <- lm(y ~ EOF1 + EOF2 + EOF3 + EOF4 + EOF5 + 
              EOF6 + EOF7 + EOF8 + EOF9 + EOF10, data = EOFs)
summary(mod)
plot(EOFs$y, predict(mod), main = "Fitted vs. Predicted")
abline(0, 1, col = "red")
```

```{r}
mod <- lm(y ~ EOF1 + EOF2 + EOF3 + EOF4 + EOF5 + 
              EOF6 + EOF7 + EOF8 + EOF9 + EOF10 +
              EOF11 + EOF12 + EOF13 + EOF14 + EOF15 + 
              EOF16 + EOF17 + EOF18 + EOF19 + EOF20, data = EOFs)
summary(mod)
plot(EOFs$y, predict(mod), main = "Fitted vs. Predicted")
abline(0, 1, col = "red")
```


- Disadvantages

    - Calculation of the SVD scales with $O(n^3)$
    - Each of the EOFs are global (almost always nonzero)
    
    
## Spatial modeling with `mgcv` package

Use the same temperature data

```{r}
data("NOAA_df_1990")
## add a factor variable for left_join
NOAA_df_1990$id_factor <- factor(NOAA_df_1990$id)
dat <- NOAA_df_1990 %>%
    subset(year == 1990 & month == 7 & proc == "Tmax") %>%
    group_by(id_factor) %>% 
    summarize(mean_Tmax = mean(z)) #

## add back in the lat/lon variables
dat <- NOAA_df_1990 %>%
    subset(year == 1990 & month == 7 & proc == "Tmax" & day == 1) %>%
    left_join(dat, by = "id_factor") 

fit <- gam(z ~ lon + lat + s(lon, lat, bs = "tp", k = 30), data = dat)
# tensor product of a 2-d thin plate regression spline and 1-d cr spline  
```


```{r}
## explore model residuals
par(mfrow = c(2, 2))
gam.check(fit)
```


```{r}
## generate model predictions
pred_coords <- as.data.frame(
    expand.grid(
        seq(min(dat$lon), max(dat$lon), length = 100), 
        seq(min(dat$lat), max(dat$lat), length = 100)
    )
)
colnames(pred_coords) <- c("lon", "lat")

preds <- predict(fit, newdata = pred_coords, se.fit = TRUE)
pred_coords$pred_mean <- preds$fit
pred_coords$pred_var <- preds$se.fit^2

ggplot(data = pred_coords, aes(x = lon, y = lat, fill = pred_mean)) +
    geom_raster() +
    geom_polygon(data = states, aes(x = long, y = lat, group = group), 
                 colour = "black", fill = NA) +
    scale_fill_viridis_c(option = "plasma") +
  ggtitle("Predicted Average July max Temperature in 1990")  +
  coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3)


ggplot(data = pred_coords, aes(x = lon, y = lat, fill = pred_var)) +
    geom_raster() +
    geom_polygon(data = states, aes(x = long, y = lat, group = group), 
                 colour = "black", fill = NA) +
    scale_fill_viridis_c(option = "plasma") +
  ggtitle("Average July max Temperature prediction variance in 1990")  +
  coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3)
```
    
    
## Spatial modeling with Kernels

- Kernel functions can be either local or global

    - Gaussian, exponential kernels are global
    - Truncated Gaussian and truncated exponential are local

- Kernel functions require specifying a kernel shape, a "bandwith", and the number of "knots".

Use the same temperature data with Gaussian Kernels

```{r}
data("NOAA_df_1990")
## add a factor variable for left_join
NOAA_df_1990$id_factor <- factor(NOAA_df_1990$id)
dat <- NOAA_df_1990 %>%
    subset(year == 1990 & month == 7 & proc == "Tmax") %>%
    group_by(id_factor) %>% 
    summarize(mean_Tmax = mean(z)) #

## add back in the lat/lon variables
dat <- NOAA_df_1990 %>%
    subset(year == 1990 & month == 7 & proc == "Tmax" & day == 1) %>%
    left_join(dat, by = "id_factor") 
```

```{r}
## For simplicity, assume this is a square number
n_knots <- 25
knots <- expand.grid(
    seq(min(dat$lon), max(dat$lon), length = sqrt(n_knots)), 
    seq(min(dat$lat), max(dat$lat), length = sqrt(n_knots))
)

make_kernel_basis <- function(coords, knots, kernel = "gaussian", bandwith = 1, threshold = NULL) {
    if (!(kernel %in% c("gaussian", "exponential"))) {
        stop("only kernels available are gaussian and exponential")
    }
    D <- fields::rdist(as.matrix(coords), as.matrix(knots))
    X <- matrix(0, nrow(D), ncol(D))
    if (kernel == "exponential") {
        X <- exp (- D / bandwith)
    } else if (kernel == "gaussian") {
        X <- exp (- D^2 / bandwith)
    }
    
    ## add in a minimum distance threshold
    if (!is.null(threshold)) {
        X[D > threshold] <- 0
    }

    return(X)
}
```

We can plot the kernels over the domain of interest using the code below. Note that it is useful to change the bandwith parameter so that the kernel covers multiple other knots

```{r, cache = TRUE}
coords <- cbind(dat$lon, dat$lat)
coords_plot <- expand.grid(
    seq(min(dat$lon), max(dat$lon), length = 100), 
    seq(min(dat$lat), max(dat$lat), length = 100)
)
colnames(coords_plot) <- c("lon", "lat")

X_plot <- make_kernel_basis(coords_plot, knots, kernel = "gaussian", bandwith = 20)

## plot the kernel basis
dat_plot <- data.frame(
    x     = coords_plot[, 1], 
    y     = coords_plot[, 2],
    X     = c(X_plot),
    basis = factor(rep(1:ncol(X_plot), each = nrow(coords_plot)))
)
dat_knots <- data.frame(
  x = knots[, 1],
  y = knots[, 2]
)

ggplot(data = dat_plot, aes(x = x, y = y, fill = X)) +
  geom_raster() +
  # geom(aes(x = x, y = X_fourier, group = basis, color = basis)) +
  scale_color_viridis_d(option = "magma") +
  theme_bw() +
  facet_wrap( ~ basis, ncol = sqrt(n_knots)) +
  ggtitle("Kernel Basis") +
  geom_point(data = dat_knots, aes(x = x, y = y), color = "red", size = 0.25, inherit.aes = FALSE) 
```

```{r}
## apply the kernel basis to the data
bw <- c(1, 10, 20, 30, 40, 50, 75, 100, 1000)
check_AICc <- matrix(0, 8, length(bw)) 
for (j in 1:8) {
  for (k in 1:length(bw)) {
    n_knots <- j^2
    knots <- expand.grid(
      seq(min(dat$lon), max(dat$lon), length = sqrt(n_knots)), 
      seq(min(dat$lat), max(dat$lat), length = sqrt(n_knots))
    )
    X <- make_kernel_basis(coords, knots, kernel = "gaussian", bandwith = bw[k])
    colnames(X) <- paste0("X", 1:ncol(X))
    X <- data.frame(X)
    fit <- lm(dat$z ~ ., data = X)
    check_AICc[j, k] <- AIC(fit) + (2 * (j^2 + 1)^2 + 2 * (j^2 + 1)) / (nrow(X) - j^2)
  }
}
matplot(check_AICc, type = 'p')
best_idx <- which(check_AICc == min(check_AICc), arr.ind = TRUE)
## the best fitting model has 5 knots and bandwidth 20
best_idx
```

```{r}
n_knots <- best_idx[1]^2
knots <- expand.grid(
    seq(min(dat$lon), max(dat$lon), length = sqrt(n_knots)), 
    seq(min(dat$lat), max(dat$lat), length = sqrt(n_knots))
)
X <- make_kernel_basis(coords, knots, kernel = "gaussian", bandwith = bw[best_idx[2]])
colnames(X) <- paste0("X", 1:ncol(X))
X <- data.frame(X)
fit <- lm(dat$z ~ ., data = X)
```

```{r}
## generate model predictions
pred_coords <- as.data.frame(
    expand.grid(
        seq(min(dat$lon), max(dat$lon), length = 100), 
        seq(min(dat$lat), max(dat$lat), length = 100)
    )
)
colnames(pred_coords) <- c("lon", "lat")
X_pred <- make_kernel_basis(pred_coords, knots, kernel = "gaussian", bandwith = bw[best_idx[2]])
colnames(X_pred) <- paste0("X", 1:ncol(X))
X_pred <- data.frame(X_pred)

preds <- predict(fit, newdata = X_pred, se.fit = TRUE)

dat_pred <- data.frame(
  lon       = pred_coords$lon,
  lat       = pred_coords$lat,
  ## prediction mean
  pred_mean = preds$fit,
  ## prediction variance
  pred_var  = preds$se.fit^2
)

ggplot(data = dat_pred, aes(x = lon, y = lat, fill = pred_mean)) +
    geom_raster() +
    geom_polygon(data = states, aes(x = long, y = lat, group = group), 
                 colour = "black", fill = NA) +
    scale_fill_viridis_c(option = "plasma") +
  ggtitle("Predicted Average July max Temperature in 1990")  +
  coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3)


ggplot(data = dat_pred, aes(x = lon, y = lat, fill = pred_var)) +
    geom_raster() +
    geom_polygon(data = states, aes(x = long, y = lat, group = group), 
                 colour = "black", fill = NA) +
    scale_fill_viridis_c(option = "plasma") +
  ggtitle("Average July max Temperature prediction variance in 1990")  +
  coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3)
```
    

Notice that the above model is overfit -- see the large variance surface. This can be resolved with penalized models (ridge, lasso, elastic net, etc). Many basis-function models show "edge effects" in the model fit. This is because at the edges of the domain there are one or two basis functions that see little data and therefore have high variability in the estimate. One can either add duplicate knots or extend the spatial domain of the knots and add a penalty term to the fit.

### Kernel regression with a ridge penalty

* Note: glmnet doesn't have a theoretically-motivated method for calculating prediction standard errors

```{r}
## set alpha = 0 for ridge regression
X <- make_kernel_basis(coords, knots, kernel = "gaussian", bandwith = bw[best_idx[2]])
colnames(X) <- paste0("X", 1:ncol(X))
X <- data.frame(X)

cv_fit <- cv.glmnet(as.matrix(X), dat$z, alpha = 0)
fit <- glmnet(as.matrix(X), dat$z, alpha = 0, lambda = cv_fit$lambda.min)

## generate model predictions
pred_coords <- as.data.frame(
    expand.grid(
        seq(min(dat$lon), max(dat$lon), length = 100), 
        seq(min(dat$lat), max(dat$lat), length = 100)
    )
)
colnames(pred_coords) <- c("lon", "lat")

X_pred <- make_kernel_basis(pred_coords, knots, kernel = "gaussian", bandwith = bw[best_idx[2]])
colnames(X_pred) <- paste0("X", 1:ncol(X))
X_pred <- data.frame(X_pred)

preds <- predict(fit, newx = as.matrix(X_pred))
```

```{r}
dat_pred <- data.frame(
  lon       = pred_coords$lon,
  lat       = pred_coords$lat,
  pred_mean = c(preds)
)
ggplot(data = dat_pred, aes(x = lon, y = lat, fill = pred_mean)) +
    geom_raster() +
    geom_polygon(data = states, aes(x = long, y = lat, group = group), 
                 colour = "black", fill = NA) +
    scale_fill_viridis_c(option = "plasma") +
  ggtitle("Predicted Average July max Temperature in 1990")  +
  coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3)
```

### Kernel regression with a lasso penalty

* Note: glmnet doesn't have a theoretically-motivated method for calculating prediction standard errors


```{r}
## set alpha = 1 for ridge regression
X <- make_kernel_basis(coords, knots, kernel = "gaussian", bandwith = bw[best_idx[2]])
colnames(X) <- paste0("X", 1:ncol(X))
X <- data.frame(X)

cv_fit <- cv.glmnet(as.matrix(X), dat$z, alpha = 1)
fit <- glmnet(as.matrix(X), dat$z, alpha = 1, lambda = cv_fit$lambda.min)

## generate model predictions
pred_coords <- as.data.frame(
    expand.grid(
        seq(min(dat$lon), max(dat$lon), length = 100), 
        seq(min(dat$lat), max(dat$lat), length = 100)
    )
)
colnames(pred_coords) <- c("lon", "lat")

X_pred <- make_kernel_basis(pred_coords, knots, kernel = "gaussian", bandwith = bw[best_idx[2]])
colnames(X_pred) <- paste0("X", 1:ncol(X))
X_pred <- data.frame(X_pred)

preds <- predict(fit, newx = as.matrix(X_pred))
```

```{r}
dat_pred <- data.frame(
  lon       = pred_coords$lon,
  lat       = pred_coords$lat,
  pred_mean = c(preds)
)
ggplot(data = dat_pred, aes(x = lon, y = lat, fill = pred_mean)) +
    geom_raster() +
    geom_polygon(data = states, aes(x = long, y = lat, group = group), 
                 colour = "black", fill = NA) +
    scale_fill_viridis_c(option = "plasma") +
  ggtitle("Predicted Average July max Temperature in 1990")  +
  coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3)
```

### Truncated Kernels - local support

The kernels constructed above are globally supported (dense)

```{r}
mean(X_pred == 0)
```

- Can use traditional kernels and truncate using a threshold

```{r}
X_threshold <- make_kernel_basis(pred_coords, knots, kernel = "gaussian",
                                 bandwith = bw[best_idx[2]], threshold = 10)
mean(X_threshold == 0)
```


Can use other functions as a kernel:
- Wendland basis

\begin{align*}
\frac{(1 - d)^6 (35 d^2 + 18d + 3)}{3} I\{d < 1\}
\end{align*}


```{r}
wendland_basis <- function(d, radius) {
    if (any(d < 0)) {
        stop("d must be nonnegative")
    }
    d <- d / radius
    return(((1 - d)^6 * (35 * d^2 + 18 * d + 3)) / 3 * (d < 1))
}

layout(matrix(1:4, 2, 2, byrow = TRUE))
curve(wendland_basis(abs(x - 0.25), radius = 0.15), n = 1000)
curve(wendland_basis(abs(x - 0.75), radius = 0.15), n = 1000)
curve(wendland_basis(abs(x - 0.25), radius = 0.5), n = 1000)
curve(wendland_basis(abs(x - 0.75), radius = 0.5), n = 1000)
```

## Sparse vs. Dense matrix computation

- uses the `spam64` package

- compute time of $\mathbf{X}' \mathbf{X}$ for dense vs. sparse matrices

```{r, cache = TRUE}
coords <- as.data.frame(
    expand.grid(
        seq(min(dat$lon), max(dat$lon), length = 50), 
        seq(min(dat$lat), max(dat$lat), length = 50)
    )
)
colnames(coords) <- c("lon", "lat")

n_knots <- 25^2
knots <- expand.grid(
    seq(min(dat$lon), max(dat$lon), length = sqrt(n_knots)), 
    seq(min(dat$lat), max(dat$lat), length = sqrt(n_knots))
)

X_dense <- make_kernel_basis(coords, knots, kernel = "gaussian",
                                 bandwith = 5)
X_sparse <- make_kernel_basis(coords, knots, kernel = "gaussian",
                                 bandwith = 5, threshold = 5)
mean(X_dense == 0)
mean(X_sparse == 0)
## convert to a sparse matrix format
X_sparse <- as.spam(X_sparse)

## calcuate t(X) %*% X for dense and sparse matrices
bm <- microbenchmark(
    crossprod(X_dense, X_dense),
    crossprod.spam(X_sparse, X_sparse),
    times = 10
)

bm
plot(bm)
```

- almost a 10-fold speedup in computation time by using sparse matrix operations







