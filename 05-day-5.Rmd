# Day 5

```{r, message = FALSE}
library(tidyverse)
library(fields)
library(mvnfast)
library(plotly)
library(splines)
set.seed(404)
```
## Spatial means and covariances

Let $\{ y(\mathbf{s}_i) \}$ be a set of observations of a process at locations $\{ \mathbf{s}_i \in \mathcal{D}, i = 1, \ldots, n \}$. 

* In one dimension, $y(\mathbf{s})$ is a curve

```{r}
n <- 100
s <- seq(0, 1, length = n)
## calculate the pairwise distance between locations
## rdist from the fields package is much faster than the dist function
D <- rdist(s, s)
Sigma <- exp( - D)
dat <- data.frame(
    s = s,
    y = c(rmvn(1, mu = rep(0, n), sigma = Sigma))
)

dat %>%
    ggplot(aes(x = s, y = y)) +
    geom_line() +
    ylab("y(s)")
```


* In two dimensions, $y(\mathbf{s})$ is a surface

```{r}
n <- 20^2
s <- expand.grid(
    seq(0, 1, length = sqrt(n)),
    seq(0, 1, length = sqrt(n))
)
    
## calculate the pairwise distance between locations
## rdist from the fields package is much faster than the dist function
D <- rdist(s, s)
Sigma <- exp( - D)
dat <- data.frame(
    s1 = s[, 1],
    s2 = s[, 2],
    y  = c(rmvn(1, mu = rep(0, n), sigma = Sigma))
)


plot_ly(
    z = ~matrix(dat$y, sqrt(n), sqrt(n))
) %>%
    add_surface()
```

### Gaussian processes
 
* A [Gaussian process](https://en.wikipedia.org/wiki/Gaussian_process) is an infinite-dimensional function (the function is defined for inginitely many locations $\mathbf{s} \in \mathcal{D}$) with the property that the finite-dimensional vector $\mathbf{y}(\mathbf{s}) = (y(\mathbf{s}_1), \ldots, y(\mathbf{s}_n) )'$ at any finite subset of locations $\mathbf{s}_1, \ldots, \mathbf{s}_n \in \mathcal{D}$ has a multivariate Gaussian distribution. (A good book is available free online here: [http://www.gaussianprocess.org/gpml/](http://www.gaussianprocess.org/gpml/))

#### Mean and covariance

* A univariate normal distribution is fully characterized by a mean $\mu$ and a variance $\sigma^2$.

* A multivariate normal distribution is fully characeterized by a mean vector $\boldsymbol{\mu}$ and a covariance matrix $\boldsymbol{\Sigma}$. 

    * The mean is an $n$-dimensional vector with 
    
    \begin{align*}
    E\left( y(\mathbf{s}) \right) = \boldsymbol{\mu}(\mathbf{s}) = \begin{pmatrix} \mu(\mathbf{s}_1) \\ \vdots \\ \mu(\mathbf{s}_n) \end{pmatrix}
    \end{align*}
    
    * The covariance matrix is an $n \times n$ matrix with

    \begin{align*}
    \operatorname{Cov} \left( y(\mathbf{s}) \right) & = \begin{bmatrix}
    \operatorname{Var} \left( y(\mathbf{s}_1) \right) & \operatorname{Cov} \left( y(\mathbf{s}_1), y(\mathbf{s}_2) \right) & \cdots & \operatorname{Cov} \left( y(\mathbf{s}_1), y(\mathbf{s}_n) \right) \\
    \operatorname{Cov} \left( y(\mathbf{s}_2), y(\mathbf{s}_1) \right) & \operatorname{Var} \left( y(\mathbf{s}_2) \right) & \cdots & \operatorname{Cov} \left( y(\mathbf{s}_2), y(\mathbf{s}_n) \right) \\
    \vdots & \vdots & \ddots & \vdots \\
    \operatorname{Cov} \left( y(\mathbf{s}_n), y(\mathbf{s}_1) \right) &    \operatorname{Cov} \left( y(\mathbf{s}_n), y(\mathbf{s}_2) \right) & \cdots & \operatorname{Var} \left( y(\mathbf{s}_n) \right)  \\
    \end{bmatrix}
    \end{align*}
    

* Recall that the multivariate normal pdf is 

\begin{align*}
[\mathbf{y} | \boldsymbol{\mu}, \boldsymbol{\Sigma}] & = (2 \pi)^{-\frac{n}{2}} |\boldsymbol{\Sigma}|^{-\frac{1}{2}} e^{-\frac{1}{2} \left( \mathbf{y} - \boldsymbol{\mu} \right)' \boldsymbol{\Sigma}^{-1} \left( \mathbf{y} - \boldsymbol{\mu} \right)}
\end{align*}

* Define the precision matrix $\boldsymbol{\Omega} = \boldsymbol{\Sigma}^{-1}$. Then, the multivariate normal pdf can be written as

\begin{align*}
[\mathbf{y} | \boldsymbol{\mu}, \boldsymbol{\Omega}] & = (2 \pi)^{-\frac{n}{2}} |\boldsymbol{\Omega}|^{\frac{1}{2}} e^{-\frac{1}{2} \left( \mathbf{y} - \boldsymbol{\mu} \right)' \boldsymbol{\Omega} \left( \mathbf{y} - \boldsymbol{\mu} \right)}
\end{align*}    
    

#### Mean and covariance **functions**

* A Gaussian process is fully characterized by a mean function $E\left( y(\mathbf{s}) \right) = \mu(\mathbf{s})$ that maps $\mathcal{R}^d \rightarrow \mathcal{R}^1$ (for a $d$-dimensional location $\mathbf{s}$ -- typically $d=2$) and a covariance function $\operatorname{Cov} \left( y(\mathbf{s}_i), y(\mathbf{s}_j) \right) = C(\mathbf{s}, \mathbf{s}')$.

    * This means that once you know the mean function $\mu(\mathbf{s})$ and the covariance function $C(\mathbf{s}, \mathbf{s}')$ you have full knowledge of the distribution
    
    * Note: this is different than a multivariate normal distribution as this is an infinite-dimensional function -- cannot be represented with a vector and/or matrix.
    
* Any finite realization of a GP has the pdf

\begin{align*}
[\mathbf{y} | \boldsymbol{\mu}, \boldsymbol{\Sigma}] & = (2 \pi)^{-\frac{n}{2}} |\boldsymbol{\Sigma}|^{-\frac{1}{2}} e^{-\frac{1}{2} \left( \mathbf{y} - \boldsymbol{\mu} \right)' \boldsymbol{\Sigma}^{-1} \left( \mathbf{y} - \boldsymbol{\mu} \right)}
\end{align*}

where $\boldsymbol{\mu}$ is determined by the function $\mu(\cdot)$ and
$\boldsymbol{\Sigma}$ is determined by the function $C(\cdot, \cdot)$.

#### The Gaussian process mean function

* There are many possible valid choices for the mean function $\mu(\mathbf{s})$ (almost any possible function is allowed).

    * Constant function: $\mu(\mathbf{s}) \equiv \beta_0$
    
    * Spatial covariates: $\mu(\mathbf{s}) \equiv \mathbf{X}(\mathbf{s}) \boldsymbol{\beta} = \beta_0 + \sum_{j=1}^p x_j(\mathbf{s}) \beta_j$
        
        * Examples: elevation, distance to water, latitude
        
    * Linear spatial trends: $\mu(\mathbf{s}_i) \equiv \beta_0 + \beta_1 s_{i1} + \beta_2 s_{i2}$
    
    * Higher-order spatial trends: $\mu(\mathbf{s}_i) \equiv \sum_{j=1}^p f_j(\mathbf{s}) \beta_j$ where $f_j(\mathbf{s})$ is some function of location $\mathbf{s}$ (i.e., B-splines, Fourier bases, wavelets, etc.)
    
* How to choose:

    * AIC / BIC / cross-validation 
    
#### Example    

```{r}
n <- 1000
X <- seq(0, 1, length = n)
X_bs <- bs(X, df = 10)
beta <- rnorm(ncol(X_bs))
y <- X * 2 + X_bs %*% beta + rnorm(n, 0, 0.25)

dat <- data.frame(X = X, y = y, mu = X * 2 + X_bs %*% beta)
dat %>%
    ggplot(aes(x = X, y = y)) +
    geom_point() +
    geom_line(aes(x = X, y = mu), color = "red")
```


* A simple mean structure can leave behind a strong residual covariance structure

```{r, out.width = "49%", fig.show = "hold"}
dat$simple <- predict(lm(y ~ X))
dat$simple_resids <- resid(lm(y ~ X))

dat %>%
    ggplot(aes(x = X, y = y)) +
    geom_point() +
    geom_line(aes(x = X, y = mu), color = "red", lwd = 2) +
    geom_line(aes(x = X, y = simple), color = "blue", lwd = 2) +
    ggtitle("Simple linear fit") + 
    theme(plot.title = element_text(size = 30))

dat %>%
    ggplot(aes(x = X, y = simple_resids)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", lwd = 2) +
    ggtitle("Simple linear fit residuals") + 
    theme(plot.title = element_text(size = 30))
```

* A complex mean structure can lead to independent residuals

```{r, out.width = "49%", fig.show = "hold"}
dat$complex <- predict(lm(y ~ X + X_bs))
dat$complex_resids <- resid(lm(y ~ X + X_bs))

dat %>%
    ggplot(aes(x = X, y = y)) +
    geom_point() +
    geom_line(aes(x = X, y = mu), color = "red", lwd = 2) +
    geom_line(aes(x = X, y = complex), color = "blue", lwd = 2) +
    ggtitle("Complex spline fit") + 
    theme(plot.title = element_text(size = 30))

dat %>%
    ggplot(aes(x = X, y = complex_resids)) +
    geom_point() +
    geom_hline(yintercept = 0, color = "red", lwd = 2) +
    ggtitle("Complex spline fit residuals") + 
    theme(plot.title = element_text(size = 30))
```

* How do you interpret this residual correlation?

    * Missing covariates that have a spatial pattern

        * For example, what if you are modeling temperature in a mountainous region and don't include elevation as a covariate?

    * Advection/diffusion processes 
        * Example: the wind blowing, the spread of disease
        
#### Gaussian Process Covariance Functions

* Unlike the mean functions, only specific covariance functions are valid.

* The covaraince function at a finite subset of $n$ points is called the covariance matrix.

* For a covariance matrix to be from a valid covariance function, the covariance matrix $\boldsymbol{\Sigma}$ must be **symmetric** and **positive-definite**.

    * A matrix $\boldsymbol{\Sigma}$ is symmetric if $\boldsymbol{\Sigma}' = \boldsymbol{\Sigma}$
    
    * A matrix $\boldsymbol{\Sigma}$ is positive definite iff and only if
    
    \begin{align*}
    \boldsymbol{\Sigma} \mbox{ is positive definite } &  \iff \\
    \mathbf{z}' \boldsymbol{\Sigma} \mathbf{z} \geq 0 \hspace{1em} \forall \mathbf{z} \in \mathcal{R}^n & \iff \\
    \mbox{all eigenvalues of } \boldsymbol{\Sigma} \mbox{ are strictly positive} & \iff \\
    |\boldsymbol{\Sigma}| > 0
    \end{align*}
    
* Therefore, the covariance fuction $C(\mathbf{s}_i, \mathbf{s}_j)$ is a valid covariance function if the $n \times n$ covariance matrix at any finite collection of $n$ locations $\mathbf{s}_1, \ldots, \mathbf{s}_n$ has the properties

    * **symmetry:** $C(\mathbf{s}_i, \mathbf{s}_j) = C(\mathbf{s}_j, \mathbf{s}_i) \hspace{1em} \forall \mathbf{s}_i, \mathbf{s}_j$ 
    
    * **positive definite:** $\sum_{i=1}^n \sum_{j=1}^n z_i z_j C(\mathbf{s}_i, \mathbf{s}_j) > 0 \hspace{1em} \forall n, \mathbf{s}_1, \ldots, \mathbf{s}_n, \mbox{ and } z_1, \ldots, z_n \in \mathcal{R}$ 

    * proving that these properties hold is **hard** -- Often rely on spectral methods (showing the eigenvalues of the function are all strictly positive).
    
### Gaussian process assumptions

* To fit a Gaussian process, we have to make assumptions

    * Why? -- The mean and covariance functions are infinite dimensional but we only observe a finite vector $\mathbf{y} = (y(\mathbf{s}_1), \ldots, y(\mathbf{s}_1))'$ so we cannot fully specify the model
   
* **Stationarity**
    
    * Strict stationarity: the probability density function is invariant to shifts
        * $[y(\mathbf{s}_1), \ldots, y(\mathbf{s}_n)] = [y(\mathbf{s}_1 + \mathbf{h}), \ldots, y(\mathbf{s}_n + \mathbf{h})] \hspace{1em} \forall \mathbf{s}_1, \ldots, \mathbf{s}_n \in \mathcal{D} \mbox{ and } \mathbf{h}$ (such that all points shifted by the vector $\mathbf{h}$ are also in $\mathcal{D}$)
    
    * Weak stationarity: mean and covariance function are stationary
        * $E\left( y(\mathbf{s}) \right) = E\left( y(\mathbf{s} + \mathbf{h}) \right) = \mu \hspace{1em} \forall \mathbf{h}$ 
        * $C\left( y(\mathbf{s}), y(\mathbf{s} + \mathbf{h}) \right) = C\left( y(\mathbf{0}), y(\mathbf{h}) \right) = C(\mathbf{h}) \hspace{1em} \forall \mathbf{s}, \mathbf{h}$ 
        * This implies $C\left( y(\mathbf{s}_i), y(\mathbf{s}_j) \right) = C(\mathbf{s}_i - \mathbf{s}_j) = C(\mathbf{d}_{ij})$ where $\mathbf{d}_{ij} = \mathbf{s}_i - \mathbf{s}_j$ is the distance between location $\mathbf{s}_i$ and location $\mathbf{s}_j$
        

**Insert drawing here**

<!-- ```{r} -->
<!-- knitr::include_graphics(here::here("images", "stationarity.jpeg")) -->
<!-- ``` -->

* Note: for Gaussian processes, a weak stationarity implies strong stationarity.         
    
* **Isotropy**
    * A covariance function is isotropic if it is invariant to direction and rotation.
    

For future classes (basis representations) [Visual exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/)


