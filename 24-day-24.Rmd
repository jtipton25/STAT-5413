\newcommand*\Laplace{\mathop{}\!\mathbin\bigtriangleup}

# Day 24 -- Scaling GPs for large data

```{r}
library(tidyverse)
library(mvnfast)
library(fields)
library(spBayes)
library(spNNGP)
library(LatticeKrig)
```

## Announcements

- review EOF example from previous lecture


- spatial data can often be large -- thousands to even millions of sites

- the likelihood has $n \times n$ covariance matrices which have to be evaluated -- impossible to do directly

- Prediction isn't too bad -- can use local prediction using $k$ nearest locations

## Local analysis

- goal: make prediction at a location $\mathbf{s}_0$

- only use data within a distance $d_0$ to make predictions

- slide the window across the domain

**draw picture**

- Advantages
    - local inference results in many small matrices
    - parallelization/GPUs
    - nonstationary model by construction
    
- Disadvantages
    - inefficient estimation -- especially if process is stationary
    - boundary effects -- need to resolve predictions along the boundaries
    - how to choose the distance $d_0$
    
## Low rank approimxations

- In general, approximate the true process using $M <  < n$  basis functions

\begin{align*}
y(\mathbf{s}) & = \mathbf{X}(\mathbf{s}) \boldsymbol{\beta} + \sum_{m=1}^M B_m(\mathbf{s}) \alpha_m + \varepsilon(\mathbf{s})
\end{align*}

where $\mathbf{x}(\mathbf{s})$ are covariates with fixed effect coefficients $\boldsymbol{\beta}$, B_m(\mathbf{s}) are basis functions evaluated at $\mathbf{s}$, $\boldsymbol{\alpha} = (\alpha_1, \ldots, \alpha_M)' \sim N(\mathbf{0}, \boldsymbol{\Sigma}_\alpha)$ are random coefficients, and $\varepsilon(\mathbf{s}) \stackrel{iid}{\sim} N(0, \sigma^2)$ is the nugget (observation error).

- There are many choices of basis functions where as $M \rightarrow \infty$ can approximate any stationary process

## Spectral basis functions

\begin{align*}
y(\mathbf{s}) & = \mathbf{X}(\mathbf{s}) \boldsymbol{\beta} + \sum_{m=1}^{M/2} sin(\boldsymbol{\omega}_m' \mathbf{s}) \alpha_{1m} + \sum_{m=1}^{M/2} cos(\boldsymbol{\omega}_m' \mathbf{s}) \alpha_{2m} + \varepsilon(\mathbf{s})
\end{align*}

where 

$\alpha_{jm} \stackrel{independent}{\sim} N(0, \tau^2 g(\boldsymbol{\omega}_m))$.

## Kernel convolutions

\begin{align*}
y(\mathbf{s}) & = \mathbf{X}(\mathbf{s}) \boldsymbol{\beta} + \sum_{m=1}^M K(\mathbf{s}, \boldsymbol{\kappa}_m) \alpha_m + \varepsilon(\mathbf{s})
\end{align*}

where $\boldsymbol{\alpha} = (\alpha_1, \ldots, \alpha_M)' \sim N(\mathbf{0}, \boldsymbol{\Sigma}_\alpha)$ -- typically $\boldsymbol{\Sigma}_\alpha = \tau^2 \mathbf{I}$

**draw picture**

- Multi-resolution extensions 

\begin{align*}
y(\mathbf{s}) & = \mathbf{X}(\mathbf{s}) \boldsymbol{\beta} + \sum_{\ell=1}^L \sum_{m=1}^M K(\mathbf{s}, \boldsymbol{\kappa}_{\ell m}) \alpha_{\ell m} + \varepsilon(\mathbf{s})
\end{align*}

- [Multiresolution Gaussian Process Model for the Analysis of Large Spatial Datasets](https://www.tandfonline.com/doi/pdf/10.1080/10618600.2014.914946)

```{r, cache = TRUE}
## Make some data
set.seed(1)
n <- 1000
coords <- cbind(runif(n, 0, 1), runif(n, 0, 1))

X <- cbind(1, rnorm(n))

beta <- as.matrix(c(1, 5))

sigma.sq <- 5
tau.sq <- 1
phi <- 3 / 0.5

D <- as.matrix(rdist(coords))
R <- exp(-phi * D)
w <- c(rmvn(1, rep(0,n), sigma.sq * R))
y <- rnorm(n, X %*% beta + w, sqrt(tau.sq))

## x are the spatial locations, y is the response variable, Z are the fixed effects
## note that X has an intercept term that needs to be removed
fit <- LatticeKrig(x = coords, y = y, Z = X[, 2])
summary(fit)
preds <- predict(fit)
plot(y, preds)
abline(0, 1, col = "red")
```

