[
["index.html", "Notes for STAT 5413 - Spatial Statistics Preface", " Notes for STAT 5413 - Spatial Statistics John Tipton Fall 2020 Semester. Last Modified: 2020-01-27 Preface These are the lecture notes for STAT 5413 Fall 2020. "],
["day-1.html", "1 Day 1 1.1 Notation 1.2 Probability Distributions 1.3 Hierarchical modeling", " 1 Day 1 library(tidyverse) 1.1 Notation The dimensions of different mathematical objects are very important for the study of spatial statistics. To communicate this, we use the following notation. A scalar random variable is represented by a lowercase alphanumeric letter (\\(x\\), \\(y\\), \\(z\\), etc.), a vector random variable is respresented by a bold lowercase alphanumeric letter (\\(\\mathbf{x}\\), \\(\\mathbf{y}\\), \\(\\mathbf{z}\\), etc.), and a matrix random variable is respresented by a bold uppercase alphanumeric letter (\\(\\mathbf{X}\\), \\(\\mathbf{Y}\\), \\(\\mathbf{Z}\\), etc.). We use a similar notation for parameters as well where scalar parameters are represented by a lowercase Greek letter (\\(\\mu\\), \\(\\alpha\\), \\(\\beta\\), etc.), a vector parameter is respresented by a bold lowercase Greek letter (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\alpha}\\), \\(\\boldsymbol{\\beta}\\), etc.), and a matrix random variable is respresented by a bold uppercase Greek letter (\\(\\boldsymbol{\\Sigma}\\), \\(\\boldsymbol{\\Psi}\\), \\(\\boldsymbol{\\Gamma}\\), etc.). 1.2 Probability Distributions We also need notation to explain probability distributions. We use the notation \\([y]\\) to denote the probability density function \\(p(y)\\) of the random variable \\(y\\) and \\([y|x]\\) to denote the probability density function \\(p(y|x)\\) of \\(y\\) given \\(x\\). For example, if \\(y\\) is a Gaussian random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) we write \\[\\begin{align*} [y | \\mu, \\sigma] &amp; = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{-\\frac{1}{2 \\sigma^2} (y - \\mu)^2 \\right\\}. \\end{align*}\\] We can also denote that \\(y\\) has a Gaussian (normal) distribution given mean \\(\\mu\\) and variance \\(\\sigma^2\\) using the \\(\\sim\\) notation \\[\\begin{align*} y | \\mu, \\sigma &amp; \\sim \\operatorname{N}(\\mu, \\sigma^2). \\end{align*}\\] 1.2.1 Example: linear regression \\[\\begin{align*} \\left[y_i | \\boldsymbol{\\theta} \\right] &amp; \\sim \\operatorname{N}(X_i \\beta, \\sigma^2) \\\\ \\boldsymbol{\\theta} &amp; = (\\beta, \\sigma^2) \\end{align*}\\] ## Sample data set.seed(404) dat &lt;- data.frame(x=(x=runif(200, 0, 50)), y=rnorm(200, 10 * x, 100)) ## breaks: where you want to compute densities breaks &lt;- seq(0, max(dat$x), len=7)[-c(1, 7)] dat$section &lt;- cut(dat$x, breaks) ## Get the residuals dat$res &lt;- residuals(lm(y ~ x, data=dat)) ## Compute densities for each section, and flip the axes, and add means of sections ## Note: the densities need to be scaled in relation to the section size (2000 here) ys &lt;- seq(-300, 300, length = 50) xs &lt;- rep(breaks, each = 50) + 1000 * dnorm(ys, 0, 100) res &lt;- matrix(0, 50, 5) for (i in 1:5) { res[, i] &lt;- 10 * breaks[i] + ys } dens &lt;- data.frame(x = xs, y=c(res), grouping = cut(xs, breaks)) ggplot(dat, aes(x, y)) + geom_point(size = 2) + geom_smooth(method=&quot;lm&quot;, fill=NA, lwd=2, se = FALSE) + geom_path(data=dens, aes(x, y, group = grouping), color=&quot;salmon&quot;, lwd=2) + theme_bw() + geom_vline(xintercept=breaks, lty=2) 1.3 Hierarchical modeling Follow Berliner (1996) framework for hierarchical probability models Model encodes our understanding of the scientific process of interest Model accounts for as much uncertainty as possible Model results in a probability distribution Note: nature may be deterministic – often probabilistic models outperform physical models. Example: model individual rain drops vs. probability/intensity of rain Update model with data Use the model to generate parameter estimates given data 1.3.1 Bayesian Hierarchical models (BHMs) Break the model into components: Data Model. Process Model. Parameter Model. Combined, the data model, the process model, and the parameter model define a posterior distribution. \\[\\begin{align*} \\color{cyan}{[\\mathbf{z}, \\boldsymbol{\\theta}_D, \\boldsymbol{\\theta}_P | \\mathbf{y}]} &amp; \\propto \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\color{orange}{[\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P]} \\end{align*}\\] 1.3.2 Empirical Hierarchical models (EHMs) Break the model into components: Data Model. Process Model. Parameter estimates (fixed values) are substituted before fitting the model Combined, the data model and the process model define a predictive distribution. Thus, numerical evaluation of the predictive distribution is typically required to estimate unceratinty (bootstrap, MLE asymptotics) Note: the predictive distribution is not a posterior distribution because the normalizing constant is not known \\[\\begin{align*} \\color{plum}{[\\mathbf{z} | \\mathbf{y}]} &amp; \\propto \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\end{align*}\\] 1.3.3 Data Model \\[\\begin{align*} \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\end{align*}\\] Describes how the data are collected and observed. Account for measurement process and uncertainty. Model the data in the manner in which they were collected. Data \\(\\mathbf{y}\\). Noisy. Expensive. Not what you want to make inference on. Latent variables \\(\\mathbf{z}\\). Think of \\(\\mathbf{z}\\) as the ideal data. No measurement error - the exact quantity you want to observe but can’t. Data model parameters \\(\\boldsymbol{\\theta}_D\\). 1.3.4 Process Model \\[\\begin{align*} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\end{align*}\\] Where the science happens! Latent process \\(\\mathbf{z}\\) is modeled. Can be dynamic in space and/or time Process parameters \\(\\boldsymbol{\\theta}_P\\). Virtually all interesting scientific questions can be made with inference about \\(\\mathbf{z}\\) 1.3.5 Parameter (Prior) Model (BMHs only) \\[\\begin{align*} \\color{orange}{[\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P]} \\end{align*}\\] Probability distributions define “reasonable” ranges for parameters. Parameter models are useful for a variety of problems: Choosing important variables. Preventing over-fitting (regularization). “Pooling” estimates across categories. 1.3.6 Posterior Distribution \\[\\begin{align*} \\color{cyan}{[\\mathbf{z}, \\boldsymbol{\\theta}_D, \\boldsymbol{\\theta}_P | \\mathbf{y}]} &amp; \\propto [\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}] [\\mathbf{z} | \\boldsymbol{\\theta}_P] [\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P] \\end{align*}\\] Probability distribution over all unknowns in the model. Inference is made using the posterior distribution. Because the posterior distribution is a probability distribution (BHMs), uncertainty is easy to calculate. This is not true for EHMs. 1.3.7 Scientifically Motivated Statistical Modeling Criticize the model Does the model fit the data well? Do the predictions make sense? Are there subsets of the data that don’t fit the model well? Make inference using the model. If the model fits the data, use the model fit for prediction or inference. References "],
["day-2.html", "2 Day 2 2.1 Spatial Data 2.2 Types of spatial data", " 2 Day 2 library(tidyverse) library(here) library(sp) library(spatstat) 2.1 Spatial Data All data occur at some location is space and time. For know we focus on spatial analyses and will later extend this to spatio-temporal analyses. Let \\(\\mathcal{D}\\) represent the spatial domain and let \\(\\mathbf{s}\\) be a spatial location. In general, we will let \\(\\mathcal{A} \\subset \\mathcal{D}\\) be a subdomain of the spatial region of \\(\\mathbf{D}\\). knitr::include_graphics(here::here(&quot;images&quot;, &quot;spatial-domain.jpg&quot;)) 2.2 Types of spatial data There are three primary types of spatial data that we are going to consider 2.2.1 Geostatistical data Occur everywhere continuous support examples: temperature, precipitation data(&quot;NOAA_df_1990&quot;, package = &quot;STRbook&quot;) glimpse(NOAA_df_1990) ## Observations: 730,486 ## Variables: 10 ## $ julian &lt;int&gt; 726834, 726835, 726836, 726837, 726838, 726839, 726840, 726841… ## $ year &lt;int&gt; 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 19… ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ day &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… ## $ id &lt;dbl&gt; 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 38… ## $ z &lt;dbl&gt; 35, 42, 49, 59, 41, 45, 46, 42, 54, 43, 52, 38, 32, 43, 53, 55… ## $ proc &lt;chr&gt; &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;… ## $ lat &lt;dbl&gt; 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35,… ## $ lon &lt;dbl&gt; -81.43333, -81.43333, -81.43333, -81.43333, -81.43333, -81.433… ## $ date &lt;date&gt; 1990-01-01, 1990-01-02, 1990-01-03, 1990-01-04, 1990-01-05, 1… ## Only plot the states with data states &lt;- map_data(&quot;state&quot;) states &lt;- states %&gt;% subset(!(region %in% c( &quot;washington&quot;, &quot;oregon&quot;, &quot;california&quot;, &quot;nevada&quot;, &quot;idaho&quot;, &quot;utah&quot;, &quot;arizona&quot;,&quot;montana&quot;, &quot;wyoming&quot;, &quot;colorado&quot;, &quot;new mexico&quot; ) )) ## generate map NOAA_df_1990 %&gt;% subset(year == 1990 &amp; day == 1 &amp; proc == &quot;Tmax&quot;) %&gt;% ggplot(aes(x = lon, y = lat, color = z)) + geom_point() + facet_wrap(~ month, scales = &quot;free&quot;, nrow = 4) + geom_polygon(data = states, aes(x = long, y = lat, group = group), inherit.aes = FALSE, fill = NA, color = &quot;black&quot;) + scale_color_viridis_c(option = &quot;inferno&quot;) + ggtitle(&quot;Tmax for the first day of each month in 1990&quot;) 2.2.2 Areal data Occur only over discrete areas can be thought of as an integral of a continuous process over a subdomain \\(\\mathcal{A} \\in \\mathcal{D}\\) examples: cases of a disease by counties, votes in an election by congressional district data(&quot;BEA&quot;, package = &quot;STRbook&quot;) glimpse(BEA) ## Observations: 116 ## Variables: 5 ## $ Description &lt;chr&gt; &quot;Per capita personal income (dollars)&quot;, &quot;Per capita perso… ## $ NAME10 &lt;fct&gt; &quot;Adair, MO&quot;, &quot;Andrew, MO&quot;, &quot;Atchison, MO&quot;, &quot;Audrain, MO&quot;,… ## $ X1970 &lt;int&gt; 2723, 3577, 3770, 3678, 3021, 2832, 3263, 2508, 2147, 349… ## $ X1980 &lt;int&gt; 7399, 7937, 5743, 8356, 7210, 7445, 8596, 6125, 5431, 923… ## $ X1990 &lt;int&gt; 12755, 15059, 14748, 15198, 12873, 13530, 13195, 11854, 1… data(&quot;MOcounties&quot;, package = &quot;STRbook&quot;) glimpse(MOcounties) ## Observations: 214,279 ## Variables: 53 ## $ long &lt;dbl&gt; 627911.9, 627921.4, 627923.0, 627947.8, 627956.5, 627994.8… ## $ lat &lt;dbl&gt; 4473554, 4473559, 4473560, 4473577, 4473583, 4473612, 4473… ## $ order &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ hole &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ piece &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ id &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;… ## $ group &lt;fct&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1… ## $ STATEFP10 &lt;fct&gt; 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29… ## $ COUNTYFP10 &lt;fct&gt; 045, 045, 045, 045, 045, 045, 045, 045, 045, 045, 045, 045… ## $ COUNTYNS10 &lt;fct&gt; 00758477, 00758477, 00758477, 00758477, 00758477, 00758477… ## $ GEOID10 &lt;fct&gt; 29045, 29045, 29045, 29045, 29045, 29045, 29045, 29045, 29… ## $ NAME10 &lt;fct&gt; &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark… ## $ NAMELSAD10 &lt;fct&gt; Clark County, Clark County, Clark County, Clark County, Cl… ## $ LSAD10 &lt;fct&gt; 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06… ## $ CLASSFP10 &lt;fct&gt; H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1… ## $ MTFCC10 &lt;fct&gt; G4020, G4020, G4020, G4020, G4020, G4020, G4020, G4020, G4… ## $ CSAFP10 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ CBSAFP10 &lt;fct&gt; 22800, 22800, 22800, 22800, 22800, 22800, 22800, 22800, 22… ## $ METDIVFP10 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ FUNCSTAT10 &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A… ## $ ALAND10 &lt;dbl&gt; 1307146971, 1307146971, 1307146971, 1307146971, 1307146971… ## $ AWATER10 &lt;dbl&gt; 18473547, 18473547, 18473547, 18473547, 18473547, 18473547… ## $ INTPTLAT10 &lt;fct&gt; +40.4072748, +40.4072748, +40.4072748, +40.4072748, +40.40… ## $ INTPTLON10 &lt;fct&gt; -091.7294720, -091.7294720, -091.7294720, -091.7294720, -0… ## $ AREA &lt;dbl&gt; 1324937990, 1324937990, 1324937990, 1324937990, 1324937990… ## $ PERIMETER &lt;dbl&gt; 161503.6, 161503.6, 161503.6, 161503.6, 161503.6, 161503.6… ## $ COUNTY10_ &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ COUNTY10_I &lt;int&gt; 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115… ## $ POP90 &lt;int&gt; 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547… ## $ WHITE90 &lt;int&gt; 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528… ## $ BLACK90 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… ## $ ASIANPI90 &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4… ## $ AMIND90 &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7… ## $ OTHER90 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ HISP90 &lt;int&gt; 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26… ## $ POP00 &lt;int&gt; 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416… ## $ WHITE00 &lt;int&gt; 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329… ## $ BLACK00 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ ASIAN00 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ AMIND00 &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15… ## $ HAWNPI00 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ OTHER00 &lt;int&gt; 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16… ## $ MULTRA00 &lt;int&gt; 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45… ## $ HISP00 &lt;int&gt; 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52… ## $ POP10 &lt;int&gt; 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139… ## $ WHITE10 &lt;int&gt; 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011… ## $ BLACK10 &lt;int&gt; 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19… ## $ ASIAN10 &lt;int&gt; 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23… ## $ AMIND10 &lt;int&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9… ## $ HAWNPI10 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ OTHER10 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ MULTRA10 &lt;int&gt; 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72… ## $ HISP10 &lt;int&gt; 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42… MOcounties &lt;- left_join(MOcounties, BEA, by = &quot;NAME10&quot;) ggplot(MOcounties) + geom_polygon(aes(x = long, y = lat, # county boundary group = NAME10, # county group fill = log(X1970))) + # log of income geom_path(aes(x = long, y = lat, group = NAME10)) + scale_fill_viridis_c(limits = c(7.5, 10.2), option = &quot;plasma&quot;, name = &quot;log($)&quot;) + coord_fixed() + ggtitle(&quot;1970&quot;) + xlab(&quot;x (m)&quot;) + ylab(&quot;y (m)&quot;) + theme_bw() 2.2.3 Point process data The count and location of the data are random examples: tornados, lightning strikes # uncomment out this line to download the data # load(url(&quot;http://github.com/mgimond/Spatial/raw/master/Data/ppa.RData&quot;)) # save(starbucks, ma, pop, file = here::here(&quot;data&quot;, &quot;ppa-starbucks.RData&quot;)) load(here::here(&quot;data&quot;, &quot;ppa-starbucks.RData&quot;)) glimpse(starbucks) ## List of 5 ## $ window :List of 4 ## ..$ type : chr &quot;rectangle&quot; ## ..$ xrange: num [1:2] 648032 917741 ## ..$ yrange: num [1:2] 4609785 4748107 ## ..$ units :List of 3 ## .. ..$ singular : chr &quot;unit&quot; ## .. ..$ plural : chr &quot;units&quot; ## .. ..$ multiplier: num 1 ## .. ..- attr(*, &quot;class&quot;)= chr &quot;unitname&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;owin&quot; ## $ n : int 171 ## $ x : num [1:171] 917741 911147 902987 876188 875868 ... ## $ y : num [1:171] 4637151 4628510 4628982 4616741 4616719 ... ## $ markformat: chr &quot;none&quot; ## - attr(*, &quot;class&quot;)= chr &quot;ppp&quot; ## uses spatstat library ## add the massachusetts polygon Window(starbucks) &lt;- ma marks(starbucks) &lt;- NULL ## plot using the plot function from spatstat plot(starbucks) "],
["day-3.html", "3 Day 3 3.1 Anouncements 3.2 Files for spatial data 3.3 Textbook package 3.4 Spatial Visualization", " 3 Day 3 library(tidyverse) library(here) library(sp) 3.1 Anouncements Course audits Show gitHub page for site https://github.com/jtipton25/STAT-5413 Show how to download files and data Example Gerrymandering https://uglygerry.com/ library(showtext) ## Loading required package: sysfonts ## Loading required package: showtextdb font_add(&quot;myfont&quot;, here::here(&quot;fonts&quot;, &quot;Gerry.otf&quot;)) plot(cars, family = &quot;myfont&quot;) title( main = &quot;This Font is made of \\n Gerrymandered Political Districts&quot;, family = &quot;myfont&quot;, cex.main = 1.5 ) 3.2 Files for spatial data Many different file types for spatial data Typically data are in “flat files” like comma-seperated value (CSV) files read.csv(here(&quot;path&quot;, &quot;to&quot;, &quot;file.csv&quot;)) “shapefiles” which can be read using rgdal or maptools packages library(rgdal) library(maptools) “NetCDF” files cane be read using ncdf4 or RNetCDF library(ncdf4) library(RNetCDF) 3.3 Textbook package To install the data from the textbook, go to https://spacetimewithr.org/ and follow the link to the code. # install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;andrewzm/STRbook&quot;) Note that this package is relatively large because it contains a decent amount of spatial data. library(STRbook) ## ## Attaching package: &#39;STRbook&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## MOcounties 3.4 Spatial Visualization 3.4.1 Spatial visualization using fields Simulate a process with some random locations library(fields) ## Loading required package: spam ## Loading required package: dotCall64 ## Loading required package: grid ## Spam version 2.5-1 (2019-12-12) is loaded. ## Type &#39;help( Spam)&#39; or &#39;demo( spam)&#39; for a short introduction ## and overview of this package. ## Help for individual functions is also obtained by adding the ## suffix &#39;.spam&#39; to the function name, e.g. &#39;help( chol.spam)&#39;. ## ## Attaching package: &#39;spam&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## backsolve, forwardsolve ## Loading required package: maps ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map ## See https://github.com/NCAR/Fields for ## an extensive vignette, other supplements and source code ## longitude and latitude of approximately the center of Arkansas lon_lat_center &lt;- c(-92.33, 35.00) n &lt;- 1000 ## simulate some random locations lon &lt;- runif(n, lon_lat_center[1] - 2, lon_lat_center[1] + 2) lat &lt;- runif(n, lon_lat_center[2] - 2, lon_lat_center[2] + 2) y &lt;- rnorm(n, lat + lon, 0.1) plot(lon, lat) quilt.plot(lon, lat, y, nx = 30, ny = 30) points(lon, lat, cex = .3) quilt.plot(lon, lat, y, nx = 6, ny = 10) points(lon, lat, cex = .3) Simulate a process on a regular grid n &lt;- 50^2 ## simulate locations on a grid lon &lt;- seq(lon_lat_center[1] - 2, lon_lat_center[1] + 2, length = sqrt(n)) lat &lt;- seq(lon_lat_center[2] - 2, lon_lat_center[2] + 2, length = sqrt(n)) s &lt;- expand.grid(lon, lat) head(lon) ## [1] -94.33000 -94.24837 -94.16673 -94.08510 -94.00347 -93.92184 head(lat) ## [1] 33.00000 33.08163 33.16327 33.24490 33.32653 33.40816 head(s) ## Var1 Var2 ## 1 -94.33000 33 ## 2 -94.24837 33 ## 3 -94.16673 33 ## 4 -94.08510 33 ## 5 -94.00347 33 ## 6 -93.92184 33 plot(s, cex = 0.3) ## simulate some fake data with a north/south trend y &lt;- 120 - 1.5 * s[, 2] + matrix(rnorm(n), sqrt(n), sqrt(n)) image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;) contour(lon, lat, y, main = &quot;Contour plot of simulated data&quot;) image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;) contour(lon, lat, y, main = &quot;Contour plot of simulated data&quot;, add = TRUE, nlevels = 10) ## adding in maps library(maps) maps::map(&quot;world&quot;) maps::map(&quot;state&quot;) maps::map(&quot;county&quot;) maps::map(&quot;county&quot;, &quot;Arkansas&quot;) points(s, cex = 0.3) state &lt;- map.where(&quot;state&quot;, x = s[, 1], y = s[, 2]) head(state) ## [1] &quot;texas&quot; &quot;texas&quot; &quot;texas&quot; &quot;texas&quot; &quot;louisiana&quot; &quot;louisiana&quot; table(state) ## state ## arkansas louisiana mississippi missouri texas ## 1903 34 180 351 32 ## subset only points in arkansas dat &lt;- data.frame( lon = s[, 1], lat = s[, 2], state = state ) maps::map(&quot;county&quot;, &quot;Arkansas&quot;) dat %&gt;% subset(state == &quot;arkansas&quot;) %&gt;% points(cex = 0.3) # points(subset(dat, state == &quot;arkansas&quot;), cex = 0.3) Plot the simulated data with the county boundaries image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;) maps::map(&quot;county&quot;, add = TRUE, lwd = 2) ## change the aspect ratio image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;, asp = 1.3) maps::map(&quot;county&quot;, add = TRUE, lwd = 2) "],
["day-4.html", "4 Day 4 4.1 Announcements 4.2 Visualization (continued) 4.3 Interactive visualization (Interactive with HTML format only)", " 4 Day 4 library(tidyverse) 4.1 Announcements 4.2 Visualization (continued) 4.2.1 Spatial visualization using fields nx &lt;- 100 ny &lt;- 100 library(maps) # for map.where # Corner of the USA corners &lt;- c(-124.733056, -66.947028, 24.520833, 49.384472) # create grid grid &lt;- expand.grid( seq(corners[1], corners[2], length = nx), seq(corners[3], corners[4], length = ny) ) dat &lt;- data.frame( lon = grid[, 1], lat = grid[, 2], inUS = ifelse(is.na(map.where(&quot;usa&quot;, x = grid[, 1], y = grid[, 2])), FALSE, TRUE) ) ## Plot only points in the us dat %&gt;% subset(inUS) %&gt;% ## this selects only the true values ggplot(aes(x = lon, y = lat)) + geom_point(size = 0.6, alpha = 0.5) ## Simulate some data over the grid dat$y &lt;- sin(2 * pi * dat$lon / 10) + cos(2 *pi * dat$lat / 10) + sin(2 * pi * dat$lon / 10) * cos(2 *pi * dat$lat / 10) ## plot each of the responses grouped by latitude dat %&gt;% ggplot(aes(x = lon, y = y, group = lat, color = lat)) + geom_line() ## Function to generate maps map_points &lt;- function (dat, color_low = &quot;white&quot;, color_high = &quot;darkred&quot;, color_na = gray(0.9), zeroiswhite = FALSE, xlim = NULL, ylim = NULL, zlim = NULL, mainTitle = NULL, legendTitle = &quot;&quot;) { library(ggplot2) ## check if the data.fram dat contains the correct variables if (is.null(dat$lon)) { stop(&#39;The data.frame dat must contain a &quot;lon&quot; variable&#39;) } if (is.null(dat$lat)) { stop(&#39;The data.frame dat must contain a &quot;lat&quot; variable&#39;) } if (is.null(dat$y)) { stop(&#39;The data.frame dat must contain a &quot;y&quot; variable&#39;) } # Store the base data of the underlying map states &lt;- map_data(&quot;state&quot;) # Set limits for x, y, z if not specified as parameters if (is.null(xlim)) { xlim &lt;- range(dat$lon, na.rm = TRUE) } if (is.null(ylim)) { ylim &lt;- range(dat$lat, na.rm = TRUE) } if (is.null(zlim)) { zlim &lt;- range(dat$y, na.rm = TRUE) } # Create the plot p &lt;- ggplot(dat, aes(x = lon, y = lat)) + theme_bw() p &lt;- p + theme(plot.title = element_text(size = rel(1.5))) p &lt;- p + geom_point(aes(colour = y)) ## add in the map p &lt;- p + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) ## a 1.3 coordinate ratio is visually appealing p &lt;- p + coord_fixed(ratio = 1.3, xlim = xlim, ylim = ylim) p &lt;- p + labs(title = paste(mainTitle, &quot;\\n&quot;, sep=&quot;&quot;), x = &quot;&quot;, y = &quot;&quot;) if(zeroiswhite){ p &lt;- p + scale_colour_gradient2( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } if(!zeroiswhite){ p &lt;- p + scale_colour_gradient( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } return(p) } ## Let&#39;s make some plots dat %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, mainTitle = &quot;Entire United States&quot; ) ## Subset only the US dat %&gt;% subset(inUS) %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, mainTitle = &quot;Entire United States&quot; ) ## plot only a subset of points dat %&gt;% subset(inUS) %&gt;% ## sample 500 points at random sample_n(500) %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, zeroiswhite = TRUE, mainTitle = &quot;Entire United States&quot; ) ## Truncate the southeastern US dat %&gt;% subset(inUS) %&gt;% ## sample 500 points at random sample_n(500) %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, zeroiswhite = TRUE, xlim = c(-95, -75), ylim = c(25, 37.5), mainTitle = &quot;Southeastern United States&quot;, legendTitle = &quot;Widgets&quot; ) Heatmaps can also be used for plotting. In general, there are two ggplot geoms that are useful for spatial data: geom_tile is good for irregularly spaced data, geom_raster is best for regularly spaced data as it is faster to process. ## Function to generate maps map_heat &lt;- function (dat, color_low = &quot;white&quot;, color_high = &quot;darkred&quot;, color_na = gray(0.9), zeroiswhite = FALSE, xlim = NULL, ylim = NULL, zlim = NULL, mainTitle = NULL, legendTitle = &quot;&quot;, geom = &quot;raster&quot;) { library(ggplot2) ## check if the data.fram dat contains the correct variables if (is.null(dat$lon)) { stop(&#39;The data.frame dat must contain a &quot;lon&quot; variable&#39;) } if (is.null(dat$lat)) { stop(&#39;The data.frame dat must contain a &quot;lat&quot; variable&#39;) } if (is.null(dat$y)) { stop(&#39;The data.frame dat must contain a &quot;y&quot; variable&#39;) } if (!(geom %in% c(&quot;raster&quot;, &quot;tile&quot;))) { stop(&#39;The only options for geom are &quot;raster&quot; or &quot;tile&quot;&#39;) } # Store the base data of the underlying map states &lt;- map_data(&quot;state&quot;) # Set limits for x, y, z if not specified as parameters if (is.null(xlim)) { xlim &lt;- range(dat$lon, na.rm = TRUE) } if (is.null(ylim)) { ylim &lt;- range(dat$lat, na.rm = TRUE) } if (is.null(zlim)) { zlim &lt;- range(dat$y, na.rm = TRUE) } # Create the plot p &lt;- ggplot(dat, aes(x = lon, y = lat)) + theme_bw() p &lt;- p + theme(plot.title = element_text(size = rel(1.5))) if (geom == &quot;raster&quot;) { p &lt;- p + geom_raster(aes(fill = y)) } if (geom == &quot;tile&quot;) { p &lt;- p + geom_tile(aes(fill = y)) } ## add in the map p &lt;- p + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) ## a 1.3 coordinate ratio is visually appealing p &lt;- p + coord_fixed(ratio = 1.3, xlim = xlim, ylim = ylim) p &lt;- p + labs(title = paste(mainTitle, &quot;\\n&quot;, sep=&quot;&quot;), x = &quot;&quot;, y = &quot;&quot;) if(zeroiswhite){ p &lt;- p + scale_colour_gradient2( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } if(!zeroiswhite){ p &lt;- p + scale_colour_gradient( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } return(p) } ## Subset only the US dat %&gt;% subset(inUS) %&gt;% map_heat( color_low = &quot;blue&quot;, color_high = &quot;yellow&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;raster&quot; ) ## Subset only the US dat %&gt;% subset(inUS) %&gt;% map_heat( color_low = &quot;blue&quot;, color_high = &quot;yellow&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;tile&quot; ) ## Subsample the data dat %&gt;% subset(inUS) %&gt;% sample_n(1000) %&gt;% map_heat( color_low = &quot;blue&quot;, color_high = &quot;green&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;raster&quot; ) ## Warning in f(...): Raster pixels are placed at uneven horizontal intervals and ## will be shifted. Consider using geom_tile() instead. ## Warning in f(...): Raster pixels are placed at uneven vertical intervals and ## will be shifted. Consider using geom_tile() instead. ## Subsample the data dat %&gt;% subset(inUS) %&gt;% sample_n(1000) %&gt;% map_heat( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;tile&quot; ) Plotting spatial data using google maps ## longitude and latitude of approximately the center of Arkansas arkansas_center &lt;- c(-92.33, 35.00) library(maps) library(ggplot2) library(ggmap) ## Google&#39;s Terms of Service: https://cloud.google.com/maps-platform/terms/. ## Please cite ggmap if you use it! See citation(&quot;ggmap&quot;) for details. lon &lt;- arkansas_center[1] + seq(-2, 2, length = 10) lat &lt;- arkansas_center[2] + seq(-2, 2, length = 10) s &lt;- expand.grid(lon, lat) head(lon) ## [1] -94.33000 -93.88556 -93.44111 -92.99667 -92.55222 -92.10778 head(lat) ## [1] 33.00000 33.44444 33.88889 34.33333 34.77778 35.22222 str(s) ## &#39;data.frame&#39;: 100 obs. of 2 variables: ## $ Var1: num -94.3 -93.9 -93.4 -93 -92.6 ... ## $ Var2: num 33 33 33 33 33 33 33 33 33 33 ... ## - attr(*, &quot;out.attrs&quot;)=List of 2 ## ..$ dim : int 10 10 ## ..$ dimnames:List of 2 ## .. ..$ Var1: chr &quot;Var1=-94.33000&quot; &quot;Var1=-93.88556&quot; &quot;Var1=-93.44111&quot; &quot;Var1=-92.99667&quot; ... ## .. ..$ Var2: chr &quot;Var2=33.00000&quot; &quot;Var2=33.44444&quot; &quot;Var2=33.88889&quot; &quot;Var2=34.33333&quot; ... plot(s) points(arkansas_center, pch = 19, col = 2) dat &lt;- data.frame(lon = lon, lat = lat) Using Google maps requires registration of a key. See https://www.littlemissdata.com/blog/maps for details. Plotting areal data The example is from https://www4.stat.ncsu.edu/~reich/SpatialStats/code/Guns.pdf taken from https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(15)01026-0/fulltext ## process the guns data # load(here::here(&quot;data&quot;, &quot;guns.RData&quot;)) # names(Y)[1:5] # region &lt;- tolower(names(Y)) # region[1:5] # rate &lt;- 10000*Y/N # numlaws &lt;- rowSums(X) # crime &lt;- data.frame(Y=Y,N=N,rate=rate,X=X,numlaws,region=region) # dat &lt;- data.frame( # deaths_2010 = Y, # population = N, # deaths_per_10000 = Z[, 1], # firearm_quartile = Z[, 2], # unemployment_quartile = Z[, 3], # non_firearm_homocide = Z[, 4], # firearm_export_quartile = Z[, 5], # numlaws = apply(X, 1, sum), # region = region # ) # save(dat, file = here::here(&quot;data&quot;, &quot;guns_processed.RData&quot;)) load(here::here(&quot;data&quot;, &quot;guns_processed.RData&quot;)) ## mutate a death rate dat &lt;- dat %&gt;% mutate(rate = 10000 * deaths_2010 / population) dat %&gt;% ggplot(aes(x = numlaws, y = rate, color = region == &quot;arkansas&quot;)) + geom_point() + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;)) + xlab(&quot;Number of gun control laws&quot;) + ylab(&quot;Homicide rate (deaths/100K)&quot;) + ggtitle(&quot;Arkansas in red&quot;) + theme(legend.position = &quot;none&quot;) lm(rate ~ numlaws, data = dat) %&gt;% summary() ## ## Call: ## lm(formula = rate ~ numlaws, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46715 -0.16720 -0.02576 0.16171 0.72809 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.344693 0.055145 24.385 &lt; 2e-16 *** ## numlaws -0.045276 0.007302 -6.201 1.24e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2867 on 48 degrees of freedom ## Multiple R-squared: 0.4448, Adjusted R-squared: 0.4332 ## F-statistic: 38.45 on 1 and 48 DF, p-value: 1.236e-07 us &lt;- map_data(&quot;state&quot;) head(us) ## long lat group order region subregion ## 1 -87.46201 30.38968 1 1 alabama &lt;NA&gt; ## 2 -87.48493 30.37249 1 2 alabama &lt;NA&gt; ## 3 -87.52503 30.37249 1 3 alabama &lt;NA&gt; ## 4 -87.53076 30.33239 1 4 alabama &lt;NA&gt; ## 5 -87.57087 30.32665 1 5 alabama &lt;NA&gt; ## 6 -87.58806 30.32665 1 6 alabama &lt;NA&gt; gg &lt;- ggplot() gg &lt;- gg + geom_map(data = us, map = us, aes(x = long, y = lat, map_id = region), fill = &quot;#ffffff&quot;, color = &quot;#ffffff&quot;, size = 0.15) ## Warning: Ignoring unknown aesthetics: x, y gg gg &lt;- gg + geom_map( data = dat, map = us, aes(fill = rate, map_id = region), color = &quot;#ffffff&quot;, size = 0.15 ) gg &lt;- gg + scale_fill_continuous( low = &#39;thistle2&#39;, high = &#39;darkred&#39;, guide= &#39;colorbar&#39;, name = &quot;Deaths/100K&quot; ) gg &lt;- gg + labs(x = NULL, y = NULL, title = &quot;Homicide rates&quot;) gg &lt;- gg + coord_map(&quot;albers&quot;, lat0 = 39, lat1 = 45) gg &lt;- gg + theme(panel.border = element_blank()) gg &lt;- gg + theme(panel.background = element_blank()) gg &lt;- gg + theme(axis.ticks = element_blank()) gg &lt;- gg + theme(axis.text = element_blank()) gg The map looks right according to http://www.deathpenaltyinfo.org/murder-rates-nationally-and-state#MRord 4.2.2 In Class Activity: From Lab 2.1 on the textbook site ## Wikle, C. K., Zammit-Mangion, A., and Cressie, N. (2019), ## Spatio-Temporal Statistics with R, Boca Raton, FL: Chapman &amp; Hall/CRC ## Copyright (c) 2019 Wikle, Zammit-Mangion, Cressie ## ## This program is free software; you can redistribute it and/or ## modify it under the terms of the GNU General Public License ## as published by the Free Software Foundation; either version 2 ## of the License, or (at your option) any later version. ## ## This program is distributed in the hope that it will be useful, ## but WITHOUT ANY WARRANTY; without even the implied warranty of ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the ## GNU General Public License for more details. library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) library(&quot;STRbook&quot;) ## ------------------------------------------------------------------------ locs &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Stationinfo.dat&quot;, package = &quot;STRbook&quot;), col.names = c(&quot;id&quot;, &quot;lat&quot;, &quot;lon&quot;)) Times &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Times_1990.dat&quot;, package = &quot;STRbook&quot;), col.names = c(&quot;julian&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) Tmax &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Tmax_1990.dat&quot;, package = &quot;STRbook&quot;)) ## ------------------------------------------------------------------------ names(Tmax) &lt;- locs$id ## ------------------------------------------------------------------------ Tmax &lt;- cbind(Times, Tmax) head(names(Tmax), 10) ## ------------------------------------------------------------------------ Tmax_long &lt;- gather(Tmax, id, z, -julian, -year, -month, -day) head(Tmax_long) ## ------------------------------------------------------------------------ Tmax_long$id &lt;- as.integer(Tmax_long$id) ## ----------------------------------------------------------- nrow(Tmax_long) Tmax_long &lt;- filter(Tmax_long, !(z &lt;= -9998)) nrow(Tmax_long) ## ------------------------------------------------------------------------ Tmax_long &lt;- mutate(Tmax_long, proc = &quot;Tmax&quot;) head(Tmax_long) ## ------------------------------------------------------------------------ data(Tmin_long, package = &quot;STRbook&quot;) data(TDP_long, package = &quot;STRbook&quot;) data(Precip_long, package = &quot;STRbook&quot;) ## ------------------------------------------------------------------------ NOAA_df_1990 &lt;- rbind(Tmax_long, Tmin_long, TDP_long, Precip_long) ## ------------------------------------------------------------------------ summ &lt;- group_by(NOAA_df_1990, year, proc) %&gt;% # groupings summarise(mean_proc = mean(z)) # operation ## ------------------------------------------------------------------------ NOAA_precip &lt;- filter(NOAA_df_1990, proc == &quot;Precip&quot; &amp; month == 6) summ &lt;- group_by(NOAA_precip, year, id) %&gt;% summarise(days_no_precip = sum(z == 0)) head(summ) ## ------------------------------------------------------------------------ median(summ$days_no_precip) ## ------------------------------------------------------------- grps &lt;- group_by(NOAA_precip, year, id) summ &lt;- summarise(grps, days_no_precip = sum(z == 0)) ## ------------------------------------------------------------------------ NOAA_df_sorted &lt;- arrange(NOAA_df_1990, julian, id) ## ------------------------------------------------------------------------ df1 &lt;- select(NOAA_df_1990, julian, z) df2 &lt;- select(NOAA_df_1990, -julian) ## ------------------------------------------------------------------------ NOAA_df_1990 &lt;- left_join(NOAA_df_1990, locs, by = &quot;id&quot;) ## ------------------------------------------------------------------------ Tmax_long_sel &lt;- select(Tmax_long, julian, id, z) Tmax_wide &lt;- spread(Tmax_long_sel, id, z) dim(Tmax_wide) ## ------------------------------------------------------------------------ M &lt;- select(Tmax_wide, -julian) %&gt;% as.matrix() ## ----------------------------------------------------------- library(&quot;sp&quot;) library(&quot;spacetime&quot;) ## ------------------------------------------------------------------------ NOAA_df_1990$date &lt;- with(NOAA_df_1990, paste(year, month, day, sep = &quot;-&quot;)) head(NOAA_df_1990$date, 4) # show first four elements ## ------------------------------------------------------------------------ NOAA_df_1990$date &lt;- as.Date(NOAA_df_1990$date) class(NOAA_df_1990$date) ## ------------------------------------------------------------------------ Tmax_long2 &lt;- filter(NOAA_df_1990, proc == &quot;Tmax&quot;) STObj &lt;- stConstruct(x = Tmax_long2, # data set space = c(&quot;lon&quot;, &quot;lat&quot;), # spatial fields time = &quot;date&quot;) # time field class(STObj) ## ------------------------------------------------------------------------ spat_part &lt;- SpatialPoints(coords = Tmax_long2[, c(&quot;lon&quot;, &quot;lat&quot;)]) temp_part &lt;- Tmax_long2$date STObj2 &lt;- STIDF(sp = spat_part, time = temp_part, data = select(Tmax_long2, -date, -lon, -lat)) class(STObj2) ## ------------------------------------------------------------------------ spat_part &lt;- SpatialPoints(coords = locs[, c(&quot;lon&quot;, &quot;lat&quot;)]) temp_part &lt;- with(Times, paste(year, month, day, sep = &quot;-&quot;)) temp_part &lt;- as.Date(temp_part) ## ------------------------------------------------------------------------ Tmax_long3 &lt;- gather(Tmax, id, z, -julian, -year, -month, -day) ## ------------------------------------------------------------------------ Tmax_long3$id &lt;- as.integer(Tmax_long3$id) Tmax_long3 &lt;- arrange(Tmax_long3,julian,id) ## ------------------------------------------------------------------------ all(unique(Tmax_long3$id) == locs$id) ## ------------------------------------------------------------------------ STObj3 &lt;- STFDF(sp = spat_part, time = temp_part, data = Tmax_long3) class(STObj3) ## ------------------------------------------------------------------------ proj4string(STObj3) &lt;- CRS(&quot;+proj=longlat +ellps=WGS84&quot;) ## ------------------------------------------------------------------------ STObj3$z[STObj3$z == -9999] &lt;- NA 4.3 Interactive visualization (Interactive with HTML format only) 4.3.1 Interactive display of data ## first you need to install these packages # install.packages(&quot;webshot&quot;) # webshot::install_phantomjs() DT::datatable(iris) 4.3.2 Animations Lets animate the NOAA_df_1990 dataset data(&quot;NOAA_df_1990&quot;, package = &quot;STRbook&quot;) ## 48 unique months and years month_year &lt;- NOAA_df_1990 %&gt;% subset(day == 1 &amp; id == 3804 &amp; proc == &quot;Tmax&quot;) %&gt;% # group_by(year, month) %&gt;% select(month, year) ## limits of the temperature range zlim &lt;- NOAA_df_1990 %&gt;% subset(proc == &quot;Tmax&quot;) %&gt;% select(z) %&gt;% range() ## Only plot the states with data states &lt;- map_data(&quot;state&quot;) states &lt;- states %&gt;% subset(!(region %in% c( &quot;washington&quot;, &quot;oregon&quot;, &quot;california&quot;, &quot;nevada&quot;, &quot;idaho&quot;, &quot;utah&quot;, &quot;arizona&quot;,&quot;montana&quot;, &quot;wyoming&quot;, &quot;colorado&quot;, &quot;new mexico&quot; ))) ## generate a plotting function make_plot &lt;- function() { for (i in 1:nrow(month_year)) { p &lt;- NOAA_df_1990 %&gt;% subset( day == 1 &amp; proc == &quot;Tmax&quot; &amp; month == month_year$month[i] &amp; year == month_year$year[i] ) %&gt;% ggplot(aes(x = lon, y = lat, color = z)) + geom_point(size = 2) + geom_polygon(data = states, aes(x = long, y = lat, group = group), inherit.aes = FALSE, fill = NA, color = &quot;black&quot;) + scale_color_viridis_c(option = &quot;inferno&quot;, limits = zlim) + theme( plot.title = element_text(size = rel(2.5)) ) + ggtitle( paste(&quot;Tmax for the first day of month &quot;, month_year$month[i], &quot; in &quot;, month_year$year[i], sep = &quot;&quot;) ) print(p) } } if (!file.exists(here::here(&quot;images&quot;, &quot;NOAA_df_1990-animation.gif&quot;))) { gifski::save_gif( make_plot(), gif_file = here::here(&quot;images&quot;, &quot;NOAA_df_1990-animation.gif&quot;), progress = FALSE, delay = 0.5, height = 360, width = 640, units = &quot;px&quot; ) } knitr::include_graphics(here::here(&quot;images&quot;, &quot;NOAA_df_1990-animation.gif&quot;)) 4.3.3 Interactive plotting using plotly library(plotly) ## Plot Tmax for June 1991 p &lt;- NOAA_df_1990 %&gt;% subset( day == 1 &amp; proc == &quot;Tmax&quot; &amp; month == &quot;6&quot; &amp; year == &quot;1991&quot; ) %&gt;% ggplot(aes(x = lon, y = lat, color = z)) + geom_point(size = 2) + geom_polygon(data = states, aes(x = long, y = lat, group = group), inherit.aes = FALSE, fill = NA, color = &quot;black&quot;) + scale_color_viridis_c(option = &quot;inferno&quot;, limits = zlim) + theme( plot.title = element_text(size = rel(2.5)) ) + ggtitle( paste(&quot;Tmax for the first day of June, 1991&quot;) ) ## Check if the ploty map has been produced. If so, use it, otherwise ## run the code and produce the plot if (!file.exists(here::here(&quot;images&quot;, &quot;temp-june-1991.html&quot;))) { p_plotly &lt;- ggplotly(p, width = 800, height = 450) htmlwidgets::saveWidget(p_plotly, file = here::here(&quot;images&quot;, &quot;temp-june-1991.html&quot;)) } else { # htmlwidgets::saveWidget() htmltools::includeHTML(here::here(&quot;images&quot;, &quot;temp-june-1991.html&quot;)) } plotly .container-fluid.crosstalk-bscols { margin-left: -30px; margin-right: -30px; white-space: normal; } body > .container-fluid.crosstalk-bscols { margin-left: auto; margin-right: auto; } .crosstalk-input-checkboxgroup .crosstalk-options-group .crosstalk-options-column { display: inline-block; padding-right: 12px; vertical-align: top; } @media only screen and (max-width:480px) { .crosstalk-input-checkboxgroup .crosstalk-options-group .crosstalk-options-column { display: block; padding-right: inherit; } } slide:not(.current) .plotly.html-widget{ display: none; } "],
["day-5.html", "5 Day 5 5.1 Announcements 5.2 Spatial means and covariances", " 5 Day 5 5.1 Announcements HW Assignment Another datacamp coming soon library(tidyverse) library(fields) library(mvnfast) library(plotly) library(splines) set.seed(404) 5.2 Spatial means and covariances Let \\(\\{ y(\\mathbf{s}_i) \\}\\) be a set of observations of a process at locations \\(\\{ \\mathbf{s}_i \\in \\mathcal{D}, i = 1, \\ldots, n \\}\\). In one dimension, \\(y(\\mathbf{s})\\) is a curve n &lt;- 100 s &lt;- seq(0, 1, length = n) ## calculate the pairwise distance between locations ## rdist from the fields package is much faster than the dist function D &lt;- rdist(s, s) Sigma &lt;- exp( - D) dat &lt;- data.frame( s = s, y = c(rmvn(1, mu = rep(0, n), sigma = Sigma)) ) dat %&gt;% ggplot(aes(x = s, y = y)) + geom_line() + ylab(&quot;y(s)&quot;) In two dimensions, \\(y(\\mathbf{s})\\) is a surface n &lt;- 20^2 s &lt;- expand.grid( seq(0, 1, length = sqrt(n)), seq(0, 1, length = sqrt(n)) ) ## calculate the pairwise distance between locations ## rdist from the fields package is much faster than the dist function D &lt;- rdist(s, s) Sigma &lt;- exp( - D) dat &lt;- data.frame( s1 = s[, 1], s2 = s[, 2], y = c(rmvn(1, mu = rep(0, n), sigma = Sigma)) ) plot_ly( z = ~matrix(dat$y, sqrt(n), sqrt(n)) ) %&gt;% add_surface() 5.2.1 Gaussian processes A Gaussian process is an infinite-dimensional function (the function is defined for inginitely many locations \\(\\mathbf{s} \\in \\mathcal{D}\\)) with the property that the finite-dimensional vector \\(\\mathbf{y}(\\mathbf{s}) = (y(\\mathbf{s}_1), \\ldots, y(\\mathbf{s}_n) )&#39;\\) at any finite subset of locations \\(\\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\in \\mathcal{D}\\) has a multivariate Gaussian distribution. (A good book is available free online here: http://www.gaussianprocess.org/gpml/) 5.2.1.1 Mean and covariance A univariate normal distribution is fully characterized by a mean \\(\\mu\\) and a variance \\(\\sigma^2\\). A multivariate normal distribution is fully characeterized by a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\boldsymbol{\\Sigma}\\). The mean is an \\(n\\)-dimensional vector with \\[\\begin{align*} E\\left( y(\\mathbf{s}) \\right) = \\boldsymbol{\\mu}(\\mathbf{s}) = \\begin{pmatrix} \\mu(\\mathbf{s}_1) \\\\ \\vdots \\\\ \\mu(\\mathbf{s}_n) \\end{pmatrix} \\end{align*}\\] The covariance matrix is an \\(n \\times n\\) matrix with \\[\\begin{align*} \\operatorname{Cov} \\left( y(\\mathbf{s}) \\right) &amp; = \\begin{bmatrix} \\operatorname{Var} \\left( y(\\mathbf{s}_1) \\right) &amp; \\operatorname{Cov} \\left( y(\\mathbf{s}_1), y(\\mathbf{s}_2) \\right) &amp; \\cdots &amp; \\operatorname{Cov} \\left( y(\\mathbf{s}_1), y(\\mathbf{s}_n) \\right) \\\\ \\operatorname{Cov} \\left( y(\\mathbf{s}_2), y(\\mathbf{s}_1) \\right) &amp; \\operatorname{Var} \\left( y(\\mathbf{s}_2) \\right) &amp; \\cdots &amp; \\operatorname{Cov} \\left( y(\\mathbf{s}_2), y(\\mathbf{s}_n) \\right) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\operatorname{Cov} \\left( y(\\mathbf{s}_n), y(\\mathbf{s}_1) \\right) &amp; \\operatorname{Cov} \\left( y(\\mathbf{s}_n), y(\\mathbf{s}_2) \\right) &amp; \\cdots &amp; \\operatorname{Var} \\left( y(\\mathbf{s}_n) \\right) \\\\ \\end{bmatrix} \\end{align*}\\] Recall that the multivariate normal pdf is \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}] &amp; = (2 \\pi)^{-\\frac{n}{2}} |\\boldsymbol{\\Sigma}|^{-\\frac{1}{2}} e^{-\\frac{1}{2} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)&#39; \\boldsymbol{\\Sigma}^{-1} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)} \\end{align*}\\] Define the precision matrix \\(\\boldsymbol{\\Omega} = \\boldsymbol{\\Sigma}^{-1}\\). Then, the multivariate normal pdf can be written as \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Omega}] &amp; = (2 \\pi)^{-\\frac{n}{2}} |\\boldsymbol{\\Omega}|^{\\frac{1}{2}} e^{-\\frac{1}{2} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)&#39; \\boldsymbol{\\Omega} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)} \\end{align*}\\] 5.2.1.2 Mean and covariance functions A Gaussian process is fully characterized by a mean function \\(E\\left( y(\\mathbf{s}) \\right) = \\mu(\\mathbf{s})\\) that maps \\(\\mathcal{R}^d \\rightarrow \\mathcal{R}^1\\) (for a \\(d\\)-dimensional location \\(\\mathbf{s}\\) – typically \\(d=2\\)) and a covariance function \\(\\operatorname{Cov} \\left( y(\\mathbf{s}_i), y(\\mathbf{s}_j) \\right) = C(\\mathbf{s}, \\mathbf{s}&#39;)\\). This means that once you know the mean function \\(\\mu(\\mathbf{s})\\) and the covariance function \\(C(\\mathbf{s}, \\mathbf{s}&#39;)\\) you have full knowledge of the distribution Note: this is different than a multivariate normal distribution as this is an infinite-dimensional function – cannot be represented with a vector and/or matrix. Any finite realization of a GP has the pdf \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}] &amp; = (2 \\pi)^{-\\frac{n}{2}} |\\boldsymbol{\\Sigma}|^{-\\frac{1}{2}} e^{-\\frac{1}{2} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)&#39; \\boldsymbol{\\Sigma}^{-1} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)} \\end{align*}\\] where \\(\\boldsymbol{\\mu}\\) is determined by the function \\(\\mu(\\cdot)\\) and \\(\\boldsymbol{\\Sigma}\\) is determined by the function \\(C(\\cdot, \\cdot)\\). 5.2.1.3 The Gaussian process mean function There are many possible valid choices for the mean function \\(\\mu(\\mathbf{s})\\) (almost any possible function is allowed). Constant function: \\(\\mu(\\mathbf{s}) \\equiv \\beta_0\\) Spatial covariates: \\(\\mu(\\mathbf{s}) \\equiv \\mathbf{X}(\\mathbf{s}) \\boldsymbol{\\beta} = \\beta_0 + \\sum_{j=1}^p x_j(\\mathbf{s}) \\beta_j\\) Examples: elevation, distance to water, latitude Linear spatial trends: \\(\\mu(\\mathbf{s}_i) \\equiv \\beta_0 + \\beta_1 s_{i1} + \\beta_2 s_{i2}\\) Higher-order spatial trends: \\(\\mu(\\mathbf{s}_i) \\equiv \\sum_{j=1}^p f_j(\\mathbf{s}) \\beta_j\\) where \\(f_j(\\mathbf{s})\\) is some function of location \\(\\mathbf{s}\\) (i.e., B-splines, Fourier bases, wavelets, etc.) How to choose: AIC / BIC / cross-validation 5.2.1.4 Example n &lt;- 1000 X &lt;- seq(0, 1, length = n) X_bs &lt;- bs(X, df = 10) beta &lt;- rnorm(ncol(X_bs)) y &lt;- X * 2 + X_bs %*% beta + rnorm(n, 0, 0.25) dat &lt;- data.frame(X = X, y = y, mu = X * 2 + X_bs %*% beta) dat %&gt;% ggplot(aes(x = X, y = y)) + geom_point() + geom_line(aes(x = X, y = mu), color = &quot;red&quot;) A simple mean structure can leave behind a strong residual covariance structure dat$simple &lt;- predict(lm(y ~ X)) dat$simple_resids &lt;- resid(lm(y ~ X)) dat %&gt;% ggplot(aes(x = X, y = y)) + geom_point() + geom_line(aes(x = X, y = mu), color = &quot;red&quot;, lwd = 2) + geom_line(aes(x = X, y = simple), color = &quot;blue&quot;, lwd = 2) + ggtitle(&quot;Simple linear fit&quot;) + theme(plot.title = element_text(size = 30)) dat %&gt;% ggplot(aes(x = X, y = simple_resids)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, lwd = 2) + ggtitle(&quot;Simple linear fit residuals&quot;) + theme(plot.title = element_text(size = 30)) A complex mean structure can lead to independent residuals dat$complex &lt;- predict(lm(y ~ X + X_bs)) dat$complex_resids &lt;- resid(lm(y ~ X + X_bs)) dat %&gt;% ggplot(aes(x = X, y = y)) + geom_point() + geom_line(aes(x = X, y = mu), color = &quot;red&quot;, lwd = 2) + geom_line(aes(x = X, y = complex), color = &quot;blue&quot;, lwd = 2) + ggtitle(&quot;Complex spline fit&quot;) + theme(plot.title = element_text(size = 30)) dat %&gt;% ggplot(aes(x = X, y = complex_resids)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, lwd = 2) + ggtitle(&quot;Complex spline fit residuals&quot;) + theme(plot.title = element_text(size = 30)) How do you interpret this residual correlation? Missing covariates that have a spatial pattern For example, what if you are modeling temperature in a mountainous region and don’t include elevation as a covariate? Advection/diffusion processes Example: the wind blowing, the spread of disease 5.2.1.5 Gaussian Process Covariance Functions Unlike the mean functions, only specific covariance functions are valid. The covaraince function at a finite subset of \\(n\\) points is called the covariance matrix. For a covariance matrix to be from a valid covariance function, the covariance matrix \\(\\boldsymbol{\\Sigma}\\) must be symmetric and positive-definite. A matrix \\(\\boldsymbol{\\Sigma}\\) is symmetric if \\(\\boldsymbol{\\Sigma}&#39; = \\boldsymbol{\\Sigma}\\) A matrix \\(\\boldsymbol{\\Sigma}\\) is positive definite iff and only if \\[\\begin{align*} \\boldsymbol{\\Sigma} \\mbox{ is positive definite } &amp; \\iff \\\\ \\mathbf{z}&#39; \\boldsymbol{\\Sigma} \\mathbf{z} \\geq 0 \\hspace{1em} \\forall \\mathbf{z} \\in \\mathcal{R}^n &amp; \\iff \\\\ \\mbox{all eigenvalues of } \\boldsymbol{\\Sigma} \\mbox{ are strictly positive} &amp; \\iff \\\\ |\\boldsymbol{\\Sigma}| &gt; 0 \\end{align*}\\] Therefore, the covariance fuction \\(C(\\mathbf{s}_i, \\mathbf{s}_j)\\) is a valid covariance function if the \\(n \\times n\\) covariance matrix at any finite collection of \\(n\\) locations \\(\\mathbf{s}_1, \\ldots, \\mathbf{s}_n\\) has the properties symmetry: \\(C(\\mathbf{s}_i, \\mathbf{s}_j) = C(\\mathbf{s}_j, \\mathbf{s}_i) \\hspace{1em} \\forall \\mathbf{s}_i, \\mathbf{s}_j\\) positive definite: \\(\\sum_{i=1}^n \\sum_{j=1}^n z_i z_j C(\\mathbf{s}_i, \\mathbf{s}_j) &gt; 0 \\hspace{1em} \\forall n, \\mathbf{s}_1, \\ldots, \\mathbf{s}_n, \\mbox{ and } z_1, \\ldots, z_n \\in \\mathcal{R}\\) proving that these properties hold is hard – Often rely on spectral methods (showing the eigenvalues of the function are all strictly positive). "],
["day-6.html", "6 Day 6 6.1 Announcements 6.2 Gaussian process assumptions 6.3 Recall Hierachical modeling", " 6 Day 6 6.1 Announcements Assigned reading: Model Selection for Geostatistical Models library(tidyverse) library(fields) library(mvnfast) library(gstat) library(sp) 6.2 Gaussian process assumptions To fit a Gaussian process, we have to make assumptions Why? – The mean and covariance functions are infinite dimensional but we only observe a finite vector \\(\\mathbf{y} = (y(\\mathbf{s}_1), \\ldots, y(\\mathbf{s}_1))&#39;\\) so we cannot fully specify the model Stationarity Strict stationarity: the probability density function is invariant to shifts \\([y(\\mathbf{s}_1), \\ldots, y(\\mathbf{s}_n)] = [y(\\mathbf{s}_1 + \\mathbf{h}), \\ldots, y(\\mathbf{s}_n + \\mathbf{h})] \\hspace{1em} \\forall \\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\in \\mathcal{D} \\mbox{ and } \\mathbf{h}\\) (such that all points shifted by the vector \\(\\mathbf{h}\\) are also in \\(\\mathcal{D}\\)) Weak stationarity: mean and covariance function are stationary \\(E\\left( y(\\mathbf{s}) \\right) = E\\left( y(\\mathbf{s} + \\mathbf{h}) \\right) = \\mu \\hspace{1em} \\forall \\mathbf{h}\\) \\(C\\left( y(\\mathbf{s}), y(\\mathbf{s} + \\mathbf{h}) \\right) = C\\left( y(\\mathbf{0}), y(\\mathbf{h}) \\right) = C(\\mathbf{h}) \\hspace{1em} \\forall \\mathbf{s}, \\mathbf{h}\\) This implies \\(C\\left( y(\\mathbf{s}_i), y(\\mathbf{s}_j) \\right) = C(\\mathbf{s}_i - \\mathbf{s}_j) = C(\\mathbf{d}_{ij})\\) where \\(\\mathbf{d}_{ij} = \\mathbf{s}_i - \\mathbf{s}_j\\) is the difference vector between location \\(\\mathbf{s}_i\\) and location \\(\\mathbf{s}_j\\) Intrinsic stationarity (weakest form) \\(Var\\left( y(\\mathbf{s} + h) - y(\\mathbf{s}) \\right)\\) depends only on \\(\\mathbf{h}\\). Note: intrinsic stationarity doesn’t imply weak stationarity. Example: Brownian motion is intrisically stationary but is not weakly stationary. Insert drawing here Can you think of a process that is not stationary? Note: for Gaussian processes, a weak stationarity implies strong stationarity. Isotropy A covariance function is isotropic if it is invariant to direction and rotation. \\(C\\left( y(\\mathbf{s}_i), y(\\mathbf{s}_j) \\right) = C(d_{ij})\\) where \\(d_{ij} = \\|\\mathbf{s}_i - \\mathbf{s}_j\\|\\) is the distance (typically Euclidean) between \\(\\mathbf{s}_i\\) and \\(\\mathbf{s}_j\\) Can you think of examples where other distances (not Euclidean) might be better? A covariance function that is not isotropic is called anisotropic Insert drawing here Can you think of a process that is anisotropic? 6.3 Recall Hierachical modeling 6.3.1 Data Model \\[\\begin{align*} y(\\mathbf{s}) &amp; = z(\\mathbf{s}) + \\varepsilon(\\mathbf{s}) \\\\ \\tag{6.1} \\varepsilon(\\mathbf{s}) \\stackrel{iid}{\\sim} N(0, \\sigma^2) \\end{align*}\\] \\(y(\\mathbf{s})\\) is the observation at site \\(\\mathbf{s}\\) \\(z(\\mathbf{s})\\) is the process of interest at site \\(\\mathbf{s}\\) \\(\\varepsilon(\\mathbf{s}) \\sim N(0, \\sigma^2)\\) is the measurement error Commonly called the nugget Geostatistics came from mining Microsite-variability What is a process that would have a small nugget? What is a process that would have a large nugget? \\(z(\\mathbf{s})\\) is the process of interest at site \\(\\mathbf{s}\\) Process Model (using Gaussian process) \\[\\begin{align*} z(\\mathbf{s}) &amp; = \\mu(\\mathbf{s}) + \\eta(\\mathbf{s}) \\tag{6.2} \\end{align*}\\] \\(\\mu(\\mathbf{s})\\) is the GP mean function \\(\\boldsymbol{\\eta} = (\\eta(\\mathbf{s}_1), \\ldots, \\eta(\\mathbf{s}_n))&#39; \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma})\\) \\(Cov(\\eta(\\mathbf{s}_i), \\eta(\\mathbf{s}_j)) = \\tau^2 C(d_{ij})\\) \\(C(d_ij)\\) is a correlation function \\(\\tau^2\\) is often called the partial sill parameter set.seed(101) n &lt;- 40 s &lt;- runif(n) ## mean function mu &lt;- s ## covariance function -- this is positive definite Sigma &lt;- 2 * exp( - rdist(s) / 2) ## generate the spatially (1-d) correlated process ## rmvn returns a matrix -- use c() to coerce to a vector eta &lt;- c(rmvn(1, rep(0, n), Sigma)) z &lt;- mu + eta epsilon &lt;- rnorm(n, 0, 0.25) y &lt;- z + epsilon dat &lt;- data.frame( s = s, y = y, z = z, mu = mu, eta = eta, epsilon = epsilon ) dat %&gt;% ggplot(aes(x = s, y = y)) + geom_point() + geom_segment(aes(xend = s, yend = z), alpha = 0.5) + geom_line(aes(x = s, y = z), color = &quot;blue&quot;) + geom_line(aes(x = s, y = mu), color = &quot;red&quot;) + geom_line(aes(x = s, y = eta), color = &quot;purple&quot;) + theme_bw() A quick check if Sigma meets the conditions of a symmetric positive-definite matrix ## check is Sigma is symmetric all.equal(Sigma, t(Sigma)) ## [1] TRUE ## check is Sigma is posivitive definite Sigma_eigen &lt;- eigen(Sigma) ## check that all eigenvalues are positive Sigma_eigen$values ## [1] 68.614998226 6.469640157 1.846887044 1.013225060 0.526987113 ## [6] 0.350879290 0.221671629 0.170657210 0.116756563 0.076631073 ## [11] 0.067533590 0.058658782 0.053014720 0.043488483 0.037433411 ## [16] 0.036668397 0.032544320 0.030261646 0.027404563 0.023218999 ## [21] 0.021154516 0.021154380 0.017782847 0.016717572 0.014563450 ## [26] 0.013423514 0.010922956 0.009497946 0.008886870 0.008231217 ## [31] 0.007208472 0.005444425 0.005026785 0.004599434 0.004534839 ## [36] 0.004238142 0.003219602 0.002366398 0.001348648 0.001117710 all(Sigma_eigen$values &gt; 0) ## [1] TRUE Variance decomposition Combining the equations (6.1) and (6.2) \\[\\begin{align*} y(\\mathbf{s}) &amp; = \\mu(\\mathbf{s}) + \\eta(\\mathbf{s}) + \\varepsilon(\\mathbf{s}) \\end{align*}\\] we can have \\[\\begin{align*} Var \\left( y(\\mathbf{s}) \\right) &amp; = Var \\left( \\mu(\\mathbf{s}) + \\eta(\\mathbf{s}) + \\varepsilon(\\mathbf{s}) \\right) \\\\ &amp; = Var \\left( \\mu(\\mathbf{s}) \\right) + Var \\left( \\eta(\\mathbf{s}) \\right) + Var \\left( \\varepsilon(\\mathbf{s}) \\right) + \\\\ &amp; \\hspace{2em} 2 Cov \\left( \\mu(\\mathbf{s}), \\varepsilon(\\mathbf{s}) \\right) + 2 Cov \\left( \\mu(\\mathbf{s}), \\eta(\\mathbf{s}) \\right) + 2 Cov \\left( \\eta(\\mathbf{s}), \\varepsilon(\\mathbf{s}) \\right) \\\\ &amp; = 0 + Var \\left( \\eta(\\mathbf{s}) \\right) + Var \\left( \\varepsilon(\\mathbf{s}) \\right) + 0 + 0 + 0 \\\\ &amp; = \\tau^2 + \\sigma^2 \\end{align*}\\] the total variance is the sill \\(\\tau^2 + \\sigma^2\\) "],
["day-7.html", "7 Day 7 7.1 Common isotropic correlation functions 7.2 Covariograms and semivariograms 7.3 Estimation of the spatial process 7.4 Maximum likelihood", " 7 Day 7 library(tidyverse) library(fields) library(mvnfast) library(gstat) library(sp) library(MCMCpack) 7.1 Common isotropic correlation functions Tobler’s law of geography “Everything is related to everything else, but near things are more related than distant things” These functions follow Tobler’s law in that the function decays with distance These functions are proven to be symmetric and positive definite, thus are valid correlation functions ## make a function to plot the correlation functions plot_corr_function &lt;- function(corr_fun, ## notice that this input is a function d = seq(0, 10, length.out = 1000), phi = c(0.1, 0.5, 1, 5, 10), title = NULL, ...) { C_h &lt;- matrix(0, length(d), length(phi)) for (i in 1:length(phi)) { C_h[, i] &lt;- corr_fun(d, phi[i], ...) } print( data.frame(d = d, C_h = c(C_h), phi = factor(rep(phi, each = length(d)))) %&gt;% ggplot(aes(x = d, y = C_h, group = phi, color = phi)) + geom_line() + ylim(c(0, 1)) + ggtitle(title) ) } 7.1.0.1 Exponential correlation function \\[\\begin{align*} C(d) &amp; = e^{- \\frac{d}{\\phi} } \\end{align*}\\] where \\(\\phi\\) is the spatial range parameter (called the length-scale in Gaussian process literature). Note that this function is not differentiable at 0. exponential_cor &lt;- function (d, phi, ...) { return(exp( - d / phi)) } plot_corr_function(exponential_cor, title = &quot;Exponential correlation function&quot;) Sometimes this is parameterized using the inverse spatial range \\(\\theta = \\frac{1}{\\phi}\\) \\[\\begin{align*} C(d) &amp; = e^{- d\\theta} \\end{align*}\\] 7.1.0.2 Squared exponential (Gaussian) correlation functions \\[\\begin{align*} C(d) &amp; = e^{- (\\frac{d}{\\phi})^2 } \\end{align*}\\] Notice that this function is differentiable at 0. gaussian_cor &lt;- function (d, phi, ...) { return(exp( - (d / phi)^2)) } plot_corr_function(gaussian_cor, title = &quot;Gaussian (squared exponential) correlation function&quot;) 7.1.0.3 Powered exponential correlation functions \\[\\begin{align*} C(d) &amp; = e^{- (\\frac{d}{\\phi})^k } \\end{align*}\\] powered_exp_cor &lt;- function (d, phi, k, ...) { return(exp( - (d / phi)^k)) } plot_corr_function(powered_exp_cor, k = 1, title = &quot;Powered exponential correlation function, k = 1&quot;) plot_corr_function(powered_exp_cor, k = 2, title = &quot;Powered exponential correlation function, k = 2&quot;) plot_corr_function(powered_exp_cor, k = 3, title = &quot;Powered exponential correlation function, k = 3&quot;) plot_corr_function(powered_exp_cor, k = 4, title = &quot;Powered exponential correlation function, k = 4&quot;) For future classes (basis representations) Visual exploration of Gaussian Processes 7.1.0.4 Matern correlation functions \\[\\begin{align*} C(d) &amp; = \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{2 \\nu} \\frac{d}{\\phi} \\right)^\\nu K_\\nu \\left( \\sqrt{2 \\nu} \\frac{d}{\\phi} \\right) \\end{align*}\\] \\(\\Gamma(\\cdot)\\) is the gamma function \\(K_\\nu(\\cdot)\\) is the modified Bessel function of the second kind \\(\\phi\\) is the range parameter \\(\\nu\\) is the smoothness parameter ## Use the Matern fucnction from the fields library plot_corr_function(Matern, range = c(0.1, 0.5, 1, 5, 10), smoothness = 0.15, title = &quot;Matern correlation function, smoothness = 0.15&quot;) plot_corr_function(Matern, range = c(0.1, 0.5, 1, 5, 10), smoothness = 0.5, title = &quot;Matern correlation function, smoothness = 0.5&quot;) plot_corr_function(Matern, range = c(0.1, 0.5, 1, 5, 10), smoothness = 5, title = &quot;Matern correlation function, smoothness = 5&quot;) plot_corr_function(Matern, range = c(0.1, 0.5, 1, 5, 10), smoothness = 50, title = &quot;Matern correlation function, smoothness = 50&quot;) Special Cases \\(\\nu = \\frac{1}{2} \\Rightarrow C(d) = e^{- \\frac{d}{\\phi} }\\) is the exponential correlation function \\(\\nu = \\frac{3}{2} \\Rightarrow C(d) = \\left(1 - \\frac{\\sqrt{3}d}{\\phi} \\right) e^{- \\frac{\\sqrt{3}d}{\\phi} }\\) \\(\\nu = \\frac{5}{2} \\Rightarrow C(d) = \\left(1 - \\frac{\\sqrt{5}d}{\\phi} + \\frac{5 d^2}{3 \\phi^2} \\right) e^{- \\frac{\\sqrt{5}d}{\\phi} }\\) \\(\\nu \\rightarrow \\infty \\Rightarrow C(d) = e^{- \\frac{1}{2} \\left( \\frac{d}{\\phi} \\right)^2 }\\) is the Gaussian correlation function Note: these functions are valid in \\(\\mathcal{R}^2\\). There are generalizations of the functions to other geometries (spheres, stream networks, etc.) and higher dimensions (\\(\\mathcal{R}^d\\)). The Gaussian process with a Matern correlation function with parameter \\(\\nu\\) is \\(\\lceil \\nu \\rceil\\)-1 times differentiable in the mean-square sense d &lt;- seq(0, 10, length = 1000) nu &lt;- c(1/2, 3/2, 5/2) C_h &lt;- c( Matern(d, range = 1, nu = nu[1]), Matern(d, range = 1, nu = nu[2]), Matern(d, range = 1, nu = nu[3]) ) dat &lt;- data.frame( d = d, C_h = C_h, nu = factor(rep(nu, each = length(d))) ) ggplot(dat, aes(x = d, y = C_h, group = nu, color = nu)) + geom_line() + ggtitle(&quot;Matern correlation functions&quot;) ## simulate some Gaussian processes dat$y &lt;- c( rmvn( 1, mu = rep(0, 1000), sigma = Matern(rdist(seq(0, 10, length = 1000)), range = 1, nu = nu[1]) ), rmvn( 1, mu = rep(0, 1000), sigma = Matern(rdist(seq(0, 10, length = 1000)), range = 1, nu = nu[2]) ), rmvn( 1, mu = rep(0, 1000), sigma = Matern(rdist(seq(0, 10, length = 1000)), range = 1, nu = nu[3]) ) ) ggplot(dat, aes(x = d, y = y, group = nu, color = nu)) + geom_line() + ggtitle(&quot;Gaussian process realizations&quot;) 7.2 Covariograms and semivariograms How do we choose a covariance function? How do we fit a covariance function? How do we check for isotropy? 7.2.1 Semivariograms and variograms The semivariogram is defined \\[\\begin{align*} \\gamma(\\mathbf{s}_i, \\mathbf{s}_j) &amp; \\equiv \\frac{1}{2} Var(y(\\mathbf{s}_i) - y(\\mathbf{s}_j)) \\\\ &amp; E\\left( \\left( \\left( y(\\mathbf{s}_i) - \\mu(\\mathbf{s}_i) \\right) - \\left( y(\\mathbf{s}_j) - \\mu(\\mathbf{s}_j) \\right) \\right)^2 \\right) \\end{align*}\\] If the covaraince is stationary this can be written as a function of directional spatial lags \\(\\mathbf{h}_{ij} = \\mathbf{s}_i - \\mathbf{s}_j\\) (e.g., \\(\\mathbf{h}_{ij}\\) is not required to equal \\(\\mathbf{h}_{ji}\\)). \\[\\begin{align*} \\gamma(\\mathbf{h}) &amp; \\equiv \\frac{1}{2} Var(y(\\mathbf{s} + \\mathbf{h}) - y(\\mathbf{s})) \\\\ &amp; = \\frac{1}{2} E\\left( \\left( \\left( y(\\mathbf{s} + \\mathbf{h}) - \\mu(\\mathbf{s} + \\mathbf{h}) \\right) - \\left( y(\\mathbf{s}) - \\mu(\\mathbf{s}) \\right) \\right)^2 \\right) \\\\ &amp; \\frac{1}{2} E\\left( \\left( y(\\mathbf{s} + \\mathbf{h}) - \\mu(\\mathbf{s} + \\mathbf{h}) \\right)^2 \\right) - E\\left( \\left( y(\\mathbf{s} + \\mathbf{h}) - \\mu(\\mathbf{s} + \\mathbf{h}) \\right) \\left( y(\\mathbf{s}) - \\mu(\\mathbf{s}) \\right) \\right) + \\frac{1}{2} E\\left( \\left( y(\\mathbf{s}) - \\mu(\\mathbf{s}) \\right) \\right) \\\\ &amp; = C(\\mathbf{0}) - C(\\mathbf{h}) \\end{align*}\\] This implies \\[\\begin{align*} \\gamma(\\mathbf{h}) &amp; = C(\\mathbf{0}) - C(\\mathbf{h}) \\\\ &amp; \\mbox{or} \\\\ C(\\mathbf{h}) &amp; = C(\\mathbf{0}) - \\gamma(\\mathbf{h}) \\end{align*}\\] The variogram is defined as \\(2 \\gamma(\\mathbf{h})\\). 7.2.1.1 Properties of variograms \\(\\gamma(\\mathbf{s}_i, \\mathbf{s}_j) \\geq 0\\) because it is the expectation of a square \\(\\gamma(\\mathbf{s}_i, \\mathbf{s}_i) = \\gamma_i(0) = 0\\) because \\(y(\\mathbf{s}_1) - y(\\mathbf{s}_1) = 0\\) insert theoretical variogram plot from class here that shows the curve, nugget, sill, and range 7.2.1.2 Estimation of variograms It is difficult to estimate the variogram as there is no replication across space – need to pool information across space. 7.2.1.3 Non-directional variograms Using the meuse dataset of heavy metal concntrations in the topsoil near the Meuse river near Stein, NL, we can explore the concentration of zinc (log-scale). data(&quot;meuse&quot;) data(&quot;meuse.grid&quot;) coordinates(meuse) = ~ x + y bubble(meuse, &quot;zinc&quot;) Let’s explore the correlation in the data with respect to the log(zinc) variable vg &lt;- variogram(log(zinc) ~ 1, data = meuse) fit_vg &lt;- fit.variogram(vg, model = vgm(1, &quot;Exp&quot;)) plot(vg, fit_vg, as.table = TRUE) 7.2.1.4 Checking for anisotropy – Directional variograms Let’s explore the directional correlation in the data with respect to the log(zinc) variable dir_variogram &lt;- variogram(log(zinc) ~ 1, data = meuse, alpha = c(0, 45, 90, 135)) fit_variogram &lt;- fit.variogram(dir_variogram, model = vgm(1, &quot;Exp&quot;)) plot(dir_variogram, fit_variogram, as.table = TRUE) You can explore this variogram as a map map_variogram &lt;- variogram(log(zinc) ~ 1, data = meuse, cutoff = 1500, width = 100, map = TRUE) plot(map_variogram, threshold = 5) which appears to show some patterns in the variogram maps suggesting anisotropy. Perhaps there is a covariate that can explain this. Let’s use the square-root distance to the river plot(log(zinc) ~ sqrt(dist), meuse) abline(lm(log(zinc) ~ sqrt(dist), meuse), col = &quot;red&quot;) Now, we can look at the directional varigrams of the residuals in the model after accounting for square-root distance to the river dir_variogram &lt;- variogram(log(zinc) ~ sqrt(dist), data = meuse, alpha = c(0, 45, 90, 135)) fit_variogram &lt;- fit.variogram(dir_variogram, model = vgm(1, &quot;Exp&quot;)) plot(dir_variogram, fit_variogram, as.table = TRUE) where these variograms seem to fit the data better. You can explore this variogram as a map map_variogram &lt;- variogram(log(zinc) ~ sqrt(dist), data = meuse, cutoff = 1500, width = 100, map = TRUE) plot(map_variogram, threshold = 5) which shows that after modeling log(zinc) as a function of the square-root distance to the river, the residuals appear isotropic left in the residuals. 7.3 Estimation of the spatial process Assume we have the model \\[\\begin{align*} y(\\mathbf{s}) &amp; = \\mu(\\mathbf{s}) + \\eta(\\mathbf{s}) + \\varepsilon(\\mathbf{s}) \\end{align*}\\] then, \\(Cov \\left( y(\\mathbf{s}_i), y(\\mathbf{s}_j ) \\right) = \\sigma^2 C(d_{ij} | \\nu, \\phi) + \\tau^2 I\\{i = j\\}\\) where \\(C(d_{ij} | \\nu, \\phi)\\) is a Matern correlation function with smoothness parameter \\(\\nu\\) and range parameter \\(\\phi\\). We observe the data \\(\\mathbf{y} = (y(\\mathbf{s}_1), \\ldots, y(\\mathbf{s}_n))&#39;\\) at \\(n\\) locations \\(\\mathbf{s}_1, \\ldots, \\mathbf{s}_n\\). \\(\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta}))\\). \\(\\boldsymbol{\\mu} = \\mathbf{X}\\left( \\mathbf{s} \\right) \\boldsymbol{\\beta}\\) is the model for the mean process given the \\(n \\times p\\) covariate matrix \\(\\mathbf{X}(\\mathbf{s})\\) and \\(\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})\\) is the covariance matrix with parameters \\(\\boldsymbol{\\theta} = (\\tau^2, \\sigma^2, \\nu, \\phi)&#39;\\). To fit the model, we need to estimate \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\theta}\\). Traditional statistical methods: use replication With \\(k = 1, \\ldots, K\\) replications of the spatial process \\(\\mathbf{y}_k\\), we can estimate the spatial mean as \\[\\begin{align*} \\widehat{\\boldsymbol{\\mu}} = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{y}_k \\end{align*}\\] and the spatial mean as \\[\\begin{align*} \\widehat{\\boldsymbol{\\Sigma}} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{y}_k - \\widehat{\\boldsymbol{\\mu}} \\right) \\left( \\mathbf{y}_k - \\widehat{\\boldsymbol{\\mu}} \\right)&#39; \\end{align*}\\] However, we don’t have replication – we only have the single observation \\(\\mathbf{y}\\). We will explore different estimation methods using 1) variograms, 2) maximum liklihood, and 3) Bayesian methods 7.3.1 Estimation of the spatial process using variograms First, fit a model to the mean to estimate \\(\\hat{\\mu}(\\mathbf{s})\\) (use maximum likelihood, least-squares, etc.) Next, generate a sequence of \\(B\\) bins based on distance and group each pair of points \\((\\mathbf{s}_i, \\mathbf{s}_j)\\) into a bin Example bins: [0, 1), [1, 2), [2, 3), Place the pair of observations \\(\\mathbf{s}_i\\) and \\(\\mathbf{s}_j\\) that are seperated by \\(d_{b} \\in [d_b - \\epsilon, d_b + \\epsilon)\\) into one of the \\(B\\) bins. Calculate the average of the variogram within each bin For each of the \\(k\\) bins that have \\(m_k\\) points in each bin, the variogram estimate for bin \\(k\\) centered at the bin interval \\(\\mathbf{h}_k\\) is \\[\\begin{align*} \\hat{\\gamma}(\\mathbf{h}_k) = \\frac{1}{m_k} \\sum_{\\ell=1}^{m_k} \\left( y(\\mathbf{s}_{\\ell_1}) - y(\\mathbf{s}_{\\ell_2}) \\right) \\end{align*}\\] for the \\(\\ell\\)th pair of locations \\(\\mathbf{s}_{\\ell_1}\\) and \\(\\mathbf{s}_{\\ell_2}\\) insert empirical variogram plot from class here Can estimate the parameters “by eye” or using least squares \\[\\begin{align*} \\hat{\\boldsymbol{\\theta}} &amp; = (\\hat{\\tau}^2, \\hat{\\sigma}^2, \\hat{\\phi}, \\hat{\\nu})&#39; \\\\ &amp; = \\underset{\\tau^2, \\sigma^2, \\phi, \\nu}{\\operatorname{argmax}} \\sum_{b=1}^B \\left( \\hat{\\gamma}(d_b) - \\gamma(d_b)\\right)^2 w_b \\\\ &amp; = \\underset{\\tau^2, \\sigma^2, \\phi, \\nu}{\\operatorname{argmax}} \\sum_{b=1}^B \\left( \\hat{\\gamma}(d_b) - \\left( \\sigma^2 + \\tau^2 C \\left( d_b | \\phi, \\nu \\right) \\right) \\right)^2 w_b \\tag{7.1} \\end{align*}\\] given the correlation function \\(C \\left( d_b | \\phi, \\nu \\right)\\) and a set of weights \\(w_b\\). 7.3.1.1 Estimation of the mean function What is the least squares estimator of the mean function? Recall, if \\(\\mathbf{y} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma})\\), then \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X} \\mathbf{y}\\) is an unbiased estimator. \\[\\begin{align*} E(\\hat{\\boldsymbol{\\beta}}) &amp; = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X} E(\\mathbf{y}) \\\\ &amp; = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}\\mathbf{X} \\boldsymbol{\\beta} \\\\ &amp; = \\boldsymbol{\\beta} \\end{align*}\\] However, \\[\\begin{align*} Cov(\\hat{\\boldsymbol{\\beta}}) &amp; = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X} \\boldsymbol{\\Sigma} \\mathbf{X} (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\\\ &amp; \\neq \\sigma^2 (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\end{align*}\\] which is the least squares covariance estimate of \\(\\hat{\\boldsymbol{\\beta}}\\). Thus the least squares estimate has a biased covariance estimate. Given the fitted covariance matrix \\(\\hat{\\boldsymbol{\\Sigma}}\\) from the variogram, an updated mean function estimate is \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} &amp; = (\\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{X})^{-1} \\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{y} \\end{align*}\\] and the covariance is \\[\\begin{align*} Cov(\\hat{\\boldsymbol{\\beta}}) &amp; = (\\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{X})^{-1} \\end{align*}\\] This suggests that an iterative approach can be used to fit the model estimate the mean function using the estimated mean function, update the covariance function repeat steps 1 and 2 until convergence Any issues? Uncertainty estimation? How do you propogate parameter uncertainty? Prediction uncertainty is likely to be too small 7.3.2 Maximum Likelihood 7.3.2.1 The likelihood To understand the benefits of Bayesian analysis, it is useful to recall the likelihood framework. Assume that our model can be simplified so that the data are \\(\\mathbf{y} = (y_1, \\ldots, y_N)&#39;\\) and the parameters are \\(\\boldsymbol{\\theta}\\). Before observation, the data \\(\\mathbf{y}\\) are considered a random variable. After observation, the data \\(\\mathbf{y}\\) are considered fixed and known. The likelihood is a formal representation of how likely the data \\(\\mathbf{y}\\) are to arise given a probability distribution with parameters \\(\\boldsymbol{\\theta}\\). The likelihood function \\(L(\\boldsymbol{\\theta} | \\mathbf{y})\\) is defined as \\[\\begin{align*} L(\\boldsymbol{\\theta} | \\mathbf{y}) &amp; = [\\mathbf{y} | \\boldsymbol{\\theta}], \\end{align*}\\] where the function says that the likelihood of the parameter conditional on the data is is the conditional probability density of the data conditional on the parameters. If one assumes the observations are conditionally independent given the parameters \\(\\boldsymbol{\\theta}\\), the likelihood can be written as \\[\\begin{align*} L(\\boldsymbol{\\theta} | \\mathbf{y}) &amp; = \\prod_{i=1}^N [y_i | \\boldsymbol{\\theta}], \\end{align*}\\] Note the distinction between the use of the term likelihood function for the left side of the equation with the use of the term likelihood in Bayesian statistics to describe the right side of the above equation. A distinction between the likelihood function and the probability distribution function is in what is considered the random variable. In the likelihood function, the data \\(\\mathbf{y}\\) is assumed known and the parameters \\(\\boldsymbol{\\theta}\\) are random. For the probability distribution function, the data \\(\\mathbf{y}\\) are considered are random variable conditional on a fixed, known parameter \\(\\boldsymbol{\\theta}\\). To illustrate this difference, consider a univariate gamma probability distribution with shape parameter \\(\\alpha = 10\\) and scale parameter \\(\\theta=10\\). If we observe a single \\(y=2\\), the following figure shows the density function and likelihood function, with \\(\\alpha\\) assumed fixed and known. library(latex2exp) ## ## Attaching package: &#39;latex2exp&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## TeX y &lt;- 2 alpha &lt;- 10 theta &lt;- 10 density_function &lt;- function (x) { return(dgamma(x, alpha, theta)) } likelihood_function &lt;- function (x) { return(dgamma(y, alpha, x)) } layout(matrix(1:2, 2, 1)) ## plot density function curve(density_function(x), 0, 4, main = &quot;Density function&quot;, xlab=&quot;y&quot;, ylab=TeX(&quot;$\\\\lbrack$y|$\\\\theta$ $\\\\rbrack$&quot;)) text(1, 0.4, paste0( &quot;Area = &quot;, round(integrate(density_function, 0, Inf)[1]$value, digits=2))) points(2, density_function(2), col=&quot;red&quot;, pch=16) curve(likelihood_function(x), 0, 10, main = &quot;Likelihood function&quot;, xlab=TeX(&quot;$\\\\theta$&quot;), ylab=TeX(&quot;$L(y|\\\\theta)$&quot;)) text(5, 0.3, paste0( &quot;Area = &quot;, round(integrate(likelihood_function, 0, Inf)[1]$value, digits=2))) points(10, likelihood_function(10), col=&quot;red&quot;, pch=16) Notice that the area under the curve of the density function is 1 (because it is a formal probability distribution) whereas the area under the likelihood function is not 1 (the area is 10). Hence, when performing optimization using a likelihood function, one is not optimizing a probability function. Because the likelihood is not a probability, we appeal to frequentist (rather than probabilistic) interpretations when interpreting likelihood analyses. For example, the interpretation of a 95% confidence interval is “under repeated sampling from the population, 95% of the confidence intervals will contain the true value,” in comparison to the probabilistic interpretation “the probability the interval contains the true value is 95%.” The density function and the likelihood function share a common point at \\(y=2\\) and \\(\\theta=10\\), (shown in red in the figure above) given the fixed value of \\(\\alpha\\). This suggests that the likelihood and density functions are the same only when the parameter is assumed to be a fixed, known value. ## check that the density function and the likelihood function share the same point all.equal(density_function(2), likelihood_function(10)) ## [1] TRUE Philosophically, there is a subtle difference between the density function and the likelihood function that is important to understand. In the likelihood function, we allow the parameter \\(\\theta\\) to vary; however, we do not assume that \\(\\theta\\) is a random variable. To be a random variable there must be a formal probability distribution for \\(\\theta\\). We showed earlier that the likelihood function does not integrate to 1 so we don’t view \\(\\theta\\) as a random variable and the likelihood function is not a probability distribution for \\(\\theta\\). 7.4 Maximum likelihood Maximum likelihood estimation has the goal of finding the set of parameters \\(\\hat{\\boldsymbol{\\theta}}\\) that were most likely to give rise to the data. Because the likelihood does not integrate to 1, the likelihood by itself is not infomative; only comparisons among likelihoods are meaningful because the likelihood can be shifted up or down in the y-axis by an arbitraty constant \\(c\\). To cancel out the unkown constant \\(c\\), we take ratios of the likelihoods at values \\(\\theta_1\\) and \\(\\theta_2\\), giving rise to the likelihood ratio \\[\\begin{align*} \\frac{L(\\theta_1 | y)}{L(\\theta_2 | y)} &amp; = \\frac{[y|\\theta_1]}{[y|\\theta_2]}. \\end{align*}\\] The likelihood ratio expresses the strength of evidence in favor of \\(\\theta_1\\) relative to \\(\\theta_2\\). In general, we use the log likelihood ratio to express the strength of evidence where positive values of the log likelihood ratio give evidence in support of \\(\\theta_1\\) and negative values of the log likelihood ratio give evidence in support of \\(\\theta_2\\), conditional on the data. The maximum likelihood estimate is the value \\(\\hat{\\theta}\\) such that \\[\\begin{align*} \\log \\left( \\frac{L(\\hat{\\theta} | y)}{L(\\theta^\\star | y)} \\right) &amp; \\geq 0 \\end{align*}\\] for all values of \\(\\theta^\\star\\). The following figure demonstrates this idea visually. layout(matrix(1)) curve(likelihood_function(x), 0, 10, main = &quot;Likelihood function&quot;, xlab=TeX(&quot;$\\\\theta$&quot;), ylab=TeX(&quot;$L(y|\\\\theta)$&quot;), ylim=c(-0.02, 0.66)) segments(3, 0, 3, likelihood_function(3)) arrows(3, likelihood_function(3), 0, likelihood_function(3)) text(3.1, -0.02, TeX(&quot;$\\\\theta_1$ = 3&quot;)) text(1.5, likelihood_function(3)+ .02, TeX(&quot;$L(y|\\\\theta_1)$&quot;)) segments(6, 0, 6, likelihood_function(6)) arrows(6, likelihood_function(6), 0, likelihood_function(6)) text(6.1, -0.02, TeX(&quot;$\\\\theta_2$ = 6&quot;)) text(1.5, likelihood_function(6) + .02, TeX(&quot;$L(y|\\\\theta_2)$&quot;)) segments(5, 0, 5, likelihood_function(5), lty=2) text(5.3, 0.66, TeX(&quot;$\\\\hat{\\\\theta}_{MLE}$ = 5&quot;)) For example, if \\(\\theta_1 = 3\\) and \\(\\theta_2 = 6\\), the log likelihood ratio is -0.9314718 which suggests evidence is in favor of \\(\\theta_2\\) relative to \\(\\theta_1\\). In comparison, the log likelihood ratio of the MLE \\(\\hat{\\theta}_{MLE}\\) is 0.1767844 which gives evidence in favor of \\(\\hat{\\theta}_{MLE}\\). 7.4.0.1 Maximum likelihood estimation of the variogram Instead of using the variogram in (7.1), we can use maximum liklihood. More formal and principled estimation framework. Allows for parameter uncertainty quantification. Goal: estimate the parameters \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\theta} = (\\tau^2, \\sigma^2, \\nu, \\phi)&#39;\\). The log-likelihood to maximize is \\[\\begin{align*} \\log[\\boldsymbol{\\beta}, \\boldsymbol{\\theta} | \\mathbf{y}] &amp; = - \\frac{1}{2} \\log|\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})| - \\frac{1}{2} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right)&#39; \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})^{-1} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) \\end{align*}\\] This is a multivariate optimization and difficult to optimize directly. Instead, notice the estimate \\(\\hat{\\boldsymbol{\\beta}}(\\boldsymbol{\\theta}) = (\\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{X})^{-1} \\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{y}\\) is a known function of \\(\\boldsymbol{\\theta}\\) so we can profile it out of the equation. Thus, we can instead optimize the profile likelihood \\[\\begin{align*} \\log[\\boldsymbol{\\theta} | \\mathbf{y}]_{prof} &amp; = - \\frac{1}{2} \\log|\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})| - \\frac{1}{2} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}(\\boldsymbol{\\theta}) \\right)&#39; \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})^{-1} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}(\\boldsymbol{\\theta}) \\right) \\end{align*}\\] Note this equation is now just a function of \\(\\boldsymbol{\\theta}\\). Computational complexity – both the log-likelihood and the profile log-likelihood requre the determinant and inverse of the \\(n \\times n\\) covariance matrix \\(\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})\\) which require \\(O(n^3)\\) time if (!file.exists(here::here(&quot;results&quot;, &quot;matrix-inverse-timings.RData&quot;))) { n &lt;- c(10, 20, 50, 100, 200, 250, 350, 500, 600, 700, 800, 900, 1000, 1250, 1500) timings &lt;- rep(0, length(n)) for (i in 1:length(n)) { Sigma &lt;- riwish(n[i]+2, diag(n[i])) timings[i] &lt;- system.time(solve(Sigma))[3] } dat &lt;- data.frame(timings = timings, n = n) save(dat, file = here::here(&quot;results&quot;, &quot;matrix-inverse-timings.RData&quot;)) } else { load(here::here(&quot;results&quot;, &quot;matrix-inverse-timings.RData&quot;)) } ## fit the best cubic model to the data ggplot(data = dat, aes (x = n, y = timings)) + geom_point(size = 2, color = &quot;red&quot;) + stat_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 3), fullrange = TRUE) + ylab(&quot;Time to calculate inverse&quot;) + xlab(&quot;matrix size (n by n)&quot;) + ggtitle(&quot;Matrix inversion time with fitted polynomial of order 3&quot;) + xlim(c(0, 2000)) Solving the MLE for \\(\\boldsymbol{\\theta}\\) Can use standard optimization routines like optim() Can use REML to guarantee positive variance parameters “restricted maximum likelihood” Induces a bias in the estimates but guarantees realistic answers (non-negative variances) Uncertainties can be estimated using the Fisher information matrix \\[\\begin{align*} \\mathcal{I}(\\boldsymbol{\\theta})_{ij} &amp; = E \\left( \\left( \\frac{\\mathcal{d}}{\\mathcal{d} \\boldsymbol{\\theta}_i} \\log [\\boldsymbol{\\theta} | \\mathbf{y} \\right) \\left( \\frac{\\mathcal{d}}{\\mathcal{d} \\boldsymbol{\\theta}_j} \\log [\\boldsymbol{\\theta} | \\mathbf{y} \\right) \\middle| \\boldsymbol{\\theta} \\right) \\\\ &amp; = - E \\left( \\left( \\frac{\\mathcal{d}^2 }{\\mathcal{d} \\boldsymbol{\\theta}_i \\mathcal{d} \\boldsymbol{\\theta}_j} \\log [\\boldsymbol{\\theta} | \\mathbf{y} \\right) \\middle| \\boldsymbol{\\theta} \\right) \\end{align*}\\] "],
["day-8.html", "8 Day 8 8.1 Announcements 8.2 Asymptotic properties of the MLE", " 8 Day 8 8.1 Announcements 8.2 Asymptotic properties of the MLE "],
["day-9.html", "9 Day 9 9.1 Announcements 9.2 Asymptotic properties of the MLE", " 9 Day 9 9.1 Announcements 9.2 Asymptotic properties of the MLE "],
["day-10.html", "10 Day 10", " 10 Day 10 "],
["day-11.html", "11 Day 11", " 11 Day 11 "],
["day-12.html", "12 Day 12", " 12 Day 12 "],
["day-13.html", "13 Day 13", " 13 Day 13 "],
["day-14.html", "14 Day 14", " 14 Day 14 "],
["day-15.html", "15 Day 15", " 15 Day 15 "],
["day-16.html", "16 Day 16", " 16 Day 16 "],
["day-17.html", "17 Day 17", " 17 Day 17 "],
["day-18.html", "18 Day 18", " 18 Day 18 "],
["day-19.html", "19 Day 19", " 19 Day 19 "],
["day-20.html", "20 Day 20", " 20 Day 20 "],
["day-21.html", "21 Day 21", " 21 Day 21 "],
["day-22.html", "22 Day 22", " 22 Day 22 "],
["day-23.html", "23 Day 23", " 23 Day 23 "],
["day-24.html", "24 Day 24", " 24 Day 24 "],
["day-25.html", "25 Day 25", " 25 Day 25 "],
["day-26.html", "26 Day 26", " 26 Day 26 "],
["day-27.html", "27 Day 27", " 27 Day 27 "],
["day-28.html", "28 Day 28", " 28 Day 28 "],
["day-29.html", "29 Day 29", " 29 Day 29 "],
["day-30.html", "30 Day 30", " 30 Day 30 "],
["day-31.html", "31 Day 31", " 31 Day 31 "],
["day-32.html", "32 Day 32", " 32 Day 32 "],
["day-33.html", "33 Day 33", " 33 Day 33 "],
["day-34.html", "34 Day 34", " 34 Day 34 "],
["day-35.html", "35 Day 35", " 35 Day 35 "],
["day-36.html", "36 Day 36", " 36 Day 36 "],
["day-37.html", "37 Day 37", " 37 Day 37 "],
["day-38.html", "38 Day 38", " 38 Day 38 "],
["day-39.html", "39 Day 39", " 39 Day 39 "],
["day-40.html", "40 Day 40", " 40 Day 40 "],
["day-41.html", "41 Day 41", " 41 Day 41 "],
["day-42.html", "42 Day 42", " 42 Day 42 "],
["day-43.html", "43 Day 43", " 43 Day 43 "],
["references.html", "References", " References "]
]
