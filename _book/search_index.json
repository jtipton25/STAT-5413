[
["day-12.html", "12 Day 12 12.1 Announcements 12.2 Introduction to MCMC 12.3 Example: Simple linear regression 12.4 The posterior distribution 12.5 Fitting the model", " 12 Day 12 12.1 Announcements 12.2 Introduction to MCMC To estimate models within the Bayesian framework, the most commonly used method is Markov Chain Monte Carlo. Recall that the main difference between a Bayesian posterior distribution and a likelihood function is that the Bayesian posterior distribution integrates to 1 whereas the likelihood function does not. The posterior distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{y}\\) is \\[\\begin{align*} [\\boldsymbol{\\theta} | \\mathbf{y} ] &amp; = \\frac{[\\mathbf{y} | \\boldsymbol{\\theta}] [\\boldsymbol{\\theta} ]}{[\\mathbf{y}]} \\\\ \\tag{12.1} &amp; = \\frac{[\\mathbf{y} | \\boldsymbol{\\theta}] [\\boldsymbol{\\theta}]}{ \\int_{\\boldsymbol{\\theta}} [\\mathbf{y}|\\boldsymbol{\\theta}] [\\boldsymbol{\\theta}] d\\boldsymbol{\\theta}}, \\end{align*}\\] where the integral in (12.1) guarantees that the posterior distribution integrates to 1. For some models, the integral in (12.1) is analytically tractable (e.g. Normal likelihood and Normal prior), but for most interesting models the integral is not available in a closed form solution. Instead, we can approximate the integral numerically using a technique called Markov Chain Monte Carlo (MCMC) (See the seminal paper by Gelfand and Smith 1990 for more details). Instead of evaluating the probability distribution and calculating the normalizing constant, we can generate samples from the marginal posterior distribution of each parameter in such a way that the joint distribution of the samples from the marginal distributions is equivalent to samples from the posterior, up to Monte Carlo error. 12.3 Example: Simple linear regression Consider the simple linear regression model for \\(i=1, \\ldots, N\\) observations \\[\\begin{align} \\tag{12.2} y_i &amp; = \\mu + x_i \\beta + \\varepsilon_i \\end{align}\\] where \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\) and \\(x_i\\) are known univariate covariates. If we assume the prior distributions \\(\\mu \\sim N(\\mu_\\mu, \\sigma^2_\\mu)\\) with \\(\\mu_\\mu\\) and \\(\\sigma^2_\\mu\\) fixed and known, \\(\\beta \\sim N(\\mu_\\beta, \\sigma^2_\\beta)\\) with \\(\\mu_\\beta\\) and \\(\\sigma^2_\\beta\\) fixed and known, and \\(\\sigma^2 \\sim \\operatorname{inverse-gamma}(\\alpha_0, \\beta_0)\\) with \\(\\alpha_0\\) and \\(\\beta_0\\) fixed and known. where the \\(\\operatorname{inverse-gamma}(\\alpha_0, \\beta_0)\\) distribution is \\[\\begin{align} [\\sigma^2 | \\alpha_0, \\beta_0] &amp; = \\frac{\\beta_0^{\\alpha_0}} {\\Gamma(\\alpha_0)} (\\sigma^2)^{-\\alpha_0 - 1} \\exp\\left\\{ - \\frac{\\beta_0}{\\sigma^2} \\right\\}. \\end{align}\\] 12.4 The posterior distribution Given the model statement, we can write out the posterior distribution on which we want inference. The posterior distribution is \\[\\begin{align} [\\mu, \\beta, \\sigma^2 | \\mathbf{y}] &amp; = \\frac{\\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\mu] [\\beta] [\\sigma^2]}{\\int_\\mu \\int_\\beta \\int_{\\sigma^2} \\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\mu] [\\beta] [\\sigma^2] \\,d\\sigma^2 \\,d\\beta \\,d\\mu}. \\end{align}\\] Notice that we donâ€™t include \\(x_i\\) in the above statement because it is assumed fixed and known. For the model we defined in (12.2), the posterior distribution is available in closed form because we can evaluate the integrals in the denominator analytically. However, the integrals are challenging and for most interesting models, are not available in closed form. Given the posterior distribution, the next step is to find the marginal posterior distributions of \\(\\mu\\), \\(\\beta\\) and \\(\\sigma^2\\). 12.4.1 Full conditional distribution of \\(\\mu\\) The full conditional distribution (marginal posterior) for \\(\\mu\\) given all other parameters in the model is \\[\\begin{align} [\\mu | \\cdot] &amp; \\propto \\prod_{i=1^N} [y_i | \\mu, \\beta, \\sigma^2 ] [\\mu] [\\beta] [\\sigma^2] \\\\ &amp; \\propto \\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\mu] \\\\ &amp; \\propto \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(y_i - \\left(\\mu + x_i \\beta\\right)\\right)^2 \\right\\} \\frac{1}{\\sqrt{2 \\pi \\sigma^2_\\mu}} \\exp \\left\\{ -\\frac{1}{2 \\sigma_\\mu^2} \\left(\\mu - \\mu_\\mu\\right)^2 \\right\\} \\\\ &amp; \\propto \\prod_{i=1}^N \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(\\mu^2 - 2 \\mu \\left(y_i - x_i\\beta\\right)\\right) \\right\\} \\exp \\left\\{ -\\frac{1}{2 \\sigma_\\mu^2} \\left(\\mu^2 - 2 \\mu \\mu_\\mu\\right) \\right\\} \\\\ &amp; \\propto \\exp \\left\\{ -\\frac{1}{2} \\left(\\mu^2 \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_\\mu}\\right) - 2 \\mu \\left(\\frac{ \\sum_{i=1}^N y_i - x_i\\beta }{\\sigma^2} + \\frac{\\mu_\\mu}{\\sigma^2_\\mu}\\right)\\right) \\right\\} \\end{align}\\] which is a normal distribution with mean \\(a_\\mu^{-1}b_\\mu\\) and variance \\(a_\\mu^{-1}\\) where \\[\\begin{align} a_\\mu &amp; = \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_\\mu} \\\\ b_\\mu &amp; = \\frac{ \\sum_{i=1}^N y_i - x_i\\beta }{\\sigma^2} + \\frac{\\mu_\\mu}{\\sigma^2_\\mu} \\end{align}\\] 12.4.2 Full conditional distribution for \\(\\beta\\) The marginal posterior for \\(\\beta\\) given all other parameters in the model is \\[\\begin{align} [\\beta | \\cdot] &amp; \\propto \\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\beta] \\\\ &amp; \\propto \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(y_i - \\left(\\mu + x_i \\beta\\right)\\right)^2 \\right\\} \\frac{1}{\\sqrt{2 \\pi \\sigma^2_\\beta}} \\exp \\left\\{ -\\frac{1}{2 \\sigma_\\beta^2} \\left(\\beta - \\mu_\\beta \\right)^2 \\right\\} \\\\ &amp; \\propto \\prod_{i=1}^N \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(\\beta^2 x_i^2 - 2 \\beta \\left(x_i \\left( y_i - \\mu \\right) \\right) \\right) \\right\\} \\exp \\left\\{ -\\frac{1}{2 \\sigma_\\beta^2} \\left(\\beta^2 - 2 \\beta \\mu_\\beta \\right) \\right\\} \\\\ &amp; \\propto \\exp \\left\\{ -\\frac{1}{2} \\left(\\beta^2 \\left(\\frac{\\sum_{i=1}^N x_i^2}{\\sigma^2} + \\frac{1}{\\sigma^2_\\beta}\\right) - 2 \\beta \\left(\\frac{ \\sum_{i=1}^N x_i \\left( y_i - \\mu \\right) }{\\sigma^2} + \\frac{\\mu_\\beta}{\\sigma^2_\\beta}\\right)\\right) \\right\\} \\end{align}\\] which is a normal distribution with mean \\(a_\\beta^{-1}b_\\beta\\) and variance \\(a_\\beta^{-1}\\) where \\[\\begin{align} a_\\beta &amp; = \\frac{\\sum_{i=1}^N x_i^2}{\\sigma^2} + \\frac{1}{\\sigma^2_\\beta} \\\\ b_\\beta &amp; = \\frac{ \\sum_{i=1}^N x_i \\left( y_i - \\mu \\right) }{\\sigma^2} + \\frac{\\mu_\\beta}{\\sigma^2_\\beta} \\end{align}\\] 12.4.3 Full conditional distribution for \\(\\sigma^2\\) The marginal posterior for \\(\\sigma^2\\) given all other parameters in the model is \\[\\begin{align} [\\sigma^2 | \\cdot] &amp; \\propto \\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\sigma^2] \\\\ &amp; \\propto \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(y_i - \\left(\\mu + x_i \\beta\\right)\\right)^2 \\right\\} \\frac{\\beta_0^{\\alpha_0}} {\\Gamma(\\alpha_0)} (\\sigma^2)^{-\\alpha_0 - 1} \\exp\\left\\{ - \\frac{\\beta_0}{\\sigma^2} \\right\\} \\\\ &amp; \\propto \\prod_{i=1}^N \\left( \\sigma^2 \\right)^{-\\frac{1}{2}} \\exp \\left\\{ -\\frac{1}{\\sigma^2} \\frac{\\left(y_i - \\left(\\mu + x_i \\beta\\right)\\right)^2}{2} \\right\\} (\\sigma^2)^{-\\alpha_0 - 1} \\exp\\left\\{ - \\frac{\\beta_0}{\\sigma^2} \\right\\} \\\\ &amp; \\propto \\left( \\sigma^2 \\right)^{-\\frac{N}{2} - \\alpha_0 - 1} \\exp \\left\\{ -\\frac{1}{\\sigma^2} \\left( \\sum_{i=1}^N \\frac{1}{2} \\left( y_i - \\left( \\mu + x_i \\beta \\right) \\right)^2 + \\beta_0 \\right) \\right\\} \\end{align}\\] which is distributed as \\(\\operatorname{inverse-gamma} \\left( \\alpha_0 + \\frac{N}{2} , \\beta_0 + \\frac{1}{2} \\sum_{i=1}^N \\left( y_i - \\left(\\mu + x_i \\beta \\right) \\right)^2 \\right)\\). 12.5 Fitting the model To estimate parameters from the regression model, we will consider the iris dataset and let \\(y\\) be the Petal.Width variable and \\(x\\) be the Petal.Length variable in R. The data with the fitted regression line using the lm function is shown below. data(iris) library(ggplot2) ggplot(iris, aes(x=Petal.Length, y=Petal.Width)) + stat_smooth(method=&quot;lm&quot;) + geom_point() To start, we setup the variables and define default values for the prior constants. y &lt;- iris$Petal.Width x &lt;- iris$Petal.Length ## these are the prior choices. ## We will talk later about how to choose prior distributions and values later mu_mu &lt;- 0 sigma2_mu &lt;- 10000 mu_beta &lt;- 0 sigma2_beta &lt;- 10000 alpha_0 &lt;- 0.01 beta_0 &lt;- 0.01 Next, we define empty vectors to store the MCMC output. We will run the MCMC sampler for \\(K=5000\\) iterations. K &lt;- 5000 mu &lt;- rep(0, K) beta &lt;- rep(0, K) sigma2 &lt;- rep(0, K) The MCMC algorithm works by cycling through the marginal posterior distributions. The algorithm is called a Gibbs Sampler and we sample from \\[\\begin{align} \\mu | \\cdot &amp; \\sim N(a_\\mu^{-1}b_\\mu, a_\\mu^{-1}) \\\\ \\beta | \\cdot &amp; \\sim N(a_\\beta^{-1}b_\\beta, a_\\beta^{-1}) \\\\ \\sigma^2 | \\cdot &amp; \\sim \\operatorname{inverse-gamma}\\left( \\alpha_0 + \\frac{N}{2} , \\beta_0 + \\frac{1}{2} \\sum_{i=1}^N \\left( y_i - \\left(\\mu + x_i \\beta \\right) \\right)^2 \\right) \\end{align}\\] in sequential order. In R, the Gibbs sampler is set.seed(101) ## initialize mu[1] &lt;- rnorm(1, mu_mu, sqrt(sigma2_mu)) beta[1] &lt;- rnorm(1, mu_beta, sqrt(sigma2_beta)) ## note that sampling from an inverse gamma is the same as ## the inverse of a random gamma variable sigma2[1] &lt;- 1 / rgamma(1, alpha_0, beta_0) ## calculate the sample size N &lt;- length(y) for (k in 2:K) { ## sample mu a_mu &lt;- N / sigma2[k-1] + 1 / sigma2_mu b_mu &lt;- sum(y - x * beta[k-1]) / sigma2[k-1] + mu_mu / sigma2_mu mu[k] &lt;- rnorm(1, b_mu / a_mu, sqrt(1 / a_mu)) ## sample beta a_beta &lt;- sum(x^2) / sigma2[k-1] + 1 / sigma2_beta b_beta &lt;- sum(x*(y - mu[k])) / sigma2[k-1] + mu_beta / sigma2_beta beta[k] &lt;- rnorm(1, b_beta / a_beta, sqrt(1 / a_beta)) ## sample sigma2 sigma2[k] &lt;- 1 / rgamma(1, alpha_0 + N/2, beta_0 + 1/2 * sum((y - (mu[k] + x * beta[k]))^2)) } To visualize the MCMC samples, we can plot what are called trace plots samples &lt;- c(mu, beta, sigma2) library(ggplot2) df &lt;- data.frame( sample = 1:K, params = factor(c(rep(&quot;mu&quot;, K), rep(&quot;beta&quot;, K), rep(&quot;sigma2&quot;, K))), values = samples) ggplot(df, aes(x=sample, y=values, group=params)) + geom_line() + facet_wrap(~params, scales=&quot;free&quot;, ncol=1) The plotting window is strongly influenced by the initial condition. Instead, we can throw away the first 1000 samples (called burn-in) and examine the remaining 4000 samples for each parameter. burnin &lt;- 1000 ## subset the samples to only post-burnin samples &lt;- cbind(mu[-c(1:burnin)], beta[-c(1:burnin)], sigma2[-c(1:burnin)]) colnames(samples) &lt;- c(&quot;mu&quot;, &quot;beta&quot;, &quot;sigma2&quot;) df &lt;- data.frame( sample = (burnin+1):K, params = factor(c(rep(&quot;mu&quot;, K-burnin), rep(&quot;beta&quot;, K-burnin), rep(&quot;sigma2&quot;, K-burnin))), values = c(samples)) ## fit the linear model using the MLE fit &lt;- lm(y~x) df_lm &lt;- data.frame( params = factor(c(&quot;mu&quot;, &quot;beta&quot;, &quot;sigma2&quot;)), values = c(fit$coefficients[1], fit$coefficients[2], summary(fit)$sigma^2), row.names = NULL) ggplot(df, aes(x=sample, y=values, group=params)) + geom_line() + facet_wrap(~params, scales=&quot;free&quot;, ncol=1) + geom_hline(data=df_lm, aes(yintercept=values, group=params), col=&quot;red&quot;) We can think of each of the \\(k\\) iterations as a time step and can use methods from time series to examine the estimates. For example, the ACF for each parameter is layout(matrix(1:6, 3, 2, byrow=TRUE)) acf(samples[,&quot;mu&quot;]) pacf(samples[, &quot;mu&quot;]) acf(samples[,&quot;beta&quot;]) pacf(samples[, &quot;beta&quot;]) acf(samples[,&quot;sigma2&quot;]) pacf(samples[, &quot;sigma2&quot;]) We can also ask what is the effective sample size from our estimates (the equivalent sample size of uncorrelated samples) library(coda) effectiveSize(samples) ## mu beta sigma2 ## 449.7 427.7 4000.0 Because the samples of \\(\\mu\\), \\(\\beta\\), and \\(\\sigma^2\\) obtained from the Gibbs sampler are from probability distributions, we can calculate any quantity of interest directly. For example, the estimate for the mean of the parameters is simply the mean of the MCMC samples colMeans(samples) apply(samples, 2, sd) apply(samples, 2, quantile, prob=c(0.025, 0.975)) ## mu beta sigma2 ## -0.36411 0.41601 0.04331 ## mu beta sigma2 ## 0.040486 0.009816 0.005055 ## mu beta sigma2 ## 2.5% -0.4438 0.3972 0.03459 ## 97.5% -0.2859 0.4357 0.05456 and we can compare these estimates to those from the MLE summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.565 -0.124 -0.019 0.133 0.643 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.36308 0.03976 -9.13 4.7e-16 *** ## x 0.41576 0.00958 43.39 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.206 on 148 degrees of freedom ## Multiple R-squared: 0.927, Adjusted R-squared: 0.927 ## F-statistic: 1.88e+03 on 1 and 148 DF, p-value: &lt;2e-16 and the 95% credible interval for each parameter is simply the 2.5% and 97.5% quantiles of the samples. apply(samples, 2, quantile, prob= c(0.025, 0.975)) ## mu beta sigma2 ## 2.5% -0.4438 0.3972 0.03459 ## 97.5% -0.2859 0.4357 0.05456 We can compare these to the esimates from the MLE confint(fit) ## 2.5 % 97.5 % ## (Intercept) -0.4417 -0.2845 ## x 0.3968 0.4347 We can also plot the posterior distribution n_pred &lt;- 1000 x_pred &lt;- seq(min(iris$Petal.Length), max(iris$Petal.Length), length.out = n_pred) y_hat &lt;- t(sapply(1:n_pred, function(i) { samples[, &quot;mu&quot;] + x_pred[i] * samples[, &quot;beta&quot;] })) df_mcmc &lt;- data.frame( x = x_pred, y_hat = c(y_hat), iteration = rep(1:ncol(y_hat), each = nrow(y_hat)) ) if (!file.exists(here::here(&quot;images&quot;, &quot;posterior-linear-regression.png&quot;))) { png(file = here::here(&quot;images&quot;, &quot;posterior-linear-regression.png&quot;), width = 16, height = 9, units = &quot;in&quot;, res = 400) print( ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width)) + geom_point() + geom_line(data = df_mcmc, aes(x = x, y = y_hat, group = iteration), col = &quot;red&quot;, alpha = 0.05) + ggtitle(&quot;Posterior Distribution of mean response&quot;) ) dev.off() } knitr::include_graphics(here::here(&quot;images&quot;, &quot;posterior-linear-regression.png&quot;)) Posterior predictive distribution n_pred &lt;- 1000 x_pred &lt;- seq(min(iris$Petal.Length), max(iris$Petal.Length), length.out = n_pred) y_tilde &lt;- t(samples[, &quot;mu&quot;] + sapply(1:n_pred, function(i) { x_pred[i] * samples[, &quot;beta&quot;] + rnorm(length(samples[, &quot;sigma2&quot;]), 0, sqrt(samples[, &quot;sigma2&quot;]))})) df_mcmc &lt;- data.frame( x = x_pred, y_tilde = apply(y_tilde, 1, mean), lower_95 = apply(y_tilde, 1, quantile, prob = 0.025), upper_95 = apply(y_tilde, 1, quantile, prob = 0.975) ) ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width)) + geom_point() + geom_line(data = df_mcmc, aes(x = x, y = y_tilde), col = &quot;red&quot;) + geom_ribbon(data = df_mcmc, aes(x = x, ymin = lower_95, ymax = upper_95), fill = &quot;orange&quot;, alpha = 0.25, inherit.aes = FALSE) + ggtitle(&quot;Posterior predictive distribution&quot;) 12.5.1 General recommendations for writing MCMC software Have the first dimension of each object be the number of MCMC samples Start with simple models and build sequentially to the complex model of interest Example: For spatio-temporal models, first start with linear regression, then add spatial component, then add spatio-temporal model Find a workflow that works State the model Calculate the full conditional distributions Write the MCMC using a template "]
]
