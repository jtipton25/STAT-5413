[
["index.html", "Notes for STAT 5413 - Spatial Statistics Preface", " Notes for STAT 5413 - Spatial Statistics John Tipton Fall 2020 Semester. Last Modified: 2020-04-08 Preface These are the lecture notes for STAT 5413 Fall 2020. "],
["day-1.html", "1 Day 1 1.1 Notation 1.2 Probability Distributions 1.3 Hierarchical modeling", " 1 Day 1 library(tidyverse) 1.1 Notation The dimensions of different mathematical objects are very important for the study of spatial statistics. To communicate this, we use the following notation. A scalar random variable is represented by a lowercase alphanumeric letter (\\(x\\), \\(y\\), \\(z\\), etc.), a vector random variable is respresented by a bold lowercase alphanumeric letter (\\(\\mathbf{x}\\), \\(\\mathbf{y}\\), \\(\\mathbf{z}\\), etc.), and a matrix random variable is respresented by a bold uppercase alphanumeric letter (\\(\\mathbf{X}\\), \\(\\mathbf{Y}\\), \\(\\mathbf{Z}\\), etc.). We use a similar notation for parameters as well where scalar parameters are represented by a lowercase Greek letter (\\(\\mu\\), \\(\\alpha\\), \\(\\beta\\), etc.), a vector parameter is respresented by a bold lowercase Greek letter (\\(\\boldsymbol{\\mu}\\), \\(\\boldsymbol{\\alpha}\\), \\(\\boldsymbol{\\beta}\\), etc.), and a matrix random variable is respresented by a bold uppercase Greek letter (\\(\\boldsymbol{\\Sigma}\\), \\(\\boldsymbol{\\Psi}\\), \\(\\boldsymbol{\\Gamma}\\), etc.). 1.2 Probability Distributions We also need notation to explain probability distributions. We use the notation \\([y]\\) to denote the probability density function \\(p(y)\\) of the random variable \\(y\\) and \\([y|x]\\) to denote the probability density function \\(p(y|x)\\) of \\(y\\) given \\(x\\). For example, if \\(y\\) is a Gaussian random variable with mean \\(\\mu\\) and standard deviation \\(\\sigma\\) we write \\[\\begin{align*} [y | \\mu, \\sigma] &amp; = \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{-\\frac{1}{2 \\sigma^2} (y - \\mu)^2 \\right\\}. \\end{align*}\\] We can also denote that \\(y\\) has a Gaussian (normal) distribution given mean \\(\\mu\\) and variance \\(\\sigma^2\\) using the \\(\\sim\\) notation \\[\\begin{align*} y | \\mu, \\sigma &amp; \\sim \\operatorname{N}(\\mu, \\sigma^2). \\end{align*}\\] 1.2.1 Example: linear regression \\[\\begin{align*} \\left[y_i | \\boldsymbol{\\theta} \\right] &amp; \\sim \\operatorname{N}(X_i \\beta, \\sigma^2) \\\\ \\boldsymbol{\\theta} &amp; = (\\beta, \\sigma^2) \\end{align*}\\] ## Sample data set.seed(404) dat &lt;- data.frame(x=(x=runif(200, 0, 50)), y=rnorm(200, 10 * x, 100)) ## breaks: where you want to compute densities breaks &lt;- seq(0, max(dat$x), len=7)[-c(1, 7)] dat$section &lt;- cut(dat$x, breaks) ## Get the residuals dat$res &lt;- residuals(lm(y ~ x, data=dat)) ## Compute densities for each section, and flip the axes, and add means of sections ## Note: the densities need to be scaled in relation to the section size (2000 here) ys &lt;- seq(-300, 300, length = 50) xs &lt;- rep(breaks, each = 50) + 1000 * dnorm(ys, 0, 100) res &lt;- matrix(0, 50, 5) for (i in 1:5) { res[, i] &lt;- 10 * breaks[i] + ys } dens &lt;- data.frame(x = xs, y=c(res), grouping = cut(xs, breaks)) ggplot(dat, aes(x, y)) + geom_point(size = 2) + geom_smooth(method=&quot;lm&quot;, fill=NA, lwd=2, se = FALSE) + geom_path(data=dens, aes(x, y, group = grouping), color=&quot;salmon&quot;, lwd=2) + theme_bw() + geom_vline(xintercept=breaks, lty=2) ## `geom_smooth()` using formula &#39;y ~ x&#39; 1.3 Hierarchical modeling Follow Berliner (1996) framework for hierarchical probability models Model encodes our understanding of the scientific process of interest Model accounts for as much uncertainty as possible Model results in a probability distribution Note: nature may be deterministic – often probabilistic models outperform physical models. Example: model individual rain drops vs. probability/intensity of rain Update model with data Use the model to generate parameter estimates given data 1.3.1 Bayesian Hierarchical models (BHMs) Break the model into components: Data Model. Process Model. Parameter Model. Combined, the data model, the process model, and the parameter model define a posterior distribution. \\[\\begin{align*} \\color{cyan}{[\\mathbf{z}, \\boldsymbol{\\theta}_D, \\boldsymbol{\\theta}_P | \\mathbf{y}]} &amp; \\propto \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\color{orange}{[\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P]} \\end{align*}\\] 1.3.2 Empirical Hierarchical models (EHMs) Break the model into components: Data Model. Process Model. Parameter estimates (fixed values) are substituted before fitting the model Combined, the data model and the process model define a predictive distribution. Thus, numerical evaluation of the predictive distribution is typically required to estimate unceratinty (bootstrap, MLE asymptotics) Note: the predictive distribution is not a posterior distribution because the normalizing constant is not known \\[\\begin{align*} \\color{plum}{[\\mathbf{z} | \\mathbf{y}]} &amp; \\propto \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\end{align*}\\] 1.3.3 Data Model \\[\\begin{align*} \\color{red}{[\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}]} \\end{align*}\\] Describes how the data are collected and observed. Account for measurement process and uncertainty. Model the data in the manner in which they were collected. Data \\(\\mathbf{y}\\). Noisy. Expensive. Not what you want to make inference on. Latent variables \\(\\mathbf{z}\\). Think of \\(\\mathbf{z}\\) as the ideal data. No measurement error - the exact quantity you want to observe but can’t. Data model parameters \\(\\boldsymbol{\\theta}_D\\). 1.3.4 Process Model \\[\\begin{align*} \\color{blue}{[\\mathbf{z} | \\boldsymbol{\\theta}_P]} \\end{align*}\\] Where the science happens! Latent process \\(\\mathbf{z}\\) is modeled. Can be dynamic in space and/or time Process parameters \\(\\boldsymbol{\\theta}_P\\). Virtually all interesting scientific questions can be made with inference about \\(\\mathbf{z}\\) 1.3.5 Parameter (Prior) Model (BMHs only) \\[\\begin{align*} \\color{orange}{[\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P]} \\end{align*}\\] Probability distributions define “reasonable” ranges for parameters. Parameter models are useful for a variety of problems: Choosing important variables. Preventing over-fitting (regularization). “Pooling” estimates across categories. 1.3.6 Posterior Distribution \\[\\begin{align*} \\color{cyan}{[\\mathbf{z}, \\boldsymbol{\\theta}_D, \\boldsymbol{\\theta}_P | \\mathbf{y}]} &amp; \\propto [\\mathbf{y} | \\boldsymbol{\\theta}_D, \\mathbf{z}] [\\mathbf{z} | \\boldsymbol{\\theta}_P] [\\boldsymbol{\\theta}_D] [\\boldsymbol{\\theta}_P] \\end{align*}\\] Probability distribution over all unknowns in the model. Inference is made using the posterior distribution. Because the posterior distribution is a probability distribution (BHMs), uncertainty is easy to calculate. This is not true for EHMs. 1.3.7 Scientifically Motivated Statistical Modeling Criticize the model Does the model fit the data well? Do the predictions make sense? Are there subsets of the data that don’t fit the model well? Make inference using the model. If the model fits the data, use the model fit for prediction or inference. References "],
["day-2.html", "2 Day 2 2.1 Spatial Data 2.2 Types of spatial data", " 2 Day 2 library(tidyverse) library(here) library(sp) library(spatstat) 2.1 Spatial Data All data occur at some location is space and time. For know we focus on spatial analyses and will later extend this to spatio-temporal analyses. Let \\(\\mathcal{D}\\) represent the spatial domain and let \\(\\mathbf{s}\\) be a spatial location. In general, we will let \\(\\mathcal{A} \\subset \\mathcal{D}\\) be a subdomain of the spatial region of \\(\\mathbf{D}\\). knitr::include_graphics(here::here(&quot;images&quot;, &quot;spatial-domain.jpg&quot;)) 2.2 Types of spatial data There are three primary types of spatial data that we are going to consider 2.2.1 Geostatistical data Occur everywhere continuous support examples: temperature, precipitation data(&quot;NOAA_df_1990&quot;, package = &quot;STRbook&quot;) glimpse(NOAA_df_1990) ## Rows: 730,486 ## Columns: 10 ## $ julian &lt;int&gt; 726834, 726835, 726836, 726837, 726838, 726839, 726840, 726841… ## $ year &lt;int&gt; 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 1990, 19… ## $ month &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,… ## $ day &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18,… ## $ id &lt;dbl&gt; 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 3804, 38… ## $ z &lt;dbl&gt; 35, 42, 49, 59, 41, 45, 46, 42, 54, 43, 52, 38, 32, 43, 53, 55… ## $ proc &lt;chr&gt; &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;, &quot;Tmax&quot;… ## $ lat &lt;dbl&gt; 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35, 39.35,… ## $ lon &lt;dbl&gt; -81.43333, -81.43333, -81.43333, -81.43333, -81.43333, -81.433… ## $ date &lt;date&gt; 1990-01-01, 1990-01-02, 1990-01-03, 1990-01-04, 1990-01-05, 1… ## Only plot the states with data states &lt;- map_data(&quot;state&quot;) states &lt;- states %&gt;% subset(!(region %in% c( &quot;washington&quot;, &quot;oregon&quot;, &quot;california&quot;, &quot;nevada&quot;, &quot;idaho&quot;, &quot;utah&quot;, &quot;arizona&quot;,&quot;montana&quot;, &quot;wyoming&quot;, &quot;colorado&quot;, &quot;new mexico&quot; ) )) ## generate map NOAA_df_1990 %&gt;% subset(year == 1990 &amp; day == 1 &amp; proc == &quot;Tmax&quot;) %&gt;% ggplot(aes(x = lon, y = lat, color = z)) + geom_point() + facet_wrap(~ month, scales = &quot;free&quot;, nrow = 4) + geom_polygon(data = states, aes(x = long, y = lat, group = group), inherit.aes = FALSE, fill = NA, color = &quot;black&quot;) + scale_color_viridis_c(option = &quot;inferno&quot;) + ggtitle(&quot;Tmax for the first day of each month in 1990&quot;) 2.2.2 Areal data Occur only over discrete areas can be thought of as an integral of a continuous process over a subdomain \\(\\mathcal{A} \\in \\mathcal{D}\\) examples: cases of a disease by counties, votes in an election by congressional district data(&quot;BEA&quot;, package = &quot;STRbook&quot;) glimpse(BEA) ## Rows: 116 ## Columns: 5 ## $ Description &lt;chr&gt; &quot;Per capita personal income (dollars)&quot;, &quot;Per capita perso… ## $ NAME10 &lt;fct&gt; &quot;Adair, MO&quot;, &quot;Andrew, MO&quot;, &quot;Atchison, MO&quot;, &quot;Audrain, MO&quot;,… ## $ X1970 &lt;int&gt; 2723, 3577, 3770, 3678, 3021, 2832, 3263, 2508, 2147, 349… ## $ X1980 &lt;int&gt; 7399, 7937, 5743, 8356, 7210, 7445, 8596, 6125, 5431, 923… ## $ X1990 &lt;int&gt; 12755, 15059, 14748, 15198, 12873, 13530, 13195, 11854, 1… data(&quot;MOcounties&quot;, package = &quot;STRbook&quot;) glimpse(MOcounties) ## Rows: 214,279 ## Columns: 53 ## $ long &lt;dbl&gt; 627911.9, 627921.4, 627923.0, 627947.8, 627956.5, 627994.8… ## $ lat &lt;dbl&gt; 4473554, 4473559, 4473560, 4473577, 4473583, 4473612, 4473… ## $ order &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… ## $ hole &lt;lgl&gt; FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FALSE, FA… ## $ piece &lt;fct&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ id &lt;chr&gt; &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;, &quot;0&quot;… ## $ group &lt;fct&gt; 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1… ## $ STATEFP10 &lt;fct&gt; 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29, 29… ## $ COUNTYFP10 &lt;fct&gt; 045, 045, 045, 045, 045, 045, 045, 045, 045, 045, 045, 045… ## $ COUNTYNS10 &lt;fct&gt; 00758477, 00758477, 00758477, 00758477, 00758477, 00758477… ## $ GEOID10 &lt;fct&gt; 29045, 29045, 29045, 29045, 29045, 29045, 29045, 29045, 29… ## $ NAME10 &lt;fct&gt; &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark, MO&quot;, &quot;Clark… ## $ NAMELSAD10 &lt;fct&gt; Clark County, Clark County, Clark County, Clark County, Cl… ## $ LSAD10 &lt;fct&gt; 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06, 06… ## $ CLASSFP10 &lt;fct&gt; H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1, H1… ## $ MTFCC10 &lt;fct&gt; G4020, G4020, G4020, G4020, G4020, G4020, G4020, G4020, G4… ## $ CSAFP10 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ CBSAFP10 &lt;fct&gt; 22800, 22800, 22800, 22800, 22800, 22800, 22800, 22800, 22… ## $ METDIVFP10 &lt;fct&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA, NA… ## $ FUNCSTAT10 &lt;fct&gt; A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A, A… ## $ ALAND10 &lt;dbl&gt; 1307146971, 1307146971, 1307146971, 1307146971, 1307146971… ## $ AWATER10 &lt;dbl&gt; 18473547, 18473547, 18473547, 18473547, 18473547, 18473547… ## $ INTPTLAT10 &lt;fct&gt; +40.4072748, +40.4072748, +40.4072748, +40.4072748, +40.40… ## $ INTPTLON10 &lt;fct&gt; -091.7294720, -091.7294720, -091.7294720, -091.7294720, -0… ## $ AREA &lt;dbl&gt; 1324937990, 1324937990, 1324937990, 1324937990, 1324937990… ## $ PERIMETER &lt;dbl&gt; 161503.6, 161503.6, 161503.6, 161503.6, 161503.6, 161503.6… ## $ COUNTY10_ &lt;int&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… ## $ COUNTY10_I &lt;int&gt; 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115, 115… ## $ POP90 &lt;int&gt; 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547, 7547… ## $ WHITE90 &lt;int&gt; 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528, 7528… ## $ BLACK90 &lt;int&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… ## $ ASIANPI90 &lt;int&gt; 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4, 4… ## $ AMIND90 &lt;int&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7… ## $ OTHER90 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ HISP90 &lt;int&gt; 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26, 26… ## $ POP00 &lt;int&gt; 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416, 7416… ## $ WHITE00 &lt;int&gt; 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329, 7329… ## $ BLACK00 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ ASIAN00 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ AMIND00 &lt;int&gt; 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15, 15… ## $ HAWNPI00 &lt;int&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… ## $ OTHER00 &lt;int&gt; 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16, 16… ## $ MULTRA00 &lt;int&gt; 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45, 45… ## $ HISP00 &lt;int&gt; 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52, 52… ## $ POP10 &lt;int&gt; 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139, 7139… ## $ WHITE10 &lt;int&gt; 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011, 7011… ## $ BLACK10 &lt;int&gt; 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19, 19… ## $ ASIAN10 &lt;int&gt; 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23, 23… ## $ AMIND10 &lt;int&gt; 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9… ## $ HAWNPI10 &lt;int&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ OTHER10 &lt;int&gt; 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5… ## $ MULTRA10 &lt;int&gt; 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72, 72… ## $ HISP10 &lt;int&gt; 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42, 42… MOcounties &lt;- left_join(MOcounties, BEA, by = &quot;NAME10&quot;) ggplot(MOcounties) + geom_polygon(aes(x = long, y = lat, # county boundary group = NAME10, # county group fill = log(X1970))) + # log of income geom_path(aes(x = long, y = lat, group = NAME10)) + scale_fill_viridis_c(limits = c(7.5, 10.2), option = &quot;plasma&quot;, name = &quot;log($)&quot;) + coord_fixed() + ggtitle(&quot;1970&quot;) + xlab(&quot;x (m)&quot;) + ylab(&quot;y (m)&quot;) + theme_bw() 2.2.3 Point process data The count and location of the data are random examples: tornados, lightning strikes # uncomment out this line to download the data # load(url(&quot;http://github.com/mgimond/Spatial/raw/master/Data/ppa.RData&quot;)) # save(starbucks, ma, pop, file = here::here(&quot;data&quot;, &quot;ppa-starbucks.RData&quot;)) load(here::here(&quot;data&quot;, &quot;ppa-starbucks.RData&quot;)) glimpse(starbucks) ## List of 5 ## $ window :List of 4 ## ..$ type : chr &quot;rectangle&quot; ## ..$ xrange: num [1:2] 648032 917741 ## ..$ yrange: num [1:2] 4609785 4748107 ## ..$ units :List of 3 ## .. ..$ singular : chr &quot;unit&quot; ## .. ..$ plural : chr &quot;units&quot; ## .. ..$ multiplier: num 1 ## .. ..- attr(*, &quot;class&quot;)= chr &quot;unitname&quot; ## ..- attr(*, &quot;class&quot;)= chr &quot;owin&quot; ## $ n : int 171 ## $ x : num [1:171] 917741 911147 902987 876188 875868 ... ## $ y : num [1:171] 4637151 4628510 4628982 4616741 4616719 ... ## $ markformat: chr &quot;none&quot; ## - attr(*, &quot;class&quot;)= chr &quot;ppp&quot; ## uses spatstat library ## add the massachusetts polygon Window(starbucks) &lt;- ma marks(starbucks) &lt;- NULL ## plot using the plot function from spatstat plot(starbucks) "],
["day-3.html", "3 Day 3 3.1 Anouncements 3.2 Files for spatial data 3.3 Textbook package 3.4 Spatial Visualization", " 3 Day 3 library(tidyverse) library(here) library(sp) 3.1 Anouncements Course audits Show gitHub page for site https://github.com/jtipton25/STAT-5413 Show how to download files and data Example Gerrymandering https://uglygerry.com/ library(showtext) ## Loading required package: sysfonts ## Loading required package: showtextdb font_add(&quot;myfont&quot;, here::here(&quot;fonts&quot;, &quot;Gerry.otf&quot;)) plot(cars, family = &quot;myfont&quot;) title( main = &quot;This Font is made of \\n Gerrymandered Political Districts&quot;, family = &quot;myfont&quot;, cex.main = 1.5 ) 3.2 Files for spatial data Many different file types for spatial data Typically data are in “flat files” like comma-seperated value (CSV) files read.csv(here(&quot;path&quot;, &quot;to&quot;, &quot;file.csv&quot;)) “shapefiles” which can be read using rgdal or maptools packages library(rgdal) library(maptools) “NetCDF” files cane be read using ncdf4 or RNetCDF library(ncdf4) library(RNetCDF) 3.3 Textbook package To install the data from the textbook, go to https://spacetimewithr.org/ and follow the link to the code. # install.packages(&quot;devtools&quot;) library(devtools) install_github(&quot;andrewzm/STRbook&quot;) Note that this package is relatively large because it contains a decent amount of spatial data. library(STRbook) ## ## Attaching package: &#39;STRbook&#39; ## The following object is masked _by_ &#39;.GlobalEnv&#39;: ## ## MOcounties 3.4 Spatial Visualization 3.4.1 Spatial visualization using fields Simulate a process with some random locations library(fields) ## Loading required package: spam ## Loading required package: dotCall64 ## Loading required package: grid ## Spam version 2.5-1 (2019-12-12) is loaded. ## Type &#39;help( Spam)&#39; or &#39;demo( spam)&#39; for a short introduction ## and overview of this package. ## Help for individual functions is also obtained by adding the ## suffix &#39;.spam&#39; to the function name, e.g. &#39;help( chol.spam)&#39;. ## ## Attaching package: &#39;spam&#39; ## The following objects are masked from &#39;package:base&#39;: ## ## backsolve, forwardsolve ## Loading required package: maps ## ## Attaching package: &#39;maps&#39; ## The following object is masked from &#39;package:purrr&#39;: ## ## map ## See https://github.com/NCAR/Fields for ## an extensive vignette, other supplements and source code ## longitude and latitude of approximately the center of Arkansas lon_lat_center &lt;- c(-92.33, 35.00) n &lt;- 1000 ## simulate some random locations lon &lt;- runif(n, lon_lat_center[1] - 2, lon_lat_center[1] + 2) lat &lt;- runif(n, lon_lat_center[2] - 2, lon_lat_center[2] + 2) y &lt;- rnorm(n, lat + lon, 0.1) plot(lon, lat) quilt.plot(lon, lat, y, nx = 30, ny = 30) points(lon, lat, cex = .3) quilt.plot(lon, lat, y, nx = 6, ny = 10) points(lon, lat, cex = .3) Simulate a process on a regular grid n &lt;- 50^2 ## simulate locations on a grid lon &lt;- seq(lon_lat_center[1] - 2, lon_lat_center[1] + 2, length = sqrt(n)) lat &lt;- seq(lon_lat_center[2] - 2, lon_lat_center[2] + 2, length = sqrt(n)) s &lt;- expand.grid(lon, lat) head(lon) ## [1] -94.33000 -94.24837 -94.16673 -94.08510 -94.00347 -93.92184 head(lat) ## [1] 33.00000 33.08163 33.16327 33.24490 33.32653 33.40816 head(s) ## Var1 Var2 ## 1 -94.33000 33 ## 2 -94.24837 33 ## 3 -94.16673 33 ## 4 -94.08510 33 ## 5 -94.00347 33 ## 6 -93.92184 33 plot(s, cex = 0.3) ## simulate some fake data with a north/south trend y &lt;- 120 - 1.5 * s[, 2] + matrix(rnorm(n), sqrt(n), sqrt(n)) image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;) contour(lon, lat, y, main = &quot;Contour plot of simulated data&quot;) image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;) contour(lon, lat, y, main = &quot;Contour plot of simulated data&quot;, add = TRUE, nlevels = 10) ## adding in maps library(maps) maps::map(&quot;world&quot;) maps::map(&quot;state&quot;) maps::map(&quot;county&quot;) maps::map(&quot;county&quot;, &quot;Arkansas&quot;) points(s, cex = 0.3) state &lt;- map.where(&quot;state&quot;, x = s[, 1], y = s[, 2]) head(state) ## [1] &quot;texas&quot; &quot;texas&quot; &quot;texas&quot; &quot;texas&quot; &quot;louisiana&quot; &quot;louisiana&quot; table(state) ## state ## arkansas louisiana mississippi missouri texas ## 1903 34 180 351 32 ## subset only points in arkansas dat &lt;- data.frame( lon = s[, 1], lat = s[, 2], state = state ) maps::map(&quot;county&quot;, &quot;Arkansas&quot;) dat %&gt;% subset(state == &quot;arkansas&quot;) %&gt;% points(cex = 0.3) # points(subset(dat, state == &quot;arkansas&quot;), cex = 0.3) Plot the simulated data with the county boundaries image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;) maps::map(&quot;county&quot;, add = TRUE, lwd = 2) ## change the aspect ratio image.plot(lon, lat, y, main = &quot;Plot of simulated data&quot;, asp = 1.3) maps::map(&quot;county&quot;, add = TRUE, lwd = 2) "],
["day-4.html", "4 Day 4 4.1 Announcements 4.2 Visualization (continued) 4.3 Interactive visualization (Interactive with HTML format only)", " 4 Day 4 library(tidyverse) 4.1 Announcements 4.2 Visualization (continued) 4.2.1 Spatial visualization using fields nx &lt;- 100 ny &lt;- 100 library(maps) # for map.where # Corner of the USA corners &lt;- c(-124.733056, -66.947028, 24.520833, 49.384472) # create grid grid &lt;- expand.grid( seq(corners[1], corners[2], length = nx), seq(corners[3], corners[4], length = ny) ) dat &lt;- data.frame( lon = grid[, 1], lat = grid[, 2], inUS = ifelse(is.na(map.where(&quot;usa&quot;, x = grid[, 1], y = grid[, 2])), FALSE, TRUE) ) ## Plot only points in the us dat %&gt;% subset(inUS) %&gt;% ## this selects only the true values ggplot(aes(x = lon, y = lat)) + geom_point(size = 0.6, alpha = 0.5) ## Simulate some data over the grid dat$y &lt;- sin(2 * pi * dat$lon / 10) + cos(2 *pi * dat$lat / 10) + sin(2 * pi * dat$lon / 10) * cos(2 *pi * dat$lat / 10) ## plot each of the responses grouped by latitude dat %&gt;% ggplot(aes(x = lon, y = y, group = lat, color = lat)) + geom_line() ## Function to generate maps map_points &lt;- function (dat, color_low = &quot;white&quot;, color_high = &quot;darkred&quot;, color_na = gray(0.9), zeroiswhite = FALSE, xlim = NULL, ylim = NULL, zlim = NULL, mainTitle = NULL, legendTitle = &quot;&quot;) { library(ggplot2) ## check if the data.fram dat contains the correct variables if (is.null(dat$lon)) { stop(&#39;The data.frame dat must contain a &quot;lon&quot; variable&#39;) } if (is.null(dat$lat)) { stop(&#39;The data.frame dat must contain a &quot;lat&quot; variable&#39;) } if (is.null(dat$y)) { stop(&#39;The data.frame dat must contain a &quot;y&quot; variable&#39;) } # Store the base data of the underlying map states &lt;- map_data(&quot;state&quot;) # Set limits for x, y, z if not specified as parameters if (is.null(xlim)) { xlim &lt;- range(dat$lon, na.rm = TRUE) } if (is.null(ylim)) { ylim &lt;- range(dat$lat, na.rm = TRUE) } if (is.null(zlim)) { zlim &lt;- range(dat$y, na.rm = TRUE) } # Create the plot p &lt;- ggplot(dat, aes(x = lon, y = lat)) + theme_bw() p &lt;- p + theme(plot.title = element_text(size = rel(1.5))) p &lt;- p + geom_point(aes(colour = y)) ## add in the map p &lt;- p + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) ## a 1.3 coordinate ratio is visually appealing p &lt;- p + coord_fixed(ratio = 1.3, xlim = xlim, ylim = ylim) p &lt;- p + labs(title = paste(mainTitle, &quot;\\n&quot;, sep=&quot;&quot;), x = &quot;&quot;, y = &quot;&quot;) if(zeroiswhite){ p &lt;- p + scale_colour_gradient2( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } if(!zeroiswhite){ p &lt;- p + scale_colour_gradient( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } return(p) } ## Let&#39;s make some plots dat %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, mainTitle = &quot;Entire United States&quot; ) ## Subset only the US dat %&gt;% subset(inUS) %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, mainTitle = &quot;Entire United States&quot; ) ## plot only a subset of points dat %&gt;% subset(inUS) %&gt;% ## sample 500 points at random sample_n(500) %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, zeroiswhite = TRUE, mainTitle = &quot;Entire United States&quot; ) ## Truncate the southeastern US dat %&gt;% subset(inUS) %&gt;% ## sample 500 points at random sample_n(500) %&gt;% map_points( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, zeroiswhite = TRUE, xlim = c(-95, -75), ylim = c(25, 37.5), mainTitle = &quot;Southeastern United States&quot;, legendTitle = &quot;Widgets&quot; ) Heatmaps can also be used for plotting. In general, there are two ggplot geoms that are useful for spatial data: geom_tile is good for irregularly spaced data, geom_raster is best for regularly spaced data as it is faster to process. ## Function to generate maps map_heat &lt;- function (dat, color_low = &quot;white&quot;, color_high = &quot;darkred&quot;, color_na = gray(0.9), zeroiswhite = FALSE, xlim = NULL, ylim = NULL, zlim = NULL, mainTitle = NULL, legendTitle = &quot;&quot;, geom = &quot;raster&quot;) { library(ggplot2) ## check if the data.fram dat contains the correct variables if (is.null(dat$lon)) { stop(&#39;The data.frame dat must contain a &quot;lon&quot; variable&#39;) } if (is.null(dat$lat)) { stop(&#39;The data.frame dat must contain a &quot;lat&quot; variable&#39;) } if (is.null(dat$y)) { stop(&#39;The data.frame dat must contain a &quot;y&quot; variable&#39;) } if (!(geom %in% c(&quot;raster&quot;, &quot;tile&quot;))) { stop(&#39;The only options for geom are &quot;raster&quot; or &quot;tile&quot;&#39;) } # Store the base data of the underlying map states &lt;- map_data(&quot;state&quot;) # Set limits for x, y, z if not specified as parameters if (is.null(xlim)) { xlim &lt;- range(dat$lon, na.rm = TRUE) } if (is.null(ylim)) { ylim &lt;- range(dat$lat, na.rm = TRUE) } if (is.null(zlim)) { zlim &lt;- range(dat$y, na.rm = TRUE) } # Create the plot p &lt;- ggplot(dat, aes(x = lon, y = lat)) + theme_bw() p &lt;- p + theme(plot.title = element_text(size = rel(1.5))) if (geom == &quot;raster&quot;) { p &lt;- p + geom_raster(aes(fill = y)) } if (geom == &quot;tile&quot;) { p &lt;- p + geom_tile(aes(fill = y)) } ## add in the map p &lt;- p + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) ## a 1.3 coordinate ratio is visually appealing p &lt;- p + coord_fixed(ratio = 1.3, xlim = xlim, ylim = ylim) p &lt;- p + labs(title = paste(mainTitle, &quot;\\n&quot;, sep=&quot;&quot;), x = &quot;&quot;, y = &quot;&quot;) if(zeroiswhite){ p &lt;- p + scale_colour_gradient2( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } if(!zeroiswhite){ p &lt;- p + scale_colour_gradient( low = color_low, high = color_high, na.value = color_na, limits = zlim, name = legendTitle ) } return(p) } ## Subset only the US dat %&gt;% subset(inUS) %&gt;% map_heat( color_low = &quot;blue&quot;, color_high = &quot;yellow&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;raster&quot; ) ## Subset only the US dat %&gt;% subset(inUS) %&gt;% map_heat( color_low = &quot;blue&quot;, color_high = &quot;yellow&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;tile&quot; ) ## Subsample the data dat %&gt;% subset(inUS) %&gt;% sample_n(1000) %&gt;% map_heat( color_low = &quot;blue&quot;, color_high = &quot;green&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;raster&quot; ) ## Warning: Raster pixels are placed at uneven horizontal intervals and will be ## shifted. Consider using geom_tile() instead. ## Warning: Raster pixels are placed at uneven vertical intervals and will be ## shifted. Consider using geom_tile() instead. ## Subsample the data dat %&gt;% subset(inUS) %&gt;% sample_n(1000) %&gt;% map_heat( color_low = &quot;pink&quot;, color_high = &quot;black&quot;, mainTitle = &quot;Entire United States&quot;, geom = &quot;tile&quot; ) Plotting spatial data using google maps ## longitude and latitude of approximately the center of Arkansas arkansas_center &lt;- c(-92.33, 35.00) library(maps) library(ggplot2) library(ggmap) ## Google&#39;s Terms of Service: https://cloud.google.com/maps-platform/terms/. ## Please cite ggmap if you use it! See citation(&quot;ggmap&quot;) for details. lon &lt;- arkansas_center[1] + seq(-2, 2, length = 10) lat &lt;- arkansas_center[2] + seq(-2, 2, length = 10) s &lt;- expand.grid(lon, lat) head(lon) ## [1] -94.33000 -93.88556 -93.44111 -92.99667 -92.55222 -92.10778 head(lat) ## [1] 33.00000 33.44444 33.88889 34.33333 34.77778 35.22222 str(s) ## &#39;data.frame&#39;: 100 obs. of 2 variables: ## $ Var1: num -94.3 -93.9 -93.4 -93 -92.6 ... ## $ Var2: num 33 33 33 33 33 33 33 33 33 33 ... ## - attr(*, &quot;out.attrs&quot;)=List of 2 ## ..$ dim : int 10 10 ## ..$ dimnames:List of 2 ## .. ..$ Var1: chr &quot;Var1=-94.33000&quot; &quot;Var1=-93.88556&quot; &quot;Var1=-93.44111&quot; &quot;Var1=-92.99667&quot; ... ## .. ..$ Var2: chr &quot;Var2=33.00000&quot; &quot;Var2=33.44444&quot; &quot;Var2=33.88889&quot; &quot;Var2=34.33333&quot; ... plot(s) points(arkansas_center, pch = 19, col = 2) dat &lt;- data.frame(lon = lon, lat = lat) Using Google maps requires registration of a key. See https://www.littlemissdata.com/blog/maps for details. Plotting areal data The example is from https://www4.stat.ncsu.edu/~reich/SpatialStats/code/Guns.pdf taken from https://www.thelancet.com/journals/lancet/article/PIIS0140-6736(15)01026-0/fulltext ## process the guns data # load(here::here(&quot;data&quot;, &quot;guns.RData&quot;)) # names(Y)[1:5] # region &lt;- tolower(names(Y)) # region[1:5] # rate &lt;- 10000*Y/N # numlaws &lt;- rowSums(X) # crime &lt;- data.frame(Y=Y,N=N,rate=rate,X=X,numlaws,region=region) # dat &lt;- data.frame( # deaths_2010 = Y, # population = N, # deaths_per_10000 = Z[, 1], # firearm_quartile = Z[, 2], # unemployment_quartile = Z[, 3], # non_firearm_homocide = Z[, 4], # firearm_export_quartile = Z[, 5], # numlaws = apply(X, 1, sum), # region = region # ) # save(dat, file = here::here(&quot;data&quot;, &quot;guns_processed.RData&quot;)) load(here::here(&quot;data&quot;, &quot;guns_processed.RData&quot;)) ## mutate a death rate dat &lt;- dat %&gt;% mutate(rate = 10000 * deaths_2010 / population) dat %&gt;% ggplot(aes(x = numlaws, y = rate, color = region == &quot;arkansas&quot;)) + geom_point() + scale_color_manual(values = c(&quot;black&quot;, &quot;red&quot;)) + xlab(&quot;Number of gun control laws&quot;) + ylab(&quot;Homicide rate (deaths/100K)&quot;) + ggtitle(&quot;Arkansas in red&quot;) + theme(legend.position = &quot;none&quot;) lm(rate ~ numlaws, data = dat) %&gt;% summary() ## ## Call: ## lm(formula = rate ~ numlaws, data = dat) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.46715 -0.16720 -0.02576 0.16171 0.72809 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.344693 0.055145 24.385 &lt; 2e-16 *** ## numlaws -0.045276 0.007302 -6.201 1.24e-07 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.2867 on 48 degrees of freedom ## Multiple R-squared: 0.4448, Adjusted R-squared: 0.4332 ## F-statistic: 38.45 on 1 and 48 DF, p-value: 1.236e-07 us &lt;- map_data(&quot;state&quot;) head(us) ## long lat group order region subregion ## 1 -87.46201 30.38968 1 1 alabama &lt;NA&gt; ## 2 -87.48493 30.37249 1 2 alabama &lt;NA&gt; ## 3 -87.52503 30.37249 1 3 alabama &lt;NA&gt; ## 4 -87.53076 30.33239 1 4 alabama &lt;NA&gt; ## 5 -87.57087 30.32665 1 5 alabama &lt;NA&gt; ## 6 -87.58806 30.32665 1 6 alabama &lt;NA&gt; gg &lt;- ggplot() gg &lt;- gg + geom_map(data = us, map = us, aes(x = long, y = lat, map_id = region), fill = &quot;#ffffff&quot;, color = &quot;#ffffff&quot;, size = 0.15) ## Warning: Ignoring unknown aesthetics: x, y gg gg &lt;- gg + geom_map( data = dat, map = us, aes(fill = rate, map_id = region), color = &quot;#ffffff&quot;, size = 0.15 ) gg &lt;- gg + scale_fill_continuous( low = &#39;thistle2&#39;, high = &#39;darkred&#39;, guide= &#39;colorbar&#39;, name = &quot;Deaths/100K&quot; ) gg &lt;- gg + labs(x = NULL, y = NULL, title = &quot;Homicide rates&quot;) gg &lt;- gg + coord_map(&quot;albers&quot;, lat0 = 39, lat1 = 45) gg &lt;- gg + theme(panel.border = element_blank()) gg &lt;- gg + theme(panel.background = element_blank()) gg &lt;- gg + theme(axis.ticks = element_blank()) gg &lt;- gg + theme(axis.text = element_blank()) gg The map looks right according to http://www.deathpenaltyinfo.org/murder-rates-nationally-and-state#MRord 4.2.2 In Class Activity: From Lab 2.1 on the textbook site ## Wikle, C. K., Zammit-Mangion, A., and Cressie, N. (2019), ## Spatio-Temporal Statistics with R, Boca Raton, FL: Chapman &amp; Hall/CRC ## Copyright (c) 2019 Wikle, Zammit-Mangion, Cressie ## ## This program is free software; you can redistribute it and/or ## modify it under the terms of the GNU General Public License ## as published by the Free Software Foundation; either version 2 ## of the License, or (at your option) any later version. ## ## This program is distributed in the hope that it will be useful, ## but WITHOUT ANY WARRANTY; without even the implied warranty of ## MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE. See the ## GNU General Public License for more details. library(&quot;dplyr&quot;) library(&quot;tidyr&quot;) library(&quot;STRbook&quot;) ## ------------------------------------------------------------------------ locs &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Stationinfo.dat&quot;, package = &quot;STRbook&quot;), col.names = c(&quot;id&quot;, &quot;lat&quot;, &quot;lon&quot;)) Times &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Times_1990.dat&quot;, package = &quot;STRbook&quot;), col.names = c(&quot;julian&quot;, &quot;year&quot;, &quot;month&quot;, &quot;day&quot;)) Tmax &lt;- read.table(system.file(&quot;extdata&quot;, &quot;Tmax_1990.dat&quot;, package = &quot;STRbook&quot;)) ## ------------------------------------------------------------------------ names(Tmax) &lt;- locs$id ## ------------------------------------------------------------------------ Tmax &lt;- cbind(Times, Tmax) head(names(Tmax), 10) ## ------------------------------------------------------------------------ Tmax_long &lt;- gather(Tmax, id, z, -julian, -year, -month, -day) head(Tmax_long) ## ------------------------------------------------------------------------ Tmax_long$id &lt;- as.integer(Tmax_long$id) ## ----------------------------------------------------------- nrow(Tmax_long) Tmax_long &lt;- filter(Tmax_long, !(z &lt;= -9998)) nrow(Tmax_long) ## ------------------------------------------------------------------------ Tmax_long &lt;- mutate(Tmax_long, proc = &quot;Tmax&quot;) head(Tmax_long) ## ------------------------------------------------------------------------ data(Tmin_long, package = &quot;STRbook&quot;) data(TDP_long, package = &quot;STRbook&quot;) data(Precip_long, package = &quot;STRbook&quot;) ## ------------------------------------------------------------------------ NOAA_df_1990 &lt;- rbind(Tmax_long, Tmin_long, TDP_long, Precip_long) ## ------------------------------------------------------------------------ summ &lt;- group_by(NOAA_df_1990, year, proc) %&gt;% # groupings summarise(mean_proc = mean(z)) # operation ## ------------------------------------------------------------------------ NOAA_precip &lt;- filter(NOAA_df_1990, proc == &quot;Precip&quot; &amp; month == 6) summ &lt;- group_by(NOAA_precip, year, id) %&gt;% summarise(days_no_precip = sum(z == 0)) head(summ) ## ------------------------------------------------------------------------ median(summ$days_no_precip) ## ------------------------------------------------------------- grps &lt;- group_by(NOAA_precip, year, id) summ &lt;- summarise(grps, days_no_precip = sum(z == 0)) ## ------------------------------------------------------------------------ NOAA_df_sorted &lt;- arrange(NOAA_df_1990, julian, id) ## ------------------------------------------------------------------------ df1 &lt;- select(NOAA_df_1990, julian, z) df2 &lt;- select(NOAA_df_1990, -julian) ## ------------------------------------------------------------------------ NOAA_df_1990 &lt;- left_join(NOAA_df_1990, locs, by = &quot;id&quot;) ## ------------------------------------------------------------------------ Tmax_long_sel &lt;- select(Tmax_long, julian, id, z) Tmax_wide &lt;- spread(Tmax_long_sel, id, z) dim(Tmax_wide) ## ------------------------------------------------------------------------ M &lt;- select(Tmax_wide, -julian) %&gt;% as.matrix() ## ----------------------------------------------------------- library(&quot;sp&quot;) library(&quot;spacetime&quot;) ## ------------------------------------------------------------------------ NOAA_df_1990$date &lt;- with(NOAA_df_1990, paste(year, month, day, sep = &quot;-&quot;)) head(NOAA_df_1990$date, 4) # show first four elements ## ------------------------------------------------------------------------ NOAA_df_1990$date &lt;- as.Date(NOAA_df_1990$date) class(NOAA_df_1990$date) ## ------------------------------------------------------------------------ Tmax_long2 &lt;- filter(NOAA_df_1990, proc == &quot;Tmax&quot;) STObj &lt;- stConstruct(x = Tmax_long2, # data set space = c(&quot;lon&quot;, &quot;lat&quot;), # spatial fields time = &quot;date&quot;) # time field class(STObj) ## ------------------------------------------------------------------------ spat_part &lt;- SpatialPoints(coords = Tmax_long2[, c(&quot;lon&quot;, &quot;lat&quot;)]) temp_part &lt;- Tmax_long2$date STObj2 &lt;- STIDF(sp = spat_part, time = temp_part, data = select(Tmax_long2, -date, -lon, -lat)) class(STObj2) ## ------------------------------------------------------------------------ spat_part &lt;- SpatialPoints(coords = locs[, c(&quot;lon&quot;, &quot;lat&quot;)]) temp_part &lt;- with(Times, paste(year, month, day, sep = &quot;-&quot;)) temp_part &lt;- as.Date(temp_part) ## ------------------------------------------------------------------------ Tmax_long3 &lt;- gather(Tmax, id, z, -julian, -year, -month, -day) ## ------------------------------------------------------------------------ Tmax_long3$id &lt;- as.integer(Tmax_long3$id) Tmax_long3 &lt;- arrange(Tmax_long3,julian,id) ## ------------------------------------------------------------------------ all(unique(Tmax_long3$id) == locs$id) ## ------------------------------------------------------------------------ STObj3 &lt;- STFDF(sp = spat_part, time = temp_part, data = Tmax_long3) class(STObj3) ## ------------------------------------------------------------------------ proj4string(STObj3) &lt;- CRS(&quot;+proj=longlat +ellps=WGS84&quot;) ## ------------------------------------------------------------------------ STObj3$z[STObj3$z == -9999] &lt;- NA 4.3 Interactive visualization (Interactive with HTML format only) 4.3.1 Interactive display of data ## first you need to install these packages # install.packages(&quot;webshot&quot;) # webshot::install_phantomjs() DT::datatable(iris) 4.3.2 Animations Lets animate the NOAA_df_1990 dataset data(&quot;NOAA_df_1990&quot;, package = &quot;STRbook&quot;) ## 48 unique months and years month_year &lt;- NOAA_df_1990 %&gt;% subset(day == 1 &amp; id == 3804 &amp; proc == &quot;Tmax&quot;) %&gt;% # group_by(year, month) %&gt;% select(month, year) ## limits of the temperature range zlim &lt;- NOAA_df_1990 %&gt;% subset(proc == &quot;Tmax&quot;) %&gt;% select(z) %&gt;% range() ## Only plot the states with data states &lt;- map_data(&quot;state&quot;) states &lt;- states %&gt;% subset(!(region %in% c( &quot;washington&quot;, &quot;oregon&quot;, &quot;california&quot;, &quot;nevada&quot;, &quot;idaho&quot;, &quot;utah&quot;, &quot;arizona&quot;,&quot;montana&quot;, &quot;wyoming&quot;, &quot;colorado&quot;, &quot;new mexico&quot; ))) ## generate a plotting function make_plot &lt;- function() { for (i in 1:nrow(month_year)) { p &lt;- NOAA_df_1990 %&gt;% subset( day == 1 &amp; proc == &quot;Tmax&quot; &amp; month == month_year$month[i] &amp; year == month_year$year[i] ) %&gt;% ggplot(aes(x = lon, y = lat, color = z)) + geom_point(size = 2) + geom_polygon(data = states, aes(x = long, y = lat, group = group), inherit.aes = FALSE, fill = NA, color = &quot;black&quot;) + scale_color_viridis_c(option = &quot;inferno&quot;, limits = zlim) + theme( plot.title = element_text(size = rel(2.5)) ) + ggtitle( paste(&quot;Tmax for the first day of month &quot;, month_year$month[i], &quot; in &quot;, month_year$year[i], sep = &quot;&quot;) ) print(p) } } if (!file.exists(here::here(&quot;images&quot;, &quot;NOAA_df_1990-animation.gif&quot;))) { gifski::save_gif( make_plot(), gif_file = here::here(&quot;images&quot;, &quot;NOAA_df_1990-animation.gif&quot;), progress = FALSE, delay = 0.5, height = 360, width = 640, units = &quot;px&quot; ) } knitr::include_graphics(here::here(&quot;images&quot;, &quot;NOAA_df_1990-animation.gif&quot;)) 4.3.3 Interactive plotting using plotly library(plotly) ## Plot Tmax for June 1991 p &lt;- NOAA_df_1990 %&gt;% subset( day == 1 &amp; proc == &quot;Tmax&quot; &amp; month == &quot;6&quot; &amp; year == &quot;1991&quot; ) %&gt;% ggplot(aes(x = lon, y = lat, color = z)) + geom_point(size = 2) + geom_polygon(data = states, aes(x = long, y = lat, group = group), inherit.aes = FALSE, fill = NA, color = &quot;black&quot;) + scale_color_viridis_c(option = &quot;inferno&quot;, limits = zlim) + theme( plot.title = element_text(size = rel(2.5)) ) + ggtitle( paste(&quot;Tmax for the first day of June, 1991&quot;) ) ## Check if the ploty map has been produced. If so, use it, otherwise ## run the code and produce the plot if (!file.exists(here::here(&quot;images&quot;, &quot;temp-june-1991.html&quot;))) { p_plotly &lt;- ggplotly(p, width = 800, height = 450) htmlwidgets::saveWidget(p_plotly, file = here::here(&quot;images&quot;, &quot;temp-june-1991.html&quot;)) } else { # htmlwidgets::saveWidget() htmltools::includeHTML(here::here(&quot;images&quot;, &quot;temp-june-1991.html&quot;)) } plotly .container-fluid.crosstalk-bscols { margin-left: -30px; margin-right: -30px; white-space: normal; } body > .container-fluid.crosstalk-bscols { margin-left: auto; margin-right: auto; } .crosstalk-input-checkboxgroup .crosstalk-options-group .crosstalk-options-column { display: inline-block; padding-right: 12px; vertical-align: top; } @media only screen and (max-width:480px) { .crosstalk-input-checkboxgroup .crosstalk-options-group .crosstalk-options-column { display: block; padding-right: inherit; } } slide:not(.current) .plotly.html-widget{ display: none; } "],
["day-5.html", "5 Day 5 5.1 Announcements 5.2 Spatial means and covariances", " 5 Day 5 5.1 Announcements HW Assignment Another datacamp coming soon library(tidyverse) library(fields) library(mvnfast) library(plotly) library(splines) set.seed(404) 5.2 Spatial means and covariances Let \\(\\{ y(\\mathbf{s}_i) \\}\\) be a set of observations of a process at locations \\(\\{ \\mathbf{s}_i \\in \\mathcal{D}, i = 1, \\ldots, n \\}\\). In one dimension, \\(y(\\mathbf{s})\\) is a curve n &lt;- 100 s &lt;- seq(0, 1, length = n) ## calculate the pairwise distance between locations ## rdist from the fields package is much faster than the dist function D &lt;- rdist(s, s) Sigma &lt;- exp( - D) dat &lt;- data.frame( s = s, y = c(rmvn(1, mu = rep(0, n), sigma = Sigma)) ) dat %&gt;% ggplot(aes(x = s, y = y)) + geom_line() + ylab(&quot;y(s)&quot;) In two dimensions, \\(y(\\mathbf{s})\\) is a surface n &lt;- 20^2 s &lt;- expand.grid( seq(0, 1, length = sqrt(n)), seq(0, 1, length = sqrt(n)) ) ## calculate the pairwise distance between locations ## rdist from the fields package is much faster than the dist function D &lt;- rdist(s, s) Sigma &lt;- exp( - D) dat &lt;- data.frame( s1 = s[, 1], s2 = s[, 2], y = c(rmvn(1, mu = rep(0, n), sigma = Sigma)) ) plot_ly( z = ~matrix(dat$y, sqrt(n), sqrt(n)) ) %&gt;% add_surface() 5.2.1 Gaussian processes A Gaussian process is an infinite-dimensional function (the function is defined for inginitely many locations \\(\\mathbf{s} \\in \\mathcal{D}\\)) with the property that the finite-dimensional vector \\(\\mathbf{y}(\\mathbf{s}) = (y(\\mathbf{s}_1), \\ldots, y(\\mathbf{s}_n) )&#39;\\) at any finite subset of locations \\(\\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\in \\mathcal{D}\\) has a multivariate Gaussian distribution. (A good book is available free online here: http://www.gaussianprocess.org/gpml/) 5.2.1.1 Mean and covariance A univariate normal distribution is fully characterized by a mean \\(\\mu\\) and a variance \\(\\sigma^2\\). A multivariate normal distribution is fully characeterized by a mean vector \\(\\boldsymbol{\\mu}\\) and a covariance matrix \\(\\boldsymbol{\\Sigma}\\). The mean is an \\(n\\)-dimensional vector with \\[\\begin{align*} E\\left( y(\\mathbf{s}) \\right) = \\boldsymbol{\\mu}(\\mathbf{s}) = \\begin{pmatrix} \\mu(\\mathbf{s}_1) \\\\ \\vdots \\\\ \\mu(\\mathbf{s}_n) \\end{pmatrix} \\end{align*}\\] The covariance matrix is an \\(n \\times n\\) matrix with \\[\\begin{align*} \\operatorname{Cov} \\left( y(\\mathbf{s}) \\right) &amp; = \\begin{bmatrix} \\operatorname{Var} \\left( y(\\mathbf{s}_1) \\right) &amp; \\operatorname{Cov} \\left( y(\\mathbf{s}_1), y(\\mathbf{s}_2) \\right) &amp; \\cdots &amp; \\operatorname{Cov} \\left( y(\\mathbf{s}_1), y(\\mathbf{s}_n) \\right) \\\\ \\operatorname{Cov} \\left( y(\\mathbf{s}_2), y(\\mathbf{s}_1) \\right) &amp; \\operatorname{Var} \\left( y(\\mathbf{s}_2) \\right) &amp; \\cdots &amp; \\operatorname{Cov} \\left( y(\\mathbf{s}_2), y(\\mathbf{s}_n) \\right) \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\operatorname{Cov} \\left( y(\\mathbf{s}_n), y(\\mathbf{s}_1) \\right) &amp; \\operatorname{Cov} \\left( y(\\mathbf{s}_n), y(\\mathbf{s}_2) \\right) &amp; \\cdots &amp; \\operatorname{Var} \\left( y(\\mathbf{s}_n) \\right) \\\\ \\end{bmatrix} \\end{align*}\\] Recall that the multivariate normal pdf is \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}] &amp; = (2 \\pi)^{-\\frac{n}{2}} |\\boldsymbol{\\Sigma}|^{-\\frac{1}{2}} e^{-\\frac{1}{2} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)&#39; \\boldsymbol{\\Sigma}^{-1} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)} \\end{align*}\\] Define the precision matrix \\(\\boldsymbol{\\Omega} = \\boldsymbol{\\Sigma}^{-1}\\). Then, the multivariate normal pdf can be written as \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Omega}] &amp; = (2 \\pi)^{-\\frac{n}{2}} |\\boldsymbol{\\Omega}|^{\\frac{1}{2}} e^{-\\frac{1}{2} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)&#39; \\boldsymbol{\\Omega} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)} \\end{align*}\\] 5.2.1.2 Mean and covariance functions A Gaussian process is fully characterized by a mean function \\(E\\left( y(\\mathbf{s}) \\right) = \\mu(\\mathbf{s})\\) that maps \\(\\mathcal{R}^d \\rightarrow \\mathcal{R}^1\\) (for a \\(d\\)-dimensional location \\(\\mathbf{s}\\) – typically \\(d=2\\)) and a covariance function \\(\\operatorname{Cov} \\left( y(\\mathbf{s}_i), y(\\mathbf{s}_j) \\right) = C(\\mathbf{s}, \\mathbf{s}&#39;)\\). This means that once you know the mean function \\(\\mu(\\mathbf{s})\\) and the covariance function \\(C(\\mathbf{s}, \\mathbf{s}&#39;)\\) you have full knowledge of the distribution Note: this is different than a multivariate normal distribution as this is an infinite-dimensional function – cannot be represented with a vector and/or matrix. Any finite realization of a GP has the pdf \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}] &amp; = (2 \\pi)^{-\\frac{n}{2}} |\\boldsymbol{\\Sigma}|^{-\\frac{1}{2}} e^{-\\frac{1}{2} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)&#39; \\boldsymbol{\\Sigma}^{-1} \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right)} \\end{align*}\\] where \\(\\boldsymbol{\\mu}\\) is determined by the function \\(\\mu(\\cdot)\\) and \\(\\boldsymbol{\\Sigma}\\) is determined by the function \\(C(\\cdot, \\cdot)\\). 5.2.1.3 The Gaussian process mean function There are many possible valid choices for the mean function \\(\\mu(\\mathbf{s})\\) (almost any possible function is allowed). Constant function: \\(\\mu(\\mathbf{s}) \\equiv \\beta_0\\) Spatial covariates: \\(\\mu(\\mathbf{s}) \\equiv \\mathbf{X}(\\mathbf{s}) \\boldsymbol{\\beta} = \\beta_0 + \\sum_{j=1}^p x_j(\\mathbf{s}) \\beta_j\\) Examples: elevation, distance to water, latitude Linear spatial trends: \\(\\mu(\\mathbf{s}_i) \\equiv \\beta_0 + \\beta_1 s_{i1} + \\beta_2 s_{i2}\\) Higher-order spatial trends: \\(\\mu(\\mathbf{s}_i) \\equiv \\sum_{j=1}^p f_j(\\mathbf{s}) \\beta_j\\) where \\(f_j(\\mathbf{s})\\) is some function of location \\(\\mathbf{s}\\) (i.e., B-splines, Fourier bases, wavelets, etc.) How to choose: AIC / BIC / cross-validation 5.2.1.4 Example n &lt;- 1000 X &lt;- seq(0, 1, length = n) X_bs &lt;- bs(X, df = 10) beta &lt;- rnorm(ncol(X_bs)) y &lt;- X * 2 + X_bs %*% beta + rnorm(n, 0, 0.25) dat &lt;- data.frame(X = X, y = y, mu = X * 2 + X_bs %*% beta) dat %&gt;% ggplot(aes(x = X, y = y)) + geom_point() + geom_line(aes(x = X, y = mu), color = &quot;red&quot;) A simple mean structure can leave behind a strong residual covariance structure dat$simple &lt;- predict(lm(y ~ X)) dat$simple_resids &lt;- resid(lm(y ~ X)) dat %&gt;% ggplot(aes(x = X, y = y)) + geom_point() + geom_line(aes(x = X, y = mu), color = &quot;red&quot;, lwd = 2) + geom_line(aes(x = X, y = simple), color = &quot;blue&quot;, lwd = 2) + ggtitle(&quot;Simple linear fit&quot;) + theme(plot.title = element_text(size = 30)) dat %&gt;% ggplot(aes(x = X, y = simple_resids)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, lwd = 2) + ggtitle(&quot;Simple linear fit residuals&quot;) + theme(plot.title = element_text(size = 30)) A complex mean structure can lead to independent residuals dat$complex &lt;- predict(lm(y ~ X + X_bs)) dat$complex_resids &lt;- resid(lm(y ~ X + X_bs)) dat %&gt;% ggplot(aes(x = X, y = y)) + geom_point() + geom_line(aes(x = X, y = mu), color = &quot;red&quot;, lwd = 2) + geom_line(aes(x = X, y = complex), color = &quot;blue&quot;, lwd = 2) + ggtitle(&quot;Complex spline fit&quot;) + theme(plot.title = element_text(size = 30)) dat %&gt;% ggplot(aes(x = X, y = complex_resids)) + geom_point() + geom_hline(yintercept = 0, color = &quot;red&quot;, lwd = 2) + ggtitle(&quot;Complex spline fit residuals&quot;) + theme(plot.title = element_text(size = 30)) How do you interpret this residual correlation? Missing covariates that have a spatial pattern For example, what if you are modeling temperature in a mountainous region and don’t include elevation as a covariate? Advection/diffusion processes Example: the wind blowing, the spread of disease 5.2.1.5 Gaussian Process Covariance Functions Unlike the mean functions, only specific covariance functions are valid. The covaraince function at a finite subset of \\(n\\) points is called the covariance matrix. For a covariance matrix to be from a valid covariance function, the covariance matrix \\(\\boldsymbol{\\Sigma}\\) must be symmetric and positive-definite. A matrix \\(\\boldsymbol{\\Sigma}\\) is symmetric if \\(\\boldsymbol{\\Sigma}&#39; = \\boldsymbol{\\Sigma}\\) A matrix \\(\\boldsymbol{\\Sigma}\\) is positive definite iff and only if \\[\\begin{align*} \\boldsymbol{\\Sigma} \\mbox{ is positive definite } &amp; \\iff \\\\ \\mathbf{z}&#39; \\boldsymbol{\\Sigma} \\mathbf{z} \\geq 0 \\hspace{1em} \\forall \\mathbf{z} \\in \\mathcal{R}^n &amp; \\iff \\\\ \\mbox{all eigenvalues of } \\boldsymbol{\\Sigma} \\mbox{ are strictly positive} &amp; \\iff \\\\ |\\boldsymbol{\\Sigma}| &gt; 0 \\end{align*}\\] Therefore, the covariance fuction \\(C(\\mathbf{s}_i, \\mathbf{s}_j)\\) is a valid covariance function if the \\(n \\times n\\) covariance matrix at any finite collection of \\(n\\) locations \\(\\mathbf{s}_1, \\ldots, \\mathbf{s}_n\\) has the properties symmetry: \\(C(\\mathbf{s}_i, \\mathbf{s}_j) = C(\\mathbf{s}_j, \\mathbf{s}_i) \\hspace{1em} \\forall \\mathbf{s}_i, \\mathbf{s}_j\\) positive definite: \\(\\sum_{i=1}^n \\sum_{j=1}^n z_i z_j C(\\mathbf{s}_i, \\mathbf{s}_j) &gt; 0 \\hspace{1em} \\forall n, \\mathbf{s}_1, \\ldots, \\mathbf{s}_n, \\mbox{ and } z_1, \\ldots, z_n \\in \\mathcal{R}\\) proving that these properties hold is hard – Often rely on spectral methods (showing the eigenvalues of the function are all strictly positive). "],
["day-6.html", "6 Day 6 6.1 Announcements 6.2 Gaussian process assumptions 6.3 Recall Hierachical modeling", " 6 Day 6 6.1 Announcements Assigned reading: Model Selection for Geostatistical Models library(tidyverse) library(fields) library(mvnfast) library(gstat) library(sp) 6.2 Gaussian process assumptions To fit a Gaussian process, we have to make assumptions Why? – The mean and covariance functions are infinite dimensional but we only observe a finite vector \\(\\mathbf{y} = (y(\\mathbf{s}_1), \\ldots, y(\\mathbf{s}_1))&#39;\\) so we cannot fully specify the model Stationarity Strict stationarity: the probability density function is invariant to shifts \\([y(\\mathbf{s}_1), \\ldots, y(\\mathbf{s}_n)] = [y(\\mathbf{s}_1 + \\mathbf{h}), \\ldots, y(\\mathbf{s}_n + \\mathbf{h})] \\hspace{1em} \\forall \\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\in \\mathcal{D} \\mbox{ and } \\mathbf{h}\\) (such that all points shifted by the vector \\(\\mathbf{h}\\) are also in \\(\\mathcal{D}\\)) Weak stationarity: mean and covariance function are stationary \\(E\\left( y(\\mathbf{s}) \\right) = E\\left( y(\\mathbf{s} + \\mathbf{h}) \\right) = \\mu \\hspace{1em} \\forall \\mathbf{h}\\) \\(C\\left( y(\\mathbf{s}), y(\\mathbf{s} + \\mathbf{h}) \\right) = C\\left( y(\\mathbf{0}), y(\\mathbf{h}) \\right) = C(\\mathbf{h}) \\hspace{1em} \\forall \\mathbf{s}, \\mathbf{h}\\) This implies \\(C\\left( y(\\mathbf{s}_i), y(\\mathbf{s}_j) \\right) = C(\\mathbf{s}_i - \\mathbf{s}_j) = C(\\mathbf{d}_{ij})\\) where \\(\\mathbf{d}_{ij} = \\mathbf{s}_i - \\mathbf{s}_j\\) is the difference vector between location \\(\\mathbf{s}_i\\) and location \\(\\mathbf{s}_j\\) Intrinsic stationarity (weakest form) \\(Var\\left( y(\\mathbf{s} + h) - y(\\mathbf{s}) \\right)\\) depends only on \\(\\mathbf{h}\\). Note: intrinsic stationarity doesn’t imply weak stationarity. Example: Brownian motion is intrisically stationary but is not weakly stationary. knitr::include_graphics(here::here(&quot;images&quot;, &quot;covariance-shift.jpg&quot;)) Can you think of a process that is not stationary? Note: for Gaussian processes, a weak stationarity implies strong stationarity. Isotropy A covariance function is isotropic if it is invariant to direction and rotation. \\(C\\left( y(\\mathbf{s}_i), y(\\mathbf{s}_j) \\right) = C(d_{ij})\\) where \\(d_{ij} = \\|\\mathbf{s}_i - \\mathbf{s}_j\\|\\) is the distance (typically Euclidean) between \\(\\mathbf{s}_i\\) and \\(\\mathbf{s}_j\\) Can you think of examples where other distances (not Euclidean) might be better? A covariance function that is not isotropic is called anisotropic Insert drawing here Can you think of a process that is anisotropic? 6.3 Recall Hierachical modeling 6.3.1 Data Model \\[\\begin{align*} y(\\mathbf{s}) &amp; = z(\\mathbf{s}) + \\varepsilon(\\mathbf{s}) \\\\ \\tag{6.1} \\varepsilon(\\mathbf{s}) \\stackrel{iid}{\\sim} N(0, \\sigma^2) \\end{align*}\\] \\(y(\\mathbf{s})\\) is the observation at site \\(\\mathbf{s}\\) \\(z(\\mathbf{s})\\) is the process of interest at site \\(\\mathbf{s}\\) \\(\\varepsilon(\\mathbf{s}) \\sim N(0, \\sigma^2)\\) is the measurement error Commonly called the nugget Geostatistics came from mining Microsite-variability What is a process that would have a small nugget? What is a process that would have a large nugget? \\(z(\\mathbf{s})\\) is the process of interest at site \\(\\mathbf{s}\\) Process Model (using Gaussian process) \\[\\begin{align*} z(\\mathbf{s}) &amp; = \\mu(\\mathbf{s}) + \\eta(\\mathbf{s}) \\tag{6.2} \\end{align*}\\] \\(\\mu(\\mathbf{s})\\) is the GP mean function \\(\\boldsymbol{\\eta} = (\\eta(\\mathbf{s}_1), \\ldots, \\eta(\\mathbf{s}_n))&#39; \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma})\\) \\(Cov(\\eta(\\mathbf{s}_i), \\eta(\\mathbf{s}_j)) = \\tau^2 C(d_{ij})\\) \\(C(d_ij)\\) is a correlation function \\(\\tau^2\\) is often called the partial sill parameter set.seed(101) n &lt;- 40 s &lt;- runif(n) ## mean function mu &lt;- s ## covariance function -- this is positive definite Sigma &lt;- 2 * exp( - rdist(s) / 2) ## generate the spatially (1-d) correlated process ## rmvn returns a matrix -- use c() to coerce to a vector eta &lt;- c(rmvn(1, rep(0, n), Sigma)) z &lt;- mu + eta epsilon &lt;- rnorm(n, 0, 0.25) y &lt;- z + epsilon dat &lt;- data.frame( s = s, y = y, z = z, mu = mu, eta = eta, epsilon = epsilon ) dat %&gt;% ggplot(aes(x = s, y = y)) + geom_point() + geom_segment(aes(xend = s, yend = z), alpha = 0.5) + geom_line(aes(x = s, y = z), color = &quot;blue&quot;) + geom_line(aes(x = s, y = mu), color = &quot;red&quot;) + geom_line(aes(x = s, y = eta), color = &quot;purple&quot;) + theme_bw() A quick check if Sigma meets the conditions of a symmetric positive-definite matrix ## check is Sigma is symmetric all.equal(Sigma, t(Sigma)) ## [1] TRUE ## check is Sigma is posivitive definite Sigma_eigen &lt;- eigen(Sigma) ## check that all eigenvalues are positive Sigma_eigen$values ## [1] 68.614998226 6.469640157 1.846887044 1.013225060 0.526987113 ## [6] 0.350879290 0.221671629 0.170657210 0.116756563 0.076631073 ## [11] 0.067533590 0.058658782 0.053014720 0.043488483 0.037433411 ## [16] 0.036668397 0.032544320 0.030261646 0.027404563 0.023218999 ## [21] 0.021154516 0.021154380 0.017782847 0.016717572 0.014563450 ## [26] 0.013423514 0.010922956 0.009497946 0.008886870 0.008231217 ## [31] 0.007208472 0.005444425 0.005026785 0.004599434 0.004534839 ## [36] 0.004238142 0.003219602 0.002366398 0.001348648 0.001117710 all(Sigma_eigen$values &gt; 0) ## [1] TRUE Variance decomposition Combining the equations (6.1) and (6.2) \\[\\begin{align*} y(\\mathbf{s}) &amp; = \\mu(\\mathbf{s}) + \\eta(\\mathbf{s}) + \\varepsilon(\\mathbf{s}) \\end{align*}\\] we can have \\[\\begin{align*} Var \\left( y(\\mathbf{s}) \\right) &amp; = Var \\left( \\mu(\\mathbf{s}) + \\eta(\\mathbf{s}) + \\varepsilon(\\mathbf{s}) \\right) \\\\ &amp; = Var \\left( \\mu(\\mathbf{s}) \\right) + Var \\left( \\eta(\\mathbf{s}) \\right) + Var \\left( \\varepsilon(\\mathbf{s}) \\right) + \\\\ &amp; \\hspace{2em} 2 Cov \\left( \\mu(\\mathbf{s}), \\varepsilon(\\mathbf{s}) \\right) + 2 Cov \\left( \\mu(\\mathbf{s}), \\eta(\\mathbf{s}) \\right) + 2 Cov \\left( \\eta(\\mathbf{s}), \\varepsilon(\\mathbf{s}) \\right) \\\\ &amp; = 0 + Var \\left( \\eta(\\mathbf{s}) \\right) + Var \\left( \\varepsilon(\\mathbf{s}) \\right) + 0 + 0 + 0 \\\\ &amp; = \\tau^2 + \\sigma^2 \\end{align*}\\] the total variance is the sill \\(\\tau^2 + \\sigma^2\\) "],
["day-7.html", "7 Day 7 7.1 Common isotropic correlation functions 7.2 Covariograms and semivariograms", " 7 Day 7 library(tidyverse) library(fields) library(mvnfast) library(gstat) library(sp) library(MCMCpack) 7.1 Common isotropic correlation functions Tobler’s law of geography “Everything is related to everything else, but near things are more related than distant things” These functions follow Tobler’s law in that the function decays with distance These functions are proven to be symmetric and positive definite, thus are valid correlation functions ## make a function to plot the correlation functions plot_corr_function &lt;- function(corr_fun, ## notice that this input is a function d = seq(0, 10, length.out = 1000), phi = c(0.1, 0.5, 1, 5, 10), title = NULL, ...) { C_h &lt;- matrix(0, length(d), length(phi)) for (i in 1:length(phi)) { C_h[, i] &lt;- corr_fun(d, phi[i], ...) } print( data.frame(d = d, C_h = c(C_h), phi = factor(rep(phi, each = length(d)))) %&gt;% ggplot(aes(x = d, y = C_h, group = phi, color = phi)) + geom_line() + ylim(c(0, 1)) + ggtitle(title) ) } 7.1.0.1 Exponential correlation function \\[\\begin{align*} C(d) &amp; = e^{- \\frac{d}{\\phi} } \\end{align*}\\] where \\(\\phi\\) is the spatial range parameter (called the length-scale in Gaussian process literature). Note that this function is not differentiable at 0. exponential_cor &lt;- function (d, phi, ...) { return(exp( - d / phi)) } plot_corr_function(exponential_cor, title = &quot;Exponential correlation function&quot;) Sometimes this is parameterized using the inverse spatial range \\(\\theta = \\frac{1}{\\phi}\\) \\[\\begin{align*} C(d) &amp; = e^{- d\\theta} \\end{align*}\\] 7.1.0.2 Squared exponential (Gaussian) correlation functions \\[\\begin{align*} C(d) &amp; = e^{- (\\frac{d}{\\phi})^2 } \\end{align*}\\] Notice that this function is differentiable at 0. gaussian_cor &lt;- function (d, phi, ...) { return(exp( - (d / phi)^2)) } plot_corr_function(gaussian_cor, title = &quot;Gaussian (squared exponential) correlation function&quot;) 7.1.0.3 Powered exponential correlation functions \\[\\begin{align*} C(d) &amp; = e^{- (\\frac{d}{\\phi})^k } \\end{align*}\\] powered_exp_cor &lt;- function (d, phi, k, ...) { return(exp( - (d / phi)^k)) } plot_corr_function(powered_exp_cor, k = 1, title = &quot;Powered exponential correlation function, k = 1&quot;) plot_corr_function(powered_exp_cor, k = 2, title = &quot;Powered exponential correlation function, k = 2&quot;) plot_corr_function(powered_exp_cor, k = 3, title = &quot;Powered exponential correlation function, k = 3&quot;) plot_corr_function(powered_exp_cor, k = 4, title = &quot;Powered exponential correlation function, k = 4&quot;) For future classes (basis representations) Visual exploration of Gaussian Processes 7.1.0.4 Matern correlation functions \\[\\begin{align*} C(d) &amp; = \\frac{2^{1 - \\nu}}{\\Gamma(\\nu)} \\left( \\sqrt{2 \\nu} \\frac{d}{\\phi} \\right)^\\nu K_\\nu \\left( \\sqrt{2 \\nu} \\frac{d}{\\phi} \\right) \\end{align*}\\] \\(\\Gamma(\\cdot)\\) is the gamma function \\(K_\\nu(\\cdot)\\) is the modified Bessel function of the second kind \\(\\phi\\) is the range parameter \\(\\nu\\) is the smoothness parameter ## Use the Matern fucnction from the fields library plot_corr_function(Matern, range = c(0.1, 0.5, 1, 5, 10), smoothness = 0.15, title = &quot;Matern correlation function, smoothness = 0.15&quot;) plot_corr_function(Matern, range = c(0.1, 0.5, 1, 5, 10), smoothness = 0.5, title = &quot;Matern correlation function, smoothness = 0.5&quot;) plot_corr_function(Matern, range = c(0.1, 0.5, 1, 5, 10), smoothness = 5, title = &quot;Matern correlation function, smoothness = 5&quot;) plot_corr_function(Matern, range = c(0.1, 0.5, 1, 5, 10), smoothness = 50, title = &quot;Matern correlation function, smoothness = 50&quot;) Special Cases \\(\\nu = \\frac{1}{2} \\Rightarrow C(d) = e^{- \\frac{d}{\\phi} }\\) is the exponential correlation function \\(\\nu = \\frac{3}{2} \\Rightarrow C(d) = \\left(1 - \\frac{\\sqrt{3}d}{\\phi} \\right) e^{- \\frac{\\sqrt{3}d}{\\phi} }\\) \\(\\nu = \\frac{5}{2} \\Rightarrow C(d) = \\left(1 - \\frac{\\sqrt{5}d}{\\phi} + \\frac{5 d^2}{3 \\phi^2} \\right) e^{- \\frac{\\sqrt{5}d}{\\phi} }\\) \\(\\nu \\rightarrow \\infty \\Rightarrow C(d) = e^{- \\frac{1}{2} \\left( \\frac{d}{\\phi} \\right)^2 }\\) is the Gaussian correlation function Note: these functions are valid in \\(\\mathcal{R}^2\\). There are generalizations of the functions to other geometries (spheres, stream networks, etc.) and higher dimensions (\\(\\mathcal{R}^d\\)). The Gaussian process with a Matern correlation function with parameter \\(\\nu\\) is \\(\\lceil \\nu \\rceil\\)-1 times differentiable in the mean-square sense d &lt;- seq(0, 10, length = 1000) nu &lt;- c(1/2, 3/2, 5/2) C_h &lt;- c( Matern(d, range = 1, nu = nu[1]), Matern(d, range = 1, nu = nu[2]), Matern(d, range = 1, nu = nu[3]) ) dat &lt;- data.frame( d = d, C_h = C_h, nu = factor(rep(nu, each = length(d))) ) ggplot(dat, aes(x = d, y = C_h, group = nu, color = nu)) + geom_line() + ggtitle(&quot;Matern correlation functions&quot;) ## simulate some Gaussian processes dat$y &lt;- c( rmvn( 1, mu = rep(0, 1000), sigma = Matern(rdist(seq(0, 10, length = 1000)), range = 1, nu = nu[1]) ), rmvn( 1, mu = rep(0, 1000), sigma = Matern(rdist(seq(0, 10, length = 1000)), range = 1, nu = nu[2]) ), rmvn( 1, mu = rep(0, 1000), sigma = Matern(rdist(seq(0, 10, length = 1000)), range = 1, nu = nu[3]) ) ) ggplot(dat, aes(x = d, y = y, group = nu, color = nu)) + geom_line() + ggtitle(&quot;Gaussian process realizations&quot;) 7.2 Covariograms and semivariograms How do we choose a covariance function? How do we fit a covariance function? How do we check for isotropy? 7.2.1 Semivariograms and variograms The semivariogram is defined \\[\\begin{align*} \\gamma(\\mathbf{s}_i, \\mathbf{s}_j) &amp; \\equiv \\frac{1}{2} Var(y(\\mathbf{s}_i) - y(\\mathbf{s}_j)) \\\\ &amp; = \\frac{1}{2}E\\left( \\left( \\left( y(\\mathbf{s}_i) - \\mu(\\mathbf{s}_i) \\right) - \\left( y(\\mathbf{s}_j) - \\mu(\\mathbf{s}_j) \\right) \\right)^2 \\right) \\end{align*}\\] If the covaraince is stationary this can be written as a function of directional spatial lags \\(\\mathbf{h}_{ij} = \\mathbf{s}_i - \\mathbf{s}_j\\) (e.g., \\(\\mathbf{h}_{ij}\\) is not required to equal \\(\\mathbf{h}_{ji}\\)). \\[\\begin{align*} \\gamma(\\mathbf{h}) &amp; \\equiv \\frac{1}{2} Var(y(\\mathbf{s} + \\mathbf{h}) - y(\\mathbf{s})) \\\\ &amp; = \\frac{1}{2} E\\left( \\left( \\left( y(\\mathbf{s} + \\mathbf{h}) - \\mu(\\mathbf{s} + \\mathbf{h}) \\right) - \\left( y(\\mathbf{s}) - \\mu(\\mathbf{s}) \\right) \\right)^2 \\right) \\\\ &amp; = \\frac{1}{2} E\\left( \\left( y(\\mathbf{s} + \\mathbf{h}) - \\mu(\\mathbf{s} + \\mathbf{h}) \\right)^2 \\right) - 2 E\\left( \\left( y(\\mathbf{s} + \\mathbf{h}) - \\mu(\\mathbf{s} + \\mathbf{h}) \\right) \\left( y(\\mathbf{s}) - \\mu(\\mathbf{s}) \\right) \\right) + \\\\ &amp; \\hspace{6em} \\frac{1}{2} E\\left( \\left( y(\\mathbf{s}) - \\mu(\\mathbf{s}) \\right)^2 \\right) \\\\ &amp; = \\frac{1}{2} C(\\mathbf{0}) - C(\\mathbf{h}) + \\frac{1}{2} C(\\mathbf{0}) \\\\ &amp; = C(\\mathbf{0}) - C(\\mathbf{h}) \\end{align*}\\] This implies \\[\\begin{align*} \\gamma(\\mathbf{h}) &amp; = C(\\mathbf{0}) - C(\\mathbf{h}) \\\\ &amp; \\mbox{or} \\\\ C(\\mathbf{h}) &amp; = C(\\mathbf{0}) - \\gamma(\\mathbf{h}) \\end{align*}\\] The variogram is defined as \\(2 \\gamma(\\mathbf{h})\\). 7.2.1.1 Properties of variograms \\(\\gamma(\\mathbf{s}_i, \\mathbf{s}_j) \\geq 0\\) because it is the expectation of a square \\(\\gamma(\\mathbf{s}_i, \\mathbf{s}_i) = \\gamma_i(0) = 0\\) because \\(y(\\mathbf{s}_1) - y(\\mathbf{s}_1) = 0\\) 7.2.1.2 Estimation of variograms It is difficult to estimate the variogram as there is no replication across space – need to pool information across space. 7.2.1.3 Non-directional variograms Using the meuse dataset of heavy metal concntrations in the topsoil near the Meuse river near Stein, NL, we can explore the concentration of zinc (log-scale). data(&quot;meuse&quot;) data(&quot;meuse.grid&quot;) coordinates(meuse) = ~ x + y bubble(meuse, &quot;zinc&quot;) Let’s explore the correlation in the data with respect to the log(zinc) variable vg &lt;- variogram(log(zinc) ~ 1, data = meuse) fit_vg &lt;- fit.variogram(vg, model = vgm(1, &quot;Exp&quot;)) plot(vg, fit_vg, as.table = TRUE) 7.2.1.4 Checking for anisotropy – Directional variograms Let’s explore the directional correlation in the data with respect to the log(zinc) variable dir_variogram &lt;- variogram(log(zinc) ~ 1, data = meuse, alpha = c(0, 45, 90, 135)) fit_variogram &lt;- fit.variogram(dir_variogram, model = vgm(1, &quot;Exp&quot;)) plot(dir_variogram, fit_variogram, as.table = TRUE) You can explore this variogram as a map map_variogram &lt;- variogram(log(zinc) ~ 1, data = meuse, cutoff = 1500, width = 100, map = TRUE) plot(map_variogram, threshold = 5) which appears to show some patterns in the variogram maps suggesting anisotropy. Perhaps there is a covariate that can explain this. Let’s use the square-root distance to the river plot(log(zinc) ~ sqrt(dist), meuse) abline(lm(log(zinc) ~ sqrt(dist), meuse), col = &quot;red&quot;) Now, we can look at the directional varigrams of the residuals in the model after accounting for square-root distance to the river dir_variogram &lt;- variogram(log(zinc) ~ sqrt(dist), data = meuse, alpha = c(0, 45, 90, 135)) fit_variogram &lt;- fit.variogram(dir_variogram, model = vgm(1, &quot;Exp&quot;)) plot(dir_variogram, fit_variogram, as.table = TRUE) where these variograms seem to fit the data better. You can explore this variogram as a map map_variogram &lt;- variogram(log(zinc) ~ sqrt(dist), data = meuse, cutoff = 1500, width = 100, map = TRUE) plot(map_variogram, threshold = 5) which shows that after modeling log(zinc) as a function of the square-root distance to the river, the residuals appear isotropic left in the residuals. "],
["day-8.html", "8 Day 8 8.1 Announcements 8.2 Estimation of the spatial process 8.3 Maximum likelihood", " 8 Day 8 library(tidyverse) library(fields) library(mvnfast) library(gstat) library(sp) library(MCMCpack) 8.1 Announcements Finish the last of Day 7 notes… 8.2 Estimation of the spatial process Assume we have the model \\[\\begin{align*} y(\\mathbf{s}) &amp; = \\mu(\\mathbf{s}) + \\eta(\\mathbf{s}) + \\varepsilon(\\mathbf{s}) \\end{align*}\\] then, \\(Cov \\left( y(\\mathbf{s}_i), y(\\mathbf{s}_j ) \\right) = \\sigma^2 C(d_{ij} | \\nu, \\phi) + \\tau^2 I\\{i = j\\}\\) where \\(C(d_{ij} | \\nu, \\phi)\\) is a Matern correlation function with smoothness parameter \\(\\nu\\) and range parameter \\(\\phi\\). We observe the data \\(\\mathbf{y} = (y(\\mathbf{s}_1), \\ldots, y(\\mathbf{s}_n))&#39;\\) at \\(n\\) locations \\(\\mathbf{s}_1, \\ldots, \\mathbf{s}_n\\). \\(\\mathbf{y} \\sim N(\\boldsymbol{\\mu}, \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta}))\\). \\(\\boldsymbol{\\mu} = \\mathbf{X}\\left( \\mathbf{s} \\right) \\boldsymbol{\\beta}\\) is the model for the mean process given the \\(n \\times p\\) covariate matrix \\(\\mathbf{X}(\\mathbf{s})\\) and \\(\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})\\) is the covariance matrix with parameters \\(\\boldsymbol{\\theta} = (\\tau^2, \\sigma^2, \\nu, \\phi)&#39;\\). To fit the model, we need to estimate \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\theta}\\). Traditional statistical methods: use replication With \\(k = 1, \\ldots, K\\) replications of the spatial process \\(\\mathbf{y}_k\\), we can estimate the spatial mean as \\[\\begin{align*} \\widehat{\\boldsymbol{\\mu}} = \\frac{1}{K} \\sum_{k=1}^K \\mathbf{y}_k \\end{align*}\\] and the spatial mean as \\[\\begin{align*} \\widehat{\\boldsymbol{\\Sigma}} = \\frac{1}{K} \\sum_{k=1}^K \\left( \\mathbf{y}_k - \\widehat{\\boldsymbol{\\mu}} \\right) \\left( \\mathbf{y}_k - \\widehat{\\boldsymbol{\\mu}} \\right)&#39; \\end{align*}\\] However, we don’t have replication – we only have the single observation \\(\\mathbf{y}\\). We will explore different estimation methods using 1) variograms, 2) maximum liklihood, and 3) Bayesian methods 8.2.1 Estimation of the spatial process using variograms First, fit a model to the mean to estimate \\(\\hat{\\mu}(\\mathbf{s})\\) (use maximum likelihood, least-squares, etc.) Next, generate a sequence of \\(B\\) bins based on distance and group each pair of points \\((\\mathbf{s}_i, \\mathbf{s}_j)\\) into a bin Example bins: [0, 1), [1, 2), [2, 3), Place the pair of observations \\(\\mathbf{s}_i\\) and \\(\\mathbf{s}_j\\) that are seperated by \\(d_{b} \\in [d_b - \\epsilon, d_b + \\epsilon)\\) into one of the \\(B\\) bins. Calculate the average of the variogram within each bin For each of the \\(k\\) bins that have \\(m_k\\) points in each bin, the variogram estimate for bin \\(k\\) centered at the bin interval \\(\\mathbf{h}_k\\) is \\[\\begin{align*} \\hat{\\gamma}(\\mathbf{h}_k) = \\frac{1}{m_k} \\sum_{\\ell=1}^{m_k} \\left( y(\\mathbf{s}_{\\ell_1}) - y(\\mathbf{s}_{\\ell_2}) \\right)^2 \\end{align*}\\] for the \\(\\ell\\)th pair of locations \\(\\mathbf{s}_{\\ell_1}\\) and \\(\\mathbf{s}_{\\ell_2}\\) insert empirical variogram plot from class here Can estimate the parameters “by eye” or using least squares \\[\\begin{align*} \\hat{\\boldsymbol{\\theta}} &amp; = (\\hat{\\tau}^2, \\hat{\\sigma}^2, \\hat{\\phi}, \\hat{\\nu})&#39; \\\\ &amp; = \\underset{\\tau^2, \\sigma^2, \\phi, \\nu}{\\operatorname{argmax}} \\sum_{b=1}^B \\left( \\hat{\\gamma}(d_b) - \\gamma(d_b)\\right)^2 w_b \\\\ &amp; = \\underset{\\tau^2, \\sigma^2, \\phi, \\nu}{\\operatorname{argmax}} \\sum_{b=1}^B \\left( \\hat{\\gamma}(d_b) - \\left( \\sigma^2 + \\tau^2 C \\left( d_b | \\phi, \\nu \\right) \\right) \\right)^2 w_b \\tag{8.1} \\end{align*}\\] given the correlation function \\(C \\left( d_b | \\phi, \\nu \\right)\\) and a set of weights \\(w_b\\). 8.2.1.1 Estimation of the mean function What is the least squares estimator of the mean function? Recall, if \\(\\mathbf{y} \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma})\\), then \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X} \\mathbf{y}\\) is an unbiased estimator. \\[\\begin{align*} E(\\hat{\\boldsymbol{\\beta}}) &amp; = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X} E(\\mathbf{y}) \\\\ &amp; = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X}\\mathbf{X} \\boldsymbol{\\beta} \\\\ &amp; = \\boldsymbol{\\beta} \\end{align*}\\] However, \\[\\begin{align*} Cov(\\hat{\\boldsymbol{\\beta}}) &amp; = (\\mathbf{X}&#39;\\mathbf{X})^{-1}\\mathbf{X} \\boldsymbol{\\Sigma} \\mathbf{X} (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\\\ &amp; \\neq \\sigma^2 (\\mathbf{X}&#39;\\mathbf{X})^{-1} \\end{align*}\\] which is the least squares covariance estimate of \\(\\hat{\\boldsymbol{\\beta}}\\). Thus the least squares estimate has a biased covariance estimate. Given the fitted covariance matrix \\(\\hat{\\boldsymbol{\\Sigma}}\\) from the variogram, an updated mean function estimate is \\[\\begin{align*} \\hat{\\boldsymbol{\\beta}} &amp; = (\\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{X})^{-1} \\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{y} \\end{align*}\\] and the covariance is \\[\\begin{align*} Cov(\\hat{\\boldsymbol{\\beta}}) &amp; = (\\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{X})^{-1} \\end{align*}\\] This suggests that an iterative approach can be used to fit the model estimate the mean function using the estimated mean function, update the covariance function repeat steps 1 and 2 until convergence Any issues? Uncertainty estimation? How do you propogate parameter uncertainty? Prediction uncertainty is likely to be too small 8.2.2 Maximum Likelihood 8.2.2.1 The likelihood To understand the benefits of Bayesian analysis, it is useful to recall the likelihood framework. Assume that our model can be simplified so that the data are \\(\\mathbf{y} = (y_1, \\ldots, y_N)&#39;\\) and the parameters are \\(\\boldsymbol{\\theta}\\). Before observation, the data \\(\\mathbf{y}\\) are considered a random variable. After observation, the data \\(\\mathbf{y}\\) are considered fixed and known. The likelihood is a formal representation of how likely the data \\(\\mathbf{y}\\) are to arise given a probability distribution with parameters \\(\\boldsymbol{\\theta}\\). The likelihood function \\(L(\\boldsymbol{\\theta} | \\mathbf{y})\\) is defined as \\[\\begin{align*} L(\\boldsymbol{\\theta} | \\mathbf{y}) &amp; = [\\mathbf{y} | \\boldsymbol{\\theta}], \\end{align*}\\] where the function says that the likelihood of the parameter conditional on the data is is the conditional probability density of the data conditional on the parameters. If one assumes the observations are conditionally independent given the parameters \\(\\boldsymbol{\\theta}\\), the likelihood can be written as \\[\\begin{align*} L(\\boldsymbol{\\theta} | \\mathbf{y}) &amp; = \\prod_{i=1}^N [y_i | \\boldsymbol{\\theta}], \\end{align*}\\] Note the distinction between the use of the term likelihood function for the left side of the equation with the use of the term likelihood in Bayesian statistics to describe the right side of the above equation. A distinction between the likelihood function and the probability distribution function is in what is considered the random variable. In the likelihood function, the data \\(\\mathbf{y}\\) is assumed known and the parameters \\(\\boldsymbol{\\theta}\\) are random. For the probability distribution function, the data \\(\\mathbf{y}\\) are considered are random variable conditional on a fixed, known parameter \\(\\boldsymbol{\\theta}\\). To illustrate this difference, consider a univariate gamma probability distribution with shape parameter \\(\\alpha = 10\\) and scale parameter \\(\\theta=10\\). If we observe a single \\(y=2\\), the following figure shows the density function and likelihood function, with \\(\\alpha\\) assumed fixed and known. library(latex2exp) ## ## Attaching package: &#39;latex2exp&#39; ## The following object is masked from &#39;package:plotly&#39;: ## ## TeX y &lt;- 2 alpha &lt;- 10 theta &lt;- 10 density_function &lt;- function (x) { return(dgamma(x, alpha, theta)) } likelihood_function &lt;- function (x) { return(dgamma(y, alpha, x)) } layout(matrix(1:2, 2, 1)) ## plot density function curve(density_function(x), 0, 4, main = &quot;Density function&quot;, xlab=&quot;y&quot;, ylab=TeX(&quot;$\\\\lbrack$y|$\\\\theta$ $\\\\rbrack$&quot;)) text(1, 0.4, paste0( &quot;Area = &quot;, round(integrate(density_function, 0, Inf)[1]$value, digits=2))) points(2, density_function(2), col=&quot;red&quot;, pch=16) curve(likelihood_function(x), 0, 10, main = &quot;Likelihood function&quot;, xlab=TeX(&quot;$\\\\theta$&quot;), ylab=TeX(&quot;$L(y|\\\\theta)$&quot;)) text(5, 0.3, paste0( &quot;Area = &quot;, round(integrate(likelihood_function, 0, Inf)[1]$value, digits=2))) points(10, likelihood_function(10), col=&quot;red&quot;, pch=16) Notice that the area under the curve of the density function is 1 (because it is a formal probability distribution) whereas the area under the likelihood function is not 1 (the area is 10). Hence, when performing optimization using a likelihood function, one is not optimizing a probability function. Because the likelihood is not a probability, we appeal to frequentist (rather than probabilistic) interpretations when interpreting likelihood analyses. For example, the interpretation of a 95% confidence interval is “under repeated sampling from the population, 95% of the confidence intervals will contain the true value,” in comparison to the probabilistic interpretation “the probability the interval contains the true value is 95%.” The density function and the likelihood function share a common point at \\(y=2\\) and \\(\\theta=10\\), (shown in red in the figure above) given the fixed value of \\(\\alpha\\). This suggests that the likelihood and density functions are the same only when the parameter is assumed to be a fixed, known value. ## check that the density function and the likelihood function share the same point all.equal(density_function(2), likelihood_function(10)) ## [1] TRUE Philosophically, there is a subtle difference between the density function and the likelihood function that is important to understand. In the likelihood function, we allow the parameter \\(\\theta\\) to vary; however, we do not assume that \\(\\theta\\) is a random variable. To be a random variable there must be a formal probability distribution for \\(\\theta\\). We showed earlier that the likelihood function does not integrate to 1 so we don’t view \\(\\theta\\) as a random variable and the likelihood function is not a probability distribution for \\(\\theta\\). 8.3 Maximum likelihood Maximum likelihood estimation has the goal of finding the set of parameters \\(\\hat{\\boldsymbol{\\theta}}\\) that were most likely to give rise to the data. Because the likelihood does not integrate to 1, the likelihood by itself is not infomative; only comparisons among likelihoods are meaningful because the likelihood can be shifted up or down in the y-axis by an arbitraty constant \\(c\\). To cancel out the unkown constant \\(c\\), we take ratios of the likelihoods at values \\(\\theta_1\\) and \\(\\theta_2\\), giving rise to the likelihood ratio \\[\\begin{align*} \\frac{L(\\theta_1 | y)}{L(\\theta_2 | y)} &amp; = \\frac{[y|\\theta_1]}{[y|\\theta_2]}. \\end{align*}\\] The likelihood ratio expresses the strength of evidence in favor of \\(\\theta_1\\) relative to \\(\\theta_2\\). In general, we use the log likelihood ratio to express the strength of evidence where positive values of the log likelihood ratio give evidence in support of \\(\\theta_1\\) and negative values of the log likelihood ratio give evidence in support of \\(\\theta_2\\), conditional on the data. The maximum likelihood estimate is the value \\(\\hat{\\theta}\\) such that \\[\\begin{align*} \\log \\left( \\frac{L(\\hat{\\theta} | y)}{L(\\theta^\\star | y)} \\right) &amp; \\geq 0 \\end{align*}\\] for all values of \\(\\theta^\\star\\). The following figure demonstrates this idea visually. layout(matrix(1)) curve(likelihood_function(x), 0, 10, main = &quot;Likelihood function&quot;, xlab=TeX(&quot;$\\\\theta$&quot;), ylab=TeX(&quot;$L(y|\\\\theta)$&quot;), ylim=c(-0.02, 0.66)) segments(3, 0, 3, likelihood_function(3)) arrows(3, likelihood_function(3), 0, likelihood_function(3)) text(3.1, -0.02, TeX(&quot;$\\\\theta_1$ = 3&quot;)) text(1.5, likelihood_function(3)+ .02, TeX(&quot;$L(y|\\\\theta_1)$&quot;)) segments(6, 0, 6, likelihood_function(6)) arrows(6, likelihood_function(6), 0, likelihood_function(6)) text(6.1, -0.02, TeX(&quot;$\\\\theta_2$ = 6&quot;)) text(1.5, likelihood_function(6) + .02, TeX(&quot;$L(y|\\\\theta_2)$&quot;)) segments(5, 0, 5, likelihood_function(5), lty=2) text(5.3, 0.66, TeX(&quot;$\\\\hat{\\\\theta}_{MLE}$ = 5&quot;)) For example, if \\(\\theta_1 = 3\\) and \\(\\theta_2 = 6\\), the log likelihood ratio is -0.9314718 which suggests evidence is in favor of \\(\\theta_2\\) relative to \\(\\theta_1\\). In comparison, the log likelihood ratio of the MLE \\(\\hat{\\theta}_{MLE}\\) is 0.1767844 which gives evidence in favor of \\(\\hat{\\theta}_{MLE}\\). 8.3.0.1 Maximum likelihood estimation of the variogram Instead of using the variogram in (8.1), we can use maximum liklihood. More formal and principled estimation framework. Allows for parameter uncertainty quantification. Goal: estimate the parameters \\(\\boldsymbol{\\beta}\\) and \\(\\boldsymbol{\\theta} = (\\tau^2, \\sigma^2, \\nu, \\phi)&#39;\\). The log-likelihood to maximize is \\[\\begin{align*} \\log[\\boldsymbol{\\beta}, \\boldsymbol{\\theta} | \\mathbf{y}] &amp; = - \\frac{1}{2} \\log|\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})| - \\frac{1}{2} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right)&#39; \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})^{-1} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) \\end{align*}\\] This is a multivariate optimization and difficult to optimize directly. Instead, notice the estimate \\(\\hat{\\boldsymbol{\\beta}}(\\boldsymbol{\\theta}) = (\\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{X})^{-1} \\mathbf{X}&#39; \\hat{\\boldsymbol{\\Sigma}}^{-1} \\mathbf{y}\\) is a known function of \\(\\boldsymbol{\\theta}\\) so we can profile it out of the equation. Thus, we can instead optimize the profile likelihood \\[\\begin{align*} \\log[\\boldsymbol{\\theta} | \\mathbf{y}]_{prof} &amp; = - \\frac{1}{2} \\log|\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})| - \\frac{1}{2} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}(\\boldsymbol{\\theta}) \\right)&#39; \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})^{-1} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta}(\\boldsymbol{\\theta}) \\right) \\end{align*}\\] Note this equation is now just a function of \\(\\boldsymbol{\\theta}\\). Computational complexity – both the log-likelihood and the profile log-likelihood requre the determinant and inverse of the \\(n \\times n\\) covariance matrix \\(\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})\\) which require \\(O(n^3)\\) time if (!file.exists(here::here(&quot;results&quot;, &quot;matrix-inverse-timings.RData&quot;))) { n &lt;- c(10, 20, 50, 100, 200, 250, 350, 500, 600, 700, 800, 900, 1000, 1250, 1500) timings &lt;- rep(0, length(n)) for (i in 1:length(n)) { Sigma &lt;- riwish(n[i]+2, diag(n[i])) timings[i] &lt;- system.time(solve(Sigma))[3] } dat &lt;- data.frame(timings = timings, n = n) save(dat, file = here::here(&quot;results&quot;, &quot;matrix-inverse-timings.RData&quot;)) } else { load(here::here(&quot;results&quot;, &quot;matrix-inverse-timings.RData&quot;)) } ## fit the best cubic model to the data ggplot(data = dat, aes (x = n, y = timings)) + geom_point(size = 2, color = &quot;red&quot;) + stat_smooth(method = &quot;lm&quot;, formula = y ~ poly(x, 3), fullrange = TRUE) + ylab(&quot;Time to calculate inverse&quot;) + xlab(&quot;matrix size (n by n)&quot;) + ggtitle(&quot;Matrix inversion time with fitted polynomial of order 3&quot;) + xlim(c(0, 2000)) Solving the MLE for \\(\\boldsymbol{\\theta}\\) Can use standard optimization routines like optim() Can use REML to guarantee positive variance parameters “restricted maximum likelihood” Induces a bias in the estimates but guarantees realistic answers (non-negative variances) Uncertainties can be estimated using the Fisher information matrix \\[\\begin{align*} \\mathcal{I}(\\boldsymbol{\\theta})_{ij} &amp; = E \\left( \\left( \\frac{{\\partial d}}{{\\partial d} \\boldsymbol{\\theta}_i} \\log [\\boldsymbol{\\theta} | \\mathbf{y} ] \\right) \\left( \\frac{{\\partial d}}{{\\partial d} \\boldsymbol{\\theta}_j} \\log [\\boldsymbol{\\theta} | \\mathbf{y} ] \\right) \\middle| \\boldsymbol{\\theta} \\right) \\\\ &amp; = - E \\left( \\left( \\frac{{\\partial d}^2 }{{\\partial d} \\boldsymbol{\\theta}_i {\\partial d} \\boldsymbol{\\theta}_j} \\log [\\boldsymbol{\\theta} | \\mathbf{y} ]\\right) \\middle| \\boldsymbol{\\theta} \\right) \\end{align*}\\] "],
["day-9.html", "9 Day 9 9.1 Announcements 9.2 Asymptotic properties of the MLE", " 9 Day 9 9.1 Announcements 9.2 Asymptotic properties of the MLE "],
["day-10.html", "10 Day 10 10.1 Announcements 10.2 Asymptotic properties of the MLE", " 10 Day 10 10.1 Announcements 10.2 Asymptotic properties of the MLE "],
["day-11.html", "11 Day 11 11.1 Announcements 11.2 Example spatial analysis 11.3 Spatial model fitting 11.4 Kriging", " 11 Day 11 11.1 Announcements 11.2 Example spatial analysis library(tidyverse) library(fields) library(geoR) library(STRbook) library(maps) library(nlme) 11.3 Spatial model fitting We will use the NOAA_df_1990 data.frame from the STRbook package. We will data(&quot;NOAA_df_1990&quot;) ## add a factor variable for left_join NOAA_df_1990$id_factor &lt;- factor(NOAA_df_1990$id) dat &lt;- NOAA_df_1990 %&gt;% subset(year == 1990 &amp; month == 7 &amp; proc == &quot;Tmax&quot;) %&gt;% group_by(id_factor) %&gt;% summarize(mean_Tmax = mean(z)) # ## add back in the lat/lon variables dat &lt;- NOAA_df_1990 %&gt;% subset(year == 1990 &amp; month == 7 &amp; proc == &quot;Tmax&quot; &amp; day == 1) %&gt;% left_join(dat, by = &quot;id_factor&quot;) We can plot the data states &lt;- map_data(&quot;state&quot;) dat %&gt;% ggplot(aes(x = lon, y = lat, color = mean_Tmax)) + geom_point() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_color_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature in 1990&quot;) Subset to just the region of interest states &lt;- map_data(&quot;state&quot;) dat %&gt;% ggplot(aes(x = lon, y = lat, color = mean_Tmax)) + geom_point() + scale_color_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature in 1990&quot;) + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA, inherit.aes = FALSE) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) dat %&gt;% ggplot(aes(x = mean_Tmax)) + geom_histogram() + ggtitle(&quot;Average July max Temperature in 1990&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. The histogram looks non-normal. Is this a problem? No. What matters are the residuals hist(resid(lm(dat$mean_Tmax ~ dat$lat))) 11.3.1 Fit the maximum likelihood estimate using the geoR package Fit the maximum likelihood estimate using an intercept only model ) using the geoR package fit &lt;- geoR::likfit( data = dat$mean_Tmax, coords = cbind(dat$lat, dat$lon), cov.model = &quot;exponential&quot;, ini.cov.pars = c(var(dat$mean_Tmax), 1) ) ## kappa not used for the exponential correlation function ## --------------------------------------------------------------- ## likfit: likelihood maximisation using the function optim. ## likfit: Use control() to pass additional ## arguments for the maximisation function. ## For further details see documentation for optim. ## likfit: It is highly advisable to run this function several ## times with different initial values for the parameters. ## likfit: WARNING: This step can be time demanding! ## --------------------------------------------------------------- ## likfit: end of numerical maximisation. summary(fit) ## Summary of the parameter estimation ## ----------------------------------- ## Estimation method: maximum likelihood ## ## Parameters of the mean component (trend): ## beta ## 85.4817 ## ## Parameters of the spatial component: ## correlation function: exponential ## (estimated) variance parameter sigmasq (partial sill) = 62.9 ## (estimated) cor. fct. parameter phi (range parameter) = 44.6 ## anisotropy parameters: ## (fixed) anisotropy angle = 0 ( 0 degrees ) ## (fixed) anisotropy ratio = 1 ## ## Parameter of the error component: ## (estimated) nugget = 1.256 ## ## Transformation parameter: ## (fixed) Box-Cox parameter = 1 (no transformation) ## ## Practical Range with cor=0.05 for asymptotic range: 133.6098 ## ## Maximised Likelihood: ## log.L n.params AIC BIC ## &quot;-287.3&quot; &quot;4&quot; &quot;582.5&quot; &quot;594.2&quot; ## ## non spatial model: ## log.L n.params AIC BIC ## &quot;-418.5&quot; &quot;2&quot; &quot;841&quot; &quot;846.8&quot; ## ## Call: ## geoR::likfit(coords = cbind(dat$lat, dat$lon), data = dat$mean_Tmax, ## ini.cov.pars = c(var(dat$mean_Tmax), 1), cov.model = &quot;exponential&quot;) AIC(fit) ## [1] 582.5471 fit$BIC ## [1] 594.1977 Plot the estimated spatial correlation function Model: \\(\\mathbf{y} \\sim N(\\mu \\mathbf{1}, \\sigma^2 \\mathbf{I} + \\tau^2 \\mathbf{R}(\\phi))\\) sigma2 &lt;- fit$nugget tau2 &lt;- fit$sigmasq phi &lt;- fit$phi d &lt;- seq(0, 150, length.out = 1000) cov &lt;- ifelse(d == 0, sigma2, 0) + tau2 * exp( - d / phi) cor &lt;- cov / cov[1] data.frame( d = d[-1], cov = cov[-1] ) %&gt;% ggplot(aes(x = d, y = cov)) + geom_line() + geom_point(data = data.frame(d_nugget = d[1], cov_nugget = cov[1]), aes(x = d_nugget, y = cov_nugget), inherit.aes = FALSE) + ggtitle(&quot;Estimated covariance function&quot;) + ylim(c(0, max(cov))) data.frame( d = d[-1], cor = cor[-1] ) %&gt;% ggplot(aes(x = d, y = cor)) + geom_line() + geom_point(data = data.frame(d_nugget = d[1], cor_nugget = cor[1]), aes(x = d_nugget, y = cor_nugget), inherit.aes = FALSE) + ggtitle(&quot;Estimated correlation function&quot;) + ylim(c(0, max(cor))) Fit the model with coefficients fit_coef &lt;- geoR::likfit( data = dat$mean_Tmax, trend = ~ dat$lat + dat$lon, coords = cbind(dat$lat, dat$lon), cov.model = &quot;exponential&quot;, ini.cov.pars = c(var(dat$mean_Tmax), 1) ) ## kappa not used for the exponential correlation function ## --------------------------------------------------------------- ## likfit: likelihood maximisation using the function optim. ## likfit: Use control() to pass additional ## arguments for the maximisation function. ## For further details see documentation for optim. ## likfit: It is highly advisable to run this function several ## times with different initial values for the parameters. ## likfit: WARNING: This step can be time demanding! ## --------------------------------------------------------------- ## likfit: end of numerical maximisation. summary(fit_coef) ## Summary of the parameter estimation ## ----------------------------------- ## Estimation method: maximum likelihood ## ## Parameters of the mean component (trend): ## beta0 beta1 beta2 ## 112.7297 -1.0856 -0.1737 ## ## Parameters of the spatial component: ## correlation function: exponential ## (estimated) variance parameter sigmasq (partial sill) = 3.674 ## (estimated) cor. fct. parameter phi (range parameter) = 2.003 ## anisotropy parameters: ## (fixed) anisotropy angle = 0 ( 0 degrees ) ## (fixed) anisotropy ratio = 1 ## ## Parameter of the error component: ## (estimated) nugget = 1.146 ## ## Transformation parameter: ## (fixed) Box-Cox parameter = 1 (no transformation) ## ## Practical Range with cor=0.05 for asymptotic range: 6.00029 ## ## Maximised Likelihood: ## log.L n.params AIC BIC ## &quot;-278.4&quot; &quot;6&quot; &quot;568.7&quot; &quot;586.2&quot; ## ## non spatial model: ## log.L n.params AIC BIC ## &quot;-293.4&quot; &quot;4&quot; &quot;594.9&quot; &quot;606.5&quot; ## ## Call: ## geoR::likfit(coords = cbind(dat$lat, dat$lon), data = dat$mean_Tmax, ## trend = ~dat$lat + dat$lon, ini.cov.pars = c(var(dat$mean_Tmax), ## 1), cov.model = &quot;exponential&quot;) ## look at the AIC AIC(fit) ## [1] 582.5471 AIC(fit_coef) ## [1] 568.7446 ## look at the BIC fit$BIC ## [1] 594.1977 fit_coef$BIC ## [1] 586.2206 Which model is better fitting the data? Plot the estimated spatial correlation function Model: \\(\\mathbf{y} \\sim N(\\mu \\mathbf{1}, \\sigma^2 \\mathbf{I} + \\tau^2 \\mathbf{R}(\\phi))\\) sigma2 &lt;- fit_coef$nugget tau2 &lt;- fit_coef$sigmasq phi &lt;- fit_coef$phi d &lt;- seq(0, 150, length.out = 1000) cov &lt;- ifelse(d == 0, sigma2, 0) + tau2 * exp( - d / phi) cor &lt;- cov / cov[1] data.frame( d = d[-1], cov = cov[-1] ) %&gt;% ggplot(aes(x = d, y = cov)) + geom_line() + geom_point(data = data.frame(d_nugget = d[1], cov_nugget = cov[1]), aes(x = d_nugget, y = cov_nugget), inherit.aes = FALSE) + ggtitle(&quot;Estimated covariance function&quot;) + ylim(c(0, max(cov))) data.frame( d = d[-1], cor = cor[-1] ) %&gt;% ggplot(aes(x = d, y = cor)) + geom_line() + geom_point(data = data.frame(d_nugget = d[1], cor_nugget = cor[1]), aes(x = d_nugget, y = cor_nugget), inherit.aes = FALSE) + ggtitle(&quot;Estimated correlation function&quot;) + ylim(c(0, max(cor))) Other models are available – see ?cov.model for details 11.3.1.1 Spatial predictions using geoR pred_locations &lt;- expand.grid( seq(min(dat$lat), max(dat$lat), length.out = 100), seq(min(dat$lon), max(dat$lon), length.out = 100) ) names(pred_locations) &lt;- c(&quot;lat&quot;, &quot;lon&quot;) pred_locations &lt;- data.frame(pred_locations) preds &lt;- krige.conv( data = dat$mean_Tmax, coords = cbind(dat$lat, dat$lon), locations = cbind(pred_locations$lat, pred_locations$lon), krige = krige.control( cov.model = &quot;exponential&quot;, beta = fit_coef$beta, cov.pars = fit_coef$cov.pars, nugget = fit_coef$nugget ) ) ## krige.conv: model with constant mean ## krige.conv: Kriging performed using global neighbourhood dat_pred &lt;- data.frame( preds = preds$predict, var = preds$krige.var, lat = pred_locations$lat, lon = pred_locations$lon ) ggplot(dat_pred, aes(x = lon, y = lat, fill = preds)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) ggplot(dat_pred, aes(x = lon, y = lat, fill = var)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature prediction Variance in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) What explains the “patchiness” of the variance estimates? ggplot(dat_pred, aes(x = lon, y = lat, fill = var)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature prediction Variance in 1990&quot;) + geom_point(data = dat, aes(x = lon, y = lat), inherit.aes = FALSE, color = &quot;white&quot;, size = 0.6) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) 11.3.2 Fitting a spatial model using the nlme package fit &lt;- gls( mean_Tmax ~ 1, data = dat, correlation = corExp(form = ~ lat + lon, nugget = TRUE), method = &quot;ML&quot; ) summary(fit) ## Generalized least squares fit by maximum likelihood ## Model: mean_Tmax ~ 1 ## Data: dat ## AIC BIC logLik ## 582.5471 594.1977 -287.2735 ## ## Correlation Structure: Exponential spatial correlation ## Formula: ~lat + lon ## Parameter estimate(s): ## range nugget ## 44.86528018 0.01945057 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 85.4799 6.919885 12.35279 0 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -0.93914480 -0.50646918 0.06875849 0.68012613 1.31357929 ## ## Residual standard error: 8.033285 ## Degrees of freedom: 136 total; 135 residual summary(fit)$AIC ## [1] 582.5471 summary(fit)$BIC ## [1] 594.1977 Plot the estimated covariance/correaltion function Note: the gls function partitions the variance using a single overall variance \\(\\sigma^2_{gls}\\) and a proportion constant \\(\\rho \\in (0, 1)\\) so that our nugget \\(\\sigma^2 = \\rho * \\sigma^2_{gls}\\) and our partial sill \\(\\tau^2 = (1 - \\rho) * \\sigma^2_{gls}\\). ## extract the model parameters from the gls fit phi &lt;- exp(fit$model[1]$corStruct[1]) rho &lt;- 1 / (1 + exp(-fit$model[1]$corStruct[2])) sigma2 &lt;- fit$sigma^2 * (rho) tau2 &lt;- fit$sigma^2 * (1-rho) d &lt;- seq(0, 150, length.out = 1000) cov &lt;- ifelse(d == 0, sigma2, 0) + tau2 * exp( - d / phi) cor &lt;- cov / cov[1] data.frame( d = d[-1], cov = cov[-1] ) %&gt;% ggplot(aes(x = d, y = cov)) + geom_line() + geom_point(data = data.frame(d_nugget = d[1], cov_nugget = cov[1]), aes(x = d_nugget, y = cov_nugget), inherit.aes = FALSE) + ggtitle(&quot;Estimated covariance function&quot;) + ylim(c(0, max(cov))) data.frame( d = d[-1], cor = cor[-1] ) %&gt;% ggplot(aes(x = d, y = cor)) + geom_line() + geom_point(data = data.frame(d_nugget = d[1], cor_nugget = cor[1]), aes(x = d_nugget, y = cor_nugget), inherit.aes = FALSE) + ggtitle(&quot;Estimated correlation function&quot;) + ylim(c(0, max(cor))) We can also fit this model using covariates fit_coef &lt;- gls( mean_Tmax ~ lat + lon, data = dat, correlation = corExp(form = ~ lat + lon, nugget = TRUE), method = &quot;ML&quot; ) summary(fit_coef) ## Generalized least squares fit by maximum likelihood ## Model: mean_Tmax ~ lat + lon ## Data: dat ## AIC BIC logLik ## 568.7446 586.2206 -278.3723 ## ## Correlation Structure: Exponential spatial correlation ## Formula: ~lat + lon ## Parameter estimate(s): ## range nugget ## 2.0029357 0.2377618 ## ## Coefficients: ## Value Std.Error t-value p-value ## (Intercept) 112.72965 7.533049 14.964679 0.0000 ## lat -1.08557 0.097936 -11.084439 0.0000 ## lon -0.17370 0.073610 -2.359685 0.0197 ## ## Correlation: ## (Intr) lat ## lat -0.472 ## lon 0.863 0.033 ## ## Standardized residuals: ## Min Q1 Med Q3 Max ## -2.6610668315 -0.5356554805 0.0003439733 0.5456241209 2.4992950620 ## ## Residual standard error: 2.195568 ## Degrees of freedom: 136 total; 133 residual summary(fit)$AIC ## [1] 582.5471 summary(fit_coef)$AIC ## [1] 568.7446 summary(fit)$BIC ## [1] 594.1977 summary(fit_coef)$BIC ## [1] 586.2206 Plot the estimated covariance/correaltion function Note: the gls function partitions the variance using a single overall variance \\(\\sigma^2_{gls}\\) and a proportion constant \\(\\rho \\in (0, 1)\\) so that our nugget \\(sigma^2 = \\rho * \\sigma^2_{gls}\\) and our partial sill \\(\\tau^2 = (1 - \\rho) * \\sigma^2_{gls}\\). ## extract the model parameters from the gls fit phi &lt;- exp(fit_coef$model[1]$corStruct[1]) rho &lt;- 1 / (1 + exp(-fit_coef$model[1]$corStruct[2])) sigma2 &lt;- fit_coef$sigma^2 * (rho) tau2 &lt;- fit_coef$sigma^2 * (1-rho) d &lt;- seq(0, 150, length.out = 1000) cov &lt;- ifelse(d == 0, sigma2, 0) + tau2 * exp( - d / phi) cor &lt;- cov / cov[1] data.frame( d = d[-1], cov = cov[-1] ) %&gt;% ggplot(aes(x = d, y = cov)) + geom_line() + geom_point(data = data.frame(d_nugget = d[1], cov_nugget = cov[1]), aes(x = d_nugget, y = cov_nugget), inherit.aes = FALSE) + ggtitle(&quot;Estimated covariance function&quot;) + ylim(c(0, max(cov))) data.frame( d = d[-1], cor = cor[-1] ) %&gt;% ggplot(aes(x = d, y = cor)) + geom_line() + geom_point(data = data.frame(d_nugget = d[1], cor_nugget = cor[1]), aes(x = d_nugget, y = cor_nugget), inherit.aes = FALSE) + ggtitle(&quot;Estimated correlation function&quot;) + ylim(c(0, max(cor))) Other correlation structures avaiable with ?corClasses 11.3.2.1 Spatial predictions using nlme pred_locations &lt;- expand.grid( seq(min(dat$lat), max(dat$lat), length.out = 100), seq(min(dat$lon), max(dat$lon), length.out = 100) ) names(pred_locations) &lt;- c(&quot;lat&quot;, &quot;lon&quot;) pred_locations &lt;- data.frame(pred_locations) preds &lt;- predict( fit_coef, newdata = pred_locations ) dat_pred &lt;- data.frame( preds = preds, lat = pred_locations$lat, lon = pred_locations$lon ) ggplot(dat_pred, aes(x = lon, y = lat, fill = preds)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) 11.3.3 model fitting with autokrige function library(automap) dat_points &lt;- SpatialPointsDataFrame( coords = cbind(dat$lon, dat$lat), data = data.frame( mean_Tmax = dat$mean_Tmax, lat = dat$lat, lon = dat$lon ) ) pred_points &lt;- SpatialPointsDataFrame( coords = cbind(dat_pred$lon, dat_pred$lat), data = data.frame( lat = dat_pred$lat, lon = dat_pred$lon ) ) fit &lt;- autoKrige( mean_Tmax ~ 1, input_data = dat_points, new_data = pred_points ) ## [using ordinary kriging] summary(fit) ## krige_output: ## Object of class SpatialPointsDataFrame ## Coordinates: ## min max ## coords.x1 -99.96667 -80.03333 ## coords.x2 32.13334 45.86666 ## Is projected: NA ## proj4string : [NA] ## Number of points: 10000 ## Data attributes: ## var1.pred var1.var var1.stdev ## Min. :77.04 Min. :2.232 Min. :1.494 ## 1st Qu.:81.52 1st Qu.:2.377 1st Qu.:1.542 ## Median :86.38 Median :2.457 Median :1.567 ## Mean :86.25 Mean :2.563 Mean :1.597 ## 3rd Qu.:91.15 3rd Qu.:2.576 3rd Qu.:1.605 ## Max. :94.87 Max. :7.222 Max. :2.687 ## ## exp_var: ## np dist gamma dir.hor dir.ver id ## 1 14 0.2688453 1.572135 0 0 var1 ## 2 9 0.6657668 4.651058 0 0 var1 ## 3 39 0.9188058 2.466461 0 0 var1 ## 4 56 1.1530422 2.600844 0 0 var1 ## 5 295 1.6981353 4.445251 0 0 var1 ## 6 382 2.5763905 5.503639 0 0 var1 ## 7 706 3.6004969 8.450781 0 0 var1 ## 8 852 4.8837648 13.155866 0 0 var1 ## 9 900 6.1662408 18.812370 0 0 var1 ## 10 1255 7.6319285 27.499139 0 0 var1 ## ## var_model: ## model psill range kappa ## 1 Nug 1.920227e+00 0.00 0.0 ## 2 Ste 3.897518e+06 17324.57 0.9 ## Sums of squares betw. var. model and sample var.[1] 221.0082 plot(fit) fit_coef &lt;- autoKrige( mean_Tmax ~ lat + lon, input_data = dat_points, new_data = pred_points ) ## [using universal kriging] summary(fit_coef) ## krige_output: ## Object of class SpatialPointsDataFrame ## Coordinates: ## min max ## coords.x1 -99.96667 -80.03333 ## coords.x2 32.13334 45.86666 ## Is projected: NA ## proj4string : [NA] ## Number of points: 10000 ## Data attributes: ## var1.pred var1.var var1.stdev ## Min. :76.63 Min. :1.481 Min. :1.217 ## 1st Qu.:81.51 1st Qu.:2.654 1st Qu.:1.629 ## Median :86.23 Median :2.891 Median :1.700 ## Mean :86.20 Mean :2.944 Mean :1.710 ## 3rd Qu.:91.02 3rd Qu.:3.190 3rd Qu.:1.786 ## Max. :94.78 Max. :5.210 Max. :2.283 ## ## exp_var: ## np dist gamma dir.hor dir.ver id ## 1 14 0.2688453 1.644463 0 0 var1 ## 2 9 0.6657668 4.865208 0 0 var1 ## 3 39 0.9188058 1.703131 0 0 var1 ## 4 56 1.1530422 2.407162 0 0 var1 ## 5 295 1.6981353 3.509443 0 0 var1 ## 6 382 2.5763905 3.328727 0 0 var1 ## 7 706 3.6004969 4.193811 0 0 var1 ## 8 852 4.8837648 4.488593 0 0 var1 ## 9 900 6.1662408 4.472937 0 0 var1 ## 10 1255 7.6319285 4.216233 0 0 var1 ## ## var_model: ## model psill range kappa ## 1 Nug 0.7733065 0.000000 0.0 ## 2 Ste 3.8560971 2.887739 0.3 ## Sums of squares betw. var. model and sample var.[1] 197.6886 plot(fit_coef) dat_pred &lt;- data.frame( preds = fit_coef$krige_output@data$var1.pred, var = fit_coef$krige_output@data$var1.var, lat = pred_locations$lat, lon = pred_locations$lon ) ggplot(dat_pred, aes(x = lon, y = lat, fill = preds)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) ggplot(dat_pred, aes(x = lon, y = lat, fill = var)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature prediction Variance in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) 11.4 Kriging Given a set of \\(n\\) locations \\(\\mathbf{s}_{obs} = \\{ \\mathbf{s}_1, \\ldots, \\mathbf{s}_n \\}\\), the goal is often to make a prediction at a set of \\(n_0\\) new locations \\(\\mathbf{s}_{unobs} = \\{\\mathbf{s}_{unobs_1}, \\ldots, \\mathbf{s}_{unobs_{n_0}} \\}\\). Out of date terms (but still commonly used) Ordinary Kriging – Kriging using a constant mean function Universal Kriging – Kriging with an estimated polynomial mean function Not really a useful distinction – now, all Kriging is universal Kriging Kriging is named after Danie G. Krige who did early work on spatial interpolation. For now, assume \\(E(y(\\mathbf{s})) = 0 \\hspace{1em} \\forall \\mathbf{s}\\) Because our model is a Gaussian process, the joint distribution of the process \\(\\mathbf{y}_{unobs} = (y(\\mathbf{s}_{unobs_1}), \\ldots, y(\\mathbf{s}_{{unobs_{n_0}}}))&#39;\\) at the unobserved locations and the process \\(\\mathbf{y}_{obs} = (y(\\mathbf{s}_1), \\ldots, y(\\mathbf{s}_n))&#39;\\) at the observed locations is \\[\\begin{align*} \\begin{pmatrix} \\mathbf{y}_{unobs} \\\\ \\dots \\\\ \\mathbf{y}_{obs} \\end{pmatrix} &amp; \\sim \\operatorname{N} \\left( \\begin{pmatrix} \\mathbf{0}_{unobs} \\\\ \\dots \\\\ \\mathbf{0}_{obs} \\end{pmatrix} , \\begin{pmatrix} \\Sigma_{unobs} &amp; \\vdots &amp; \\boldsymbol{\\Sigma}_{unobs,obs} \\\\ \\dots \\\\ \\boldsymbol{\\Sigma}_{obs, unobs} &amp; \\vdots &amp; \\boldsymbol{\\Sigma}_{obs} \\end{pmatrix} \\right), \\end{align*}\\] where \\(\\boldsymbol{\\Sigma}_{unobs} = \\{ Cov \\left( y(\\mathbf{s}_{unobs_i}), y(\\mathbf{s}_{unobs_j}) \\right) \\}_{i,j=1}^{n_0}\\) is the \\(n_{unobs} \\times n_{unobs}\\) covariance matrix that represents the covariance function evaluated at the unobserved locations, \\(\\boldsymbol{\\Sigma}_{unobs,obs} = \\boldsymbol{\\Sigma}_{obs,unobs}&#39; = \\{ Cov \\left( y(\\mathbf{s}_{unobs_i}), y(\\mathbf{s}_{j}) \\right) \\}_{i=1,\\ldots, n_0; j=1, \\ldots, n}\\) is the \\(n_{unobs} \\times n\\) cross-covariance matrix that represents the covariance function evaluated between the unobserved and observed locations, and \\(\\boldsymbol{\\Sigma}_{obs} = \\{ Cov \\left( y(\\mathbf{s}_{i}), y(\\mathbf{s}_{j}) \\right) \\}_{i,j=1}^{n}\\) is the \\(n \\times n\\) covariance matrix that represents the covariance function evaluated at the observed locations. Using the properties of conditional multivariate normal distributions, we have the conditional distribution \\[ \\begin{align} \\mathbf{y}_{unobs} | \\mathbf{y}_{obs} &amp; \\sim \\operatorname{N} \\left( \\boldsymbol{\\Sigma}_{unobs,obs} \\boldsymbol{\\Sigma}_{obs}^{-1} \\mathbf{y}_{obs}, \\boldsymbol{\\Sigma}_{unobs,unobs} - \\boldsymbol{\\Sigma}_{unobs,obs} \\boldsymbol{\\Sigma}_{obs}^{-1} \\boldsymbol{\\Sigma}_{obs,unobs} \\right) \\end{align} \\] Can plug in the MLE estimate \\(\\boldsymbol{\\Sigma}(\\hat{\\boldsymbol{\\theta}})\\) to get the estimated distribution (not accounting for parameter uncertainty). The Kriging approach gets the same solution by not assuming normality and solving for the best linear unbiased predictor (BLUP) We generate the predictive maps from the example above using this formula fit &lt;- geoR::likfit( data = dat$mean_Tmax, coords = cbind(dat$lat, dat$lon), cov.model = &quot;exponential&quot;, ini.cov.pars = c(var(dat$mean_Tmax), 1) ) ## kappa not used for the exponential correlation function ## --------------------------------------------------------------- ## likfit: likelihood maximisation using the function optim. ## likfit: Use control() to pass additional ## arguments for the maximisation function. ## For further details see documentation for optim. ## likfit: It is highly advisable to run this function several ## times with different initial values for the parameters. ## likfit: WARNING: This step can be time demanding! ## --------------------------------------------------------------- ## likfit: end of numerical maximisation. sigma2 &lt;- fit$nugget tau2 &lt;- fit$sigmasq phi &lt;- fit$phi ## prediction locations pred_locations &lt;- expand.grid( seq(min(dat$lat), max(dat$lat), length.out = 100), seq(min(dat$lon), max(dat$lon), length.out = 100) ) names(pred_locations) &lt;- c(&quot;lat&quot;, &quot;lon&quot;) ## calculate pairwise distance matrices -- be careful with large data locs &lt;- cbind(dat$lat, dat$lon) D &lt;- rdist(locs) D_unobs &lt;- rdist(pred_locations) D_unobs_obs &lt;- rdist(pred_locations, locs) Cov &lt;- diag(nrow(dat)) * sigma2 + tau2 * exp( - D / phi) Cov_unobs &lt;- diag(nrow(pred_locations)) * sigma2 + tau2 * exp( - D_unobs / phi) Cov_unobs_obs &lt;- tau2 * exp( - D_unobs_obs / phi) Cov_inv &lt;- solve(Cov) Cov_inv &lt;- chol2inv(chol(Cov)) ## Kriging mean -- note we subtract the mean to get mean 0 then add it back in pred_mean &lt;- Cov_unobs_obs %*% Cov_inv %*% (dat$mean_Tmax - mean(dat$mean_Tmax)) + mean(dat$mean_Tmax) pred_mean_fast &lt;- Cov_unobs_obs %*% (Cov_inv %*% (dat$mean_Tmax - mean(dat$mean_Tmax))) + mean(dat$mean_Tmax) ## Kriging variance pred_var &lt;- diag(Cov_unobs - Cov_unobs_obs %*% Cov_inv %*% t(Cov_unobs_obs)) pred_var_fast &lt;- diag(Cov_unobs) - rowSums((Cov_unobs_obs %*% Cov_inv) * Cov_unobs_obs) system.time(diag(Cov_unobs_obs %*% Cov_inv %*% t(Cov_unobs_obs))) ## user system elapsed ## 9.478 0.227 9.979 system.time(rowSums((Cov_unobs_obs %*% Cov_inv) * Cov_unobs_obs)) ## user system elapsed ## 0.124 0.001 0.124 all.equal(pred_var, pred_var_fast) ## [1] TRUE dat_pred &lt;- data.frame( preds = pred_mean, var = pred_var, lat = pred_locations$lat, lon = pred_locations$lon ) ggplot(dat_pred, aes(x = lon, y = lat, fill = preds)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) ggplot(dat_pred, aes(x = lon, y = lat, fill = var)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature prediction variance in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) 11.4.1 Kriging Equations with mean \\(\\mu = 0\\) Assumptions – BLUP We will assume we are wanting predictions at a single site \\(\\mathbf{s}_0\\) Without loss of generality, assume that \\(E(y(\\mathbf{s})) = 0\\) – can always subtract the mean and add it back in (see code above) \\(Cov(\\mathbf{y}) = \\boldsymbol{\\Sigma}\\) is the covariance of the data \\(Cov(y(\\mathbf{s}_0), y(\\mathbf{s}_i)) = \\boldsymbol{\\Sigma}_{0i}\\) The optimal prediction at unobserved location \\(\\mathbf{s}_0\\) is a linear combination of the observed sites \\(\\Rightarrow\\) \\(\\hat{y}(\\mathbf{s}_0) = \\sum_{i=1}^n w_i y(\\mathbf{s}_i)\\) for a set of unknown weights \\(w_i\\) The optimal predictor is unbiased \\(\\Rightarrow\\) \\(E(\\hat{y}(\\mathbf{s}_0)) = 0\\) Therefore, unbiasedness implies that \\(E(\\hat{y}(\\mathbf{s}_0)) = \\sum_{i=1}^n w_i E(y(\\mathbf{s}_i)) = \\sum_{i=1}^n w_i 0 = 0\\). Because \\(\\mu = 0\\), this is trivially easy and we only need to minimize the variance to have the BLUP. The MSE is \\[ \\begin{align*} MSE &amp; = E((\\hat{y}(\\mathbf{s}_0) - y(\\mathbf{s}_0))^2) \\\\ &amp; = E(\\hat{y}(\\mathbf{s}_0)^2 - 2 \\hat{y}(\\mathbf{s}_0) y(\\mathbf{s}_0) + y(\\mathbf{s}_0)^2) \\\\ &amp; = E(\\hat{y}(\\mathbf{s}_0)^2) - 2 E(\\hat{y}(\\mathbf{s}_0) y(\\mathbf{s}_0)) + E(y(\\mathbf{s}_0)^2) \\end{align*} \\] Because \\(\\mu = 0\\), we have \\(E(\\hat{y}(\\mathbf{s}_0)^2) = Var(\\hat{y}(\\mathbf{s}_0))\\), \\(E(y(\\mathbf{s}_0)^2) = Var(y(\\mathbf{s}_0))\\), and \\(E(\\hat{y}(\\mathbf{s}_0)y(\\mathbf{s}_0)) = Cov(\\hat{y}(\\mathbf{s}_0), y(\\mathbf{s}_0))\\). Thus, the MSE is \\[\\begin{align*} MSE &amp; = E(\\hat{y}(\\mathbf{s}_0)^2) - 2 E(\\hat{y}(\\mathbf{s}_0) y(\\mathbf{s}_0)) + E(y(\\mathbf{s}_0)^2) \\\\ &amp; = Var(\\hat{y}(\\mathbf{s}_0)) - 2 Cov(\\hat{y}(\\mathbf{s}_0), y(\\mathbf{s}_0)) + Var(y(\\mathbf{s}_0)) \\\\ &amp; Var \\left( \\sum_{i=1}^n w_i y(\\mathbf{s}_i) \\right) - 2 Cov( \\sum_{i=1}^n w_i y(\\mathbf{s}_i), y(\\mathbf{s}_0)) + \\Sigma_0 \\\\ &amp; = \\sum_{i=1}^n \\sum_{j=1}^n w_i w_j \\Sigma_{ij} - 2 \\sum_{i=1}^n w_i \\Sigma_{0i} + \\Sigma_{00} \\\\ &amp; = \\mathbf{w}&#39; \\boldsymbol{\\Sigma} \\mathbf{w} - 2 \\mathbf{w}&#39; \\boldsymbol{\\Sigma}_{0} + \\Sigma_{00} \\end{align*}\\] To minimize the MSE, we take a derivative with respect to the Kriging weights \\(\\mathbf{w}\\) and solve the equation when set equal to 0 \\[\\begin{align*} \\frac{\\partial}{\\partial \\mathbf{w}} MSE &amp; = \\frac{\\partial}{\\partial \\mathbf{w}} \\mathbf{w}&#39; \\boldsymbol{\\Sigma} \\mathbf{w} - 2 \\mathbf{w}&#39; \\boldsymbol{\\Sigma}_{0} + \\Sigma_{00} \\\\ &amp; = 2 \\boldsymbol{\\Sigma} \\mathbf{w} - 2 \\boldsymbol{\\Sigma}_{0} \\end{align*}\\] which has solution \\(\\mathbf{w} = \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\Sigma}_0\\). 11.4.2 Kriging Equations with unknown mean First, we assume the mean as the form \\(\\mu(\\mathbf{s}) = \\mathbf{X}(\\mathbf{s}) \\boldsymbol{\\beta}\\) and can the regression coefficents can be estimated from the MLE (take derivative of the log-likelhood and set equal to 0 – generalized least squares) as \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}&#39; \\boldsymbol{\\Sigma}^{-1} \\mathbf{X})^{-1} \\mathbf{X}&#39; \\boldsymbol{\\Sigma}^{-1} \\mathbf{y}\\). We also assume that the covariate value \\(\\mathbf{x}(\\mathbf{s}_0)\\) at the prediction location is known The Kriging estimate is \\[ \\begin{align*} \\hat{y}(\\mathbf{s}_0) = \\hat{\\mu}(\\mathbf{s}_0) + \\boldsymbol{\\Sigma}_0 \\boldsymbol{\\Sigma}^{-1} (y(\\mathbf{s}) - \\hat{\\mu}(\\mathbf{s})). \\end{align*} \\] which has variance \\[ \\begin{align*} Var(\\hat{y}(\\mathbf{s}_0)) = \\boldsymbol{\\Sigma}_{00} - \\boldsymbol{\\Sigma}_0 \\boldsymbol{\\Sigma}^{-1} \\boldsymbol{\\Sigma}_0, \\end{align*} \\] Where \\(\\boldsymbol{\\Sigma}_{00}\\) is the unconditional variance of the process at location \\(\\mathbf{s}_0\\). From this equation, we see that the Kriging variance is smaller than the unconditional variance. We generate the predictive maps from the example above using the Universal Kriging formula fit_coef &lt;- geoR::likfit( data = dat$mean_Tmax, trend = ~ dat$lat + dat$lon, coords = cbind(dat$lat, dat$lon), cov.model = &quot;exponential&quot;, ini.cov.pars = c(var(dat$mean_Tmax), 1) ) ## kappa not used for the exponential correlation function ## --------------------------------------------------------------- ## likfit: likelihood maximisation using the function optim. ## likfit: Use control() to pass additional ## arguments for the maximisation function. ## For further details see documentation for optim. ## likfit: It is highly advisable to run this function several ## times with different initial values for the parameters. ## likfit: WARNING: This step can be time demanding! ## --------------------------------------------------------------- ## likfit: end of numerical maximisation. sigma2 &lt;- fit_coef$nugget tau2 &lt;- fit_coef$sigmasq phi &lt;- fit_coef$phi beta &lt;- fit_coef$beta ## prediction locations pred_locations &lt;- expand.grid( seq(min(dat$lat), max(dat$lat), length.out = 100), seq(min(dat$lon), max(dat$lon), length.out = 100) ) names(pred_locations) &lt;- c(&quot;lat&quot;, &quot;lon&quot;) ## calculate pairwise distance matrices -- be careful with large data locs &lt;- cbind(dat$lat, dat$lon) D &lt;- rdist(locs) D_unobs &lt;- rdist(pred_locations) D_unobs_obs &lt;- rdist(pred_locations, locs) Cov &lt;- diag(nrow(dat)) * sigma2 + tau2 * exp( - D / phi) Cov_unobs &lt;- diag(nrow(pred_locations)) * sigma2 + tau2 * exp( - D_unobs / phi) Cov_unobs_obs &lt;- tau2 * exp( - D_unobs_obs / phi) Cov_inv &lt;- solve(Cov) ## Kriging mean -- note we subtract the mean to get mean 0 then add it back in pred_mean &lt;- Cov_unobs_obs %*% Cov_inv %*% (dat$mean_Tmax - cbind(1, locs) %*% beta) + as.matrix(cbind(1, pred_locations)) %*% beta ## Kriging variance -- fast form pred_var &lt;- diag(Cov_unobs) - rowSums((Cov_unobs_obs %*% Cov_inv) * Cov_unobs_obs) dat_pred &lt;- data.frame( preds = pred_mean, var = pred_var, lat = pred_locations$lat, lon = pred_locations$lon ) ggplot(dat_pred, aes(x = lon, y = lat, fill = preds)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) ggplot(dat_pred, aes(x = lon, y = lat, fill = var)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature prediction variance in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) "],
["day-12.html", "12 Day 12 12.1 Announcements 12.2 Introduction to MCMC 12.3 Example: Simple linear regression 12.4 The posterior distribution 12.5 Fitting the model", " 12 Day 12 12.1 Announcements 12.2 Introduction to MCMC To estimate models within the Bayesian framework, the most commonly used method is Markov Chain Monte Carlo. Recall that the main difference between a Bayesian posterior distribution and a likelihood function is that the Bayesian posterior distribution integrates to 1 whereas the likelihood function does not. The posterior distribution of \\(\\boldsymbol{\\theta}\\) given \\(\\mathbf{y}\\) is \\[\\begin{align*} [\\boldsymbol{\\theta} | \\mathbf{y} ] &amp; = \\frac{[\\mathbf{y} | \\boldsymbol{\\theta}] [\\boldsymbol{\\theta} ]}{[\\mathbf{y}]} \\\\ \\tag{12.1} &amp; = \\frac{[\\mathbf{y} | \\boldsymbol{\\theta}] [\\boldsymbol{\\theta}]}{ \\int_{\\boldsymbol{\\theta}} [\\mathbf{y}|\\boldsymbol{\\theta}] [\\boldsymbol{\\theta}] d\\boldsymbol{\\theta}}, \\end{align*}\\] where the integral in (12.1) guarantees that the posterior distribution integrates to 1. For some models, the integral in (12.1) is analytically tractable (e.g. Normal likelihood and Normal prior), but for most interesting models the integral is not available in a closed form solution. Instead, we can approximate the integral numerically using a technique called Markov Chain Monte Carlo (MCMC) (See the seminal paper by Gelfand and Smith 1990 for more details). Instead of evaluating the probability distribution and calculating the normalizing constant, we can generate samples from the marginal posterior distribution of each parameter in such a way that the joint distribution of the samples from the marginal distributions is equivalent to samples from the posterior, up to Monte Carlo error. 12.3 Example: Simple linear regression Consider the simple linear regression model for \\(i=1, \\ldots, N\\) observations \\[\\begin{align} \\tag{12.2} y_i &amp; = \\mu + x_i \\beta + \\varepsilon_i \\end{align}\\] where \\(\\varepsilon_i \\sim N(0, \\sigma^2)\\) and \\(x_i\\) are known univariate covariates. If we assume the prior distributions \\(\\mu \\sim N(\\mu_\\mu, \\sigma^2_\\mu)\\) with \\(\\mu_\\mu\\) and \\(\\sigma^2_\\mu\\) fixed and known, \\(\\beta \\sim N(\\mu_\\beta, \\sigma^2_\\beta)\\) with \\(\\mu_\\beta\\) and \\(\\sigma^2_\\beta\\) fixed and known, and \\(\\sigma^2 \\sim \\operatorname{inverse-gamma}(\\alpha_0, \\beta_0)\\) with \\(\\alpha_0\\) and \\(\\beta_0\\) fixed and known. where the \\(\\operatorname{inverse-gamma}(\\alpha_0, \\beta_0)\\) distribution is \\[\\begin{align} [\\sigma^2 | \\alpha_0, \\beta_0] &amp; = \\frac{\\beta_0^{\\alpha_0}} {\\Gamma(\\alpha_0)} (\\sigma^2)^{-\\alpha_0 - 1} \\exp\\left\\{ - \\frac{\\beta_0}{\\sigma^2} \\right\\}. \\end{align}\\] 12.4 The posterior distribution Given the model statement, we can write out the posterior distribution on which we want inference. The posterior distribution is \\[\\begin{align} [\\mu, \\beta, \\sigma^2 | \\mathbf{y}] &amp; = \\frac{\\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\mu] [\\beta] [\\sigma^2]}{\\int_\\mu \\int_\\beta \\int_{\\sigma^2} \\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\mu] [\\beta] [\\sigma^2] \\,d\\sigma^2 \\,d\\beta \\,d\\mu}. \\end{align}\\] Notice that we don’t include \\(x_i\\) in the above statement because it is assumed fixed and known. For the model we defined in (12.2), the posterior distribution is available in closed form because we can evaluate the integrals in the denominator analytically. However, the integrals are challenging and for most interesting models, are not available in closed form. Given the posterior distribution, the next step is to find the marginal posterior distributions of \\(\\mu\\), \\(\\beta\\) and \\(\\sigma^2\\). 12.4.1 Full conditional distribution of \\(\\mu\\) The full conditional distribution (marginal posterior) for \\(\\mu\\) given all other parameters in the model is \\[\\begin{align} [\\mu | \\cdot] &amp; \\propto \\prod_{i=1^N} [y_i | \\mu, \\beta, \\sigma^2 ] [\\mu] [\\beta] [\\sigma^2] \\\\ &amp; \\propto \\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\mu] \\\\ &amp; \\propto \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(y_i - \\left(\\mu + x_i \\beta\\right)\\right)^2 \\right\\} \\frac{1}{\\sqrt{2 \\pi \\sigma^2_\\mu}} \\exp \\left\\{ -\\frac{1}{2 \\sigma_\\mu^2} \\left(\\mu - \\mu_\\mu\\right)^2 \\right\\} \\\\ &amp; \\propto \\prod_{i=1}^N \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(\\mu^2 - 2 \\mu \\left(y_i - x_i\\beta\\right)\\right) \\right\\} \\exp \\left\\{ -\\frac{1}{2 \\sigma_\\mu^2} \\left(\\mu^2 - 2 \\mu \\mu_\\mu\\right) \\right\\} \\\\ &amp; \\propto \\exp \\left\\{ -\\frac{1}{2} \\left(\\mu^2 \\left(\\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_\\mu}\\right) - 2 \\mu \\left(\\frac{ \\sum_{i=1}^N y_i - x_i\\beta }{\\sigma^2} + \\frac{\\mu_\\mu}{\\sigma^2_\\mu}\\right)\\right) \\right\\} \\end{align}\\] which is a normal distribution with mean \\(a_\\mu^{-1}b_\\mu\\) and variance \\(a_\\mu^{-1}\\) where \\[\\begin{align} a_\\mu &amp; = \\frac{N}{\\sigma^2} + \\frac{1}{\\sigma^2_\\mu} \\\\ b_\\mu &amp; = \\frac{ \\sum_{i=1}^N y_i - x_i\\beta }{\\sigma^2} + \\frac{\\mu_\\mu}{\\sigma^2_\\mu} \\end{align}\\] 12.4.2 Full conditional distribution for \\(\\beta\\) The marginal posterior for \\(\\beta\\) given all other parameters in the model is \\[\\begin{align} [\\beta | \\cdot] &amp; \\propto \\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\beta] \\\\ &amp; \\propto \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(y_i - \\left(\\mu + x_i \\beta\\right)\\right)^2 \\right\\} \\frac{1}{\\sqrt{2 \\pi \\sigma^2_\\beta}} \\exp \\left\\{ -\\frac{1}{2 \\sigma_\\beta^2} \\left(\\beta - \\mu_\\beta \\right)^2 \\right\\} \\\\ &amp; \\propto \\prod_{i=1}^N \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(\\beta^2 x_i^2 - 2 \\beta \\left(x_i \\left( y_i - \\mu \\right) \\right) \\right) \\right\\} \\exp \\left\\{ -\\frac{1}{2 \\sigma_\\beta^2} \\left(\\beta^2 - 2 \\beta \\mu_\\beta \\right) \\right\\} \\\\ &amp; \\propto \\exp \\left\\{ -\\frac{1}{2} \\left(\\beta^2 \\left(\\frac{\\sum_{i=1}^N x_i^2}{\\sigma^2} + \\frac{1}{\\sigma^2_\\beta}\\right) - 2 \\beta \\left(\\frac{ \\sum_{i=1}^N x_i \\left( y_i - \\mu \\right) }{\\sigma^2} + \\frac{\\mu_\\beta}{\\sigma^2_\\beta}\\right)\\right) \\right\\} \\end{align}\\] which is a normal distribution with mean \\(a_\\beta^{-1}b_\\beta\\) and variance \\(a_\\beta^{-1}\\) where \\[\\begin{align} a_\\beta &amp; = \\frac{\\sum_{i=1}^N x_i^2}{\\sigma^2} + \\frac{1}{\\sigma^2_\\beta} \\\\ b_\\beta &amp; = \\frac{ \\sum_{i=1}^N x_i \\left( y_i - \\mu \\right) }{\\sigma^2} + \\frac{\\mu_\\beta}{\\sigma^2_\\beta} \\end{align}\\] 12.4.3 Full conditional distribution for \\(\\sigma^2\\) The marginal posterior for \\(\\sigma^2\\) given all other parameters in the model is \\[\\begin{align} [\\sigma^2 | \\cdot] &amp; \\propto \\prod_{i=1}^N [y_i | \\mu, \\beta, \\sigma^2 ] [\\sigma^2] \\\\ &amp; \\propto \\prod_{i=1}^N \\frac{1}{\\sqrt{2 \\pi \\sigma^2}} \\exp \\left\\{ -\\frac{1}{2 \\sigma^2} \\left(y_i - \\left(\\mu + x_i \\beta\\right)\\right)^2 \\right\\} \\frac{\\beta_0^{\\alpha_0}} {\\Gamma(\\alpha_0)} (\\sigma^2)^{-\\alpha_0 - 1} \\exp\\left\\{ - \\frac{\\beta_0}{\\sigma^2} \\right\\} \\\\ &amp; \\propto \\prod_{i=1}^N \\left( \\sigma^2 \\right)^{-\\frac{1}{2}} \\exp \\left\\{ -\\frac{1}{\\sigma^2} \\frac{\\left(y_i - \\left(\\mu + x_i \\beta\\right)\\right)^2}{2} \\right\\} (\\sigma^2)^{-\\alpha_0 - 1} \\exp\\left\\{ - \\frac{\\beta_0}{\\sigma^2} \\right\\} \\\\ &amp; \\propto \\left( \\sigma^2 \\right)^{-\\frac{N}{2} - \\alpha_0 - 1} \\exp \\left\\{ -\\frac{1}{\\sigma^2} \\left( \\sum_{i=1}^N \\frac{1}{2} \\left( y_i - \\left( \\mu + x_i \\beta \\right) \\right)^2 + \\beta_0 \\right) \\right\\} \\end{align}\\] which is distributed as \\(\\operatorname{inverse-gamma} \\left( \\alpha_0 + \\frac{N}{2} , \\beta_0 + \\frac{1}{2} \\sum_{i=1}^N \\left( y_i - \\left(\\mu + x_i \\beta \\right) \\right)^2 \\right)\\). 12.5 Fitting the model To estimate parameters from the regression model, we will consider the iris dataset and let \\(y\\) be the Petal.Width variable and \\(x\\) be the Petal.Length variable in R. The data with the fitted regression line using the lm function is shown below. data(iris) library(ggplot2) ggplot(iris, aes(x=Petal.Length, y=Petal.Width)) + stat_smooth(method=&quot;lm&quot;) + geom_point() ## `geom_smooth()` using formula &#39;y ~ x&#39; To start, we setup the variables and define default values for the prior constants. y &lt;- iris$Petal.Width x &lt;- iris$Petal.Length ## these are the prior choices. ## We will talk later about how to choose prior distributions and values later mu_mu &lt;- 0 sigma2_mu &lt;- 10000 mu_beta &lt;- 0 sigma2_beta &lt;- 10000 alpha_0 &lt;- 0.01 beta_0 &lt;- 0.01 Next, we define empty vectors to store the MCMC output. We will run the MCMC sampler for \\(K=5000\\) iterations. K &lt;- 5000 mu &lt;- rep(0, K) beta &lt;- rep(0, K) sigma2 &lt;- rep(0, K) The MCMC algorithm works by cycling through the marginal posterior distributions. The algorithm is called a Gibbs Sampler and we sample from \\[\\begin{align} \\mu | \\cdot &amp; \\sim N(a_\\mu^{-1}b_\\mu, a_\\mu^{-1}) \\\\ \\beta | \\cdot &amp; \\sim N(a_\\beta^{-1}b_\\beta, a_\\beta^{-1}) \\\\ \\sigma^2 | \\cdot &amp; \\sim \\operatorname{inverse-gamma}\\left( \\alpha_0 + \\frac{N}{2} , \\beta_0 + \\frac{1}{2} \\sum_{i=1}^N \\left( y_i - \\left(\\mu + x_i \\beta \\right) \\right)^2 \\right) \\end{align}\\] in sequential order. In R, the Gibbs sampler is set.seed(101) ## initialize mu[1] &lt;- rnorm(1, mu_mu, sqrt(sigma2_mu)) beta[1] &lt;- rnorm(1, mu_beta, sqrt(sigma2_beta)) ## note that sampling from an inverse gamma is the same as ## the inverse of a random gamma variable sigma2[1] &lt;- 1 / rgamma(1, alpha_0, beta_0) ## calculate the sample size N &lt;- length(y) for (k in 2:K) { ## sample mu a_mu &lt;- N / sigma2[k-1] + 1 / sigma2_mu b_mu &lt;- sum(y - x * beta[k-1]) / sigma2[k-1] + mu_mu / sigma2_mu mu[k] &lt;- rnorm(1, b_mu / a_mu, sqrt(1 / a_mu)) ## sample beta a_beta &lt;- sum(x^2) / sigma2[k-1] + 1 / sigma2_beta b_beta &lt;- sum(x*(y - mu[k])) / sigma2[k-1] + mu_beta / sigma2_beta beta[k] &lt;- rnorm(1, b_beta / a_beta, sqrt(1 / a_beta)) ## sample sigma2 sigma2[k] &lt;- 1 / rgamma(1, alpha_0 + N/2, beta_0 + 1/2 * sum((y - (mu[k] + x * beta[k]))^2)) } To visualize the MCMC samples, we can plot what are called trace plots samples &lt;- c(mu, beta, sigma2) library(ggplot2) df &lt;- data.frame( sample = 1:K, params = factor(c(rep(&quot;mu&quot;, K), rep(&quot;beta&quot;, K), rep(&quot;sigma2&quot;, K))), values = samples) ggplot(df, aes(x=sample, y=values, group=params)) + geom_line() + facet_wrap(~params, scales=&quot;free&quot;, ncol=1) The plotting window is strongly influenced by the initial condition. Instead, we can throw away the first 1000 samples (called burn-in) and examine the remaining 4000 samples for each parameter. burnin &lt;- 1000 ## subset the samples to only post-burnin samples &lt;- cbind(mu[-c(1:burnin)], beta[-c(1:burnin)], sigma2[-c(1:burnin)]) colnames(samples) &lt;- c(&quot;mu&quot;, &quot;beta&quot;, &quot;sigma2&quot;) df &lt;- data.frame( sample = (burnin+1):K, params = factor(c(rep(&quot;mu&quot;, K-burnin), rep(&quot;beta&quot;, K-burnin), rep(&quot;sigma2&quot;, K-burnin))), values = c(samples)) ## fit the linear model using the MLE fit &lt;- lm(y~x) df_lm &lt;- data.frame( params = factor(c(&quot;mu&quot;, &quot;beta&quot;, &quot;sigma2&quot;)), values = c(fit$coefficients[1], fit$coefficients[2], summary(fit)$sigma^2), row.names = NULL) ggplot(df, aes(x=sample, y=values, group=params)) + geom_line() + facet_wrap(~params, scales=&quot;free&quot;, ncol=1) + geom_hline(data=df_lm, aes(yintercept=values, group=params), col=&quot;red&quot;) We can think of each of the \\(k\\) iterations as a time step and can use methods from time series to examine the estimates. For example, the ACF for each parameter is layout(matrix(1:6, 3, 2, byrow=TRUE)) acf(samples[,&quot;mu&quot;]) pacf(samples[, &quot;mu&quot;]) acf(samples[,&quot;beta&quot;]) pacf(samples[, &quot;beta&quot;]) acf(samples[,&quot;sigma2&quot;]) pacf(samples[, &quot;sigma2&quot;]) We can also ask what is the effective sample size from our estimates (the equivalent sample size of uncorrelated samples) library(coda) effectiveSize(samples) ## mu beta sigma2 ## 449.7 427.7 4000.0 Because the samples of \\(\\mu\\), \\(\\beta\\), and \\(\\sigma^2\\) obtained from the Gibbs sampler are from probability distributions, we can calculate any quantity of interest directly. For example, the estimate for the mean of the parameters is simply the mean of the MCMC samples colMeans(samples) apply(samples, 2, sd) apply(samples, 2, quantile, prob=c(0.025, 0.975)) ## mu beta sigma2 ## -0.36411 0.41601 0.04331 ## mu beta sigma2 ## 0.040486 0.009816 0.005055 ## mu beta sigma2 ## 2.5% -0.4438 0.3972 0.03459 ## 97.5% -0.2859 0.4357 0.05456 and we can compare these estimates to those from the MLE summary(fit) ## ## Call: ## lm(formula = y ~ x) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.565 -0.124 -0.019 0.133 0.643 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.36308 0.03976 -9.13 4.7e-16 *** ## x 0.41576 0.00958 43.39 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.206 on 148 degrees of freedom ## Multiple R-squared: 0.927, Adjusted R-squared: 0.927 ## F-statistic: 1.88e+03 on 1 and 148 DF, p-value: &lt;2e-16 and the 95% credible interval for each parameter is simply the 2.5% and 97.5% quantiles of the samples. apply(samples, 2, quantile, prob= c(0.025, 0.975)) ## mu beta sigma2 ## 2.5% -0.4438 0.3972 0.03459 ## 97.5% -0.2859 0.4357 0.05456 We can compare these to the esimates from the MLE confint(fit) ## 2.5 % 97.5 % ## (Intercept) -0.4417 -0.2845 ## x 0.3968 0.4347 We can also plot the posterior distribution n_pred &lt;- 1000 x_pred &lt;- seq(min(iris$Petal.Length), max(iris$Petal.Length), length.out = n_pred) y_hat &lt;- t(sapply(1:n_pred, function(i) { samples[, &quot;mu&quot;] + x_pred[i] * samples[, &quot;beta&quot;] })) df_mcmc &lt;- data.frame( x = x_pred, y_hat = c(y_hat), iteration = rep(1:ncol(y_hat), each = nrow(y_hat)) ) if (!file.exists(here::here(&quot;images&quot;, &quot;posterior-linear-regression.png&quot;))) { png(file = here::here(&quot;images&quot;, &quot;posterior-linear-regression.png&quot;), width = 16, height = 9, units = &quot;in&quot;, res = 400) print( ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width)) + geom_point() + geom_line(data = df_mcmc, aes(x = x, y = y_hat, group = iteration), col = &quot;red&quot;, alpha = 0.05) + ggtitle(&quot;Posterior Distribution of mean response&quot;) ) dev.off() } knitr::include_graphics(here::here(&quot;images&quot;, &quot;posterior-linear-regression.png&quot;)) Posterior predictive distribution n_pred &lt;- 1000 x_pred &lt;- seq(min(iris$Petal.Length), max(iris$Petal.Length), length.out = n_pred) y_tilde &lt;- t(samples[, &quot;mu&quot;] + sapply(1:n_pred, function(i) { x_pred[i] * samples[, &quot;beta&quot;] + rnorm(length(samples[, &quot;sigma2&quot;]), 0, sqrt(samples[, &quot;sigma2&quot;]))})) df_mcmc &lt;- data.frame( x = x_pred, y_tilde = apply(y_tilde, 1, mean), lower_95 = apply(y_tilde, 1, quantile, prob = 0.025), upper_95 = apply(y_tilde, 1, quantile, prob = 0.975) ) ggplot(data = iris, aes(x = Petal.Length, y = Petal.Width)) + geom_point() + geom_line(data = df_mcmc, aes(x = x, y = y_tilde), col = &quot;red&quot;) + geom_ribbon(data = df_mcmc, aes(x = x, ymin = lower_95, ymax = upper_95), fill = &quot;orange&quot;, alpha = 0.25, inherit.aes = FALSE) + ggtitle(&quot;Posterior predictive distribution&quot;) 12.5.1 General recommendations for writing MCMC software Have the first dimension of each object be the number of MCMC samples Start with simple models and build sequentially to the complex model of interest Example: For spatio-temporal models, first start with linear regression, then add spatial component, then add spatio-temporal model Find a workflow that works State the model Calculate the full conditional distributions Write the MCMC using a template "],
["day-13.html", "13 Day 13 13.1 Announcements 13.2 The Metropolis Algorithm 13.3 The proposal distribution 13.4 The Metropolis update", " 13 Day 13 13.1 Announcements 13.2 The Metropolis Algorithm Previously we discussed how to sample from a Bayesian Hierarchical model when the conditional posterior distributions are known analytic distributions. Often, the conditional posterior distribution is not available in closed form. There are numerous methods to sample from the conditional posterior in this setting, including importance sampling, slice sampling, and hybrid/Hamiltonian sampling, among others. Today we will introduce the Metropolis algorithm, one of the most widely used and most effective methods for sampling from a conditional posterior without an analytically known distribution. Recall the algorithm for the Gibbs sampler: The Metropolis algorithm requires two components: the full conditional distribution (known up to a proportionality constant) and a proposal distribution \\([ \\theta^\\star]\\) that generates possible values \\(\\theta^{\\star}\\) for the Markov chain given the value \\(\\theta^{(k-1)}\\) of the parameter at iteration \\(k-1\\). The algorithm is 13.3 The proposal distribution The proposal distribution can either be independent of the current state of the Markov chain \\([ \\theta^\\star]\\) or dependent on the current state \\([ \\theta^\\star | \\theta^{(k-1)} ]\\). Dependent proposal distributions are the most common, with the random walk proposal the most commonly used proposal. The random walk proposal distribution \\([ \\theta^\\star | \\theta^{(k-1)} ]\\) is \\[\\begin{align*} \\theta^{\\star} &amp; \\sim \\operatorname{N} (\\theta^{(k-1)}, \\sigma^2_{\\mbox{tune}}), \\end{align*}\\] where the mean of the random walk is the previous value of the parameter \\(\\theta^{(k-1)}\\) and the variance \\(\\sigma^2_{\\mbox{tune}}\\) is a tuning parameter that is chosen to “tune” the performance of the MCMC sampler. A small choice of \\(\\sigma^2_{\\mbox{tune}}\\) will increase the acceptance rate of the Metropolis sampler because the proposed value will be relatively “close” to the previously accepted value \\(\\theta^{(k-1)}\\) while a larger \\(\\sigma^2_{\\mbox{tune}}\\) will allow the Markov chain to make bigger transitions and explore the range of the posterior distribution. Hence, there is a balance between generating proposals that will be accepted relative to faster exploration of the range of the posterior. A very important property of the proposal distribution is symmetry. A proposal distribution is symmetric if \\([ \\theta^\\star | \\theta^{(k-1)} ] = [ \\theta^{(k-1)} | \\theta^\\star ]\\) which means that the conditional probability of \\(\\theta^{\\star}\\) given \\(\\theta^{(k-1)}\\) is equal to the conditional probability of \\(\\theta^{(k-1)}\\) given \\(\\theta^{\\star}\\). The random walk distribution is a symmetric proposal but a log-normal random walk is not symmetric. theta_k &lt;- 5 theta_star &lt;- 6 sigma2_tune &lt;- 0.5 ## check the symmetry condition for the random walk all.equal(dnorm(theta_k, theta_star, sqrt(sigma2_tune)), dnorm(theta_star, theta_k, sqrt(sigma2_tune))) ## [1] TRUE ## check the symmetry condition for the log-normal random walk all.equal(dlnorm(log(theta_k), log(theta_star), sqrt(sigma2_tune)), dlnorm(log(theta_star), log(theta_k), sqrt(sigma2_tune))) ## [1] &quot;Mean relative difference: 0.7701&quot; library(latex2exp) par(mar = c(5,5,2.75,2.75) + 0.1) layout(matrix(1:4, 2, 2)) ## normal proposal curve(dnorm(x, theta_k, sqrt(sigma2_tune)), from=2, to=8, main=&quot;Normal Proposal&quot;, ylab = expression(paste(&quot;[&quot;, theta^(k-1), &quot;|&quot;, theta^{&quot;*&quot;}, &quot;]&quot;)), xlab = expression(paste(theta^{&quot;*&quot;}))) abline(v=theta_k, lty=2) text(5.65, 0.575, expression(theta^{(k-1)})) ## reverse of normal proposal curve(dnorm(theta_star, x, sqrt(sigma2_tune)), from=3, to=9, main=&quot;Reverse of Normal Proposal&quot;, ylab = expression(paste(&quot;[&quot;, theta^{&quot;*&quot;}, &quot;|&quot;, theta^{(k-1)}, &quot;]&quot;)), xlab = expression(paste(theta^{(k-1)}))) abline(v=theta_star, lty=2) text(6.35, 0.575, expression(theta^{&quot;*&quot;})) ## log-Normal proposal curve(dlnorm(x, log(theta_k), sqrt(sigma2_tune)), from=1, to=10, main=&quot;log-Normal Proposal&quot;, ylab = expression(paste(&quot;[&quot;, theta^(k-1), &quot;|&quot;, theta^{&quot;*&quot;}, &quot;]&quot;)), xlab = expression(paste(theta^{&quot;*&quot;}))) abline(v=theta_k, lty=2) text(5.75, 0.14, expression(theta^{(k)})) ## reverse of log-Normal proposal curve(dlnorm(theta_star, x, sqrt(sigma2_tune)), from=-2, to=4, main=&quot;Reverse of log-Normal Proposal&quot;, ylab = expression(paste(&quot;[&quot;, theta^{&quot;*&quot;}, &quot;|&quot;, theta^{(k-1)}, &quot;]&quot;)), xlab = expression(paste(log(theta^{(k-1)})))) abline(v=log(theta_star), lty=2) text(0.75, 0.095, expression(log(theta^{&quot;*&quot;}))) Note that the bottom right figure shows a symmetric looking distribution. Symmetry only implies that the conditional probabilities can be interchanged, not that the probability distribution is itself symmetric. The two most commonly used proposal distributions, the normal and the uniform distribution, are symmetric. 13.4 The Metropolis update The Metropolis update is only valid for symmetric proposal distributions. The more general Metropolis-Hastings update is used for assymetric proposal distributions and will be introduced later. If the value of \\(\\theta\\) at iteration \\(k-1\\) is \\(\\theta^{(k-1)}\\), then the conditional density for \\(\\theta\\) is \\[\\begin{align*} [ \\theta^{(k-1)} | \\cdot ] &amp; = \\frac{[\\mathbf{y} | \\theta^{(k-1)}] [\\theta^{(k-1)}]}{[ \\mathbf{y} ]}, \\end{align*}\\] and the conditional density of the proposed value \\(\\theta^{\\star}\\) is \\[\\begin{align*} [ \\theta^{\\star} | \\cdot ] &amp; = \\frac{[\\mathbf{y} | \\theta^{\\star}] [\\theta^{\\star}]}{[\\mathbf{y}]}, \\end{align*}\\] where both of these expressions require the difficult evaluation of the marginal data distribution \\([\\mathbf{y}]\\). However, the ratio \\(a\\) of the two distributions \\[\\begin{align*} a &amp; = \\frac{[ \\theta^{\\star} | \\cdot ]}{[ \\theta^{(k-1)} | \\cdot ]} \\\\ a &amp; = \\frac{\\frac{[\\mathbf{y} | \\theta^{\\star}] [\\theta^{\\star}]}{[\\mathbf{y}]}} {\\frac{[\\mathbf{y} | \\theta^{(k-1)}] [\\theta^{(k-1)}]}{[ \\mathbf{y} ]}} \\\\ a &amp; = \\frac{[\\mathbf{y} | \\theta^{\\star}] [\\theta^{\\star}]} { [\\mathbf{y} | \\theta^{(k-1)}] [\\theta^{(k-1)}]} \\end{align*}\\] does not depend on the marginal distribution of the data \\([\\mathbf{y}]\\), eliminating the need to calculate the difficult integral. Given the proposal \\(\\theta^{\\star}\\) from a symmetric proposal distribution centered on the previous value \\(\\theta^{(k-1)}\\), we accept the new \\(\\theta^\\star\\) with probability \\(min(1, a)\\) and otherwise reject the proposal and set \\(\\theta^{(k)} = \\theta^{(k-1)}\\). The complete Metropolis algorithm is For example, consider the model \\[\\begin{align*} y_i &amp; \\sim \\operatorname{N} (\\theta, 1) \\end{align*}\\] where \\(\\theta\\) is unknown. Assuming a normal prior \\(\\theta \\propto \\operatorname{N}(0, 1000)\\), we will sample from the conditional posterior \\([\\theta | \\cdot] \\propto \\prod_{i=1}^N \\operatorname{N}(\\theta, 1) \\times 1\\). ## simulate data according to the model N &lt;- 100 theta &lt;- 5 y &lt;- rnorm(N, theta, 1) We will run the Metropolis algorithm for \\(K=5000\\) iterations using a normal proposal dsitribution. K &lt;- 5000 theta_sim &lt;- rep(0, K) sigma2_tune &lt;- 0.025 accept_prob &lt;- 0 for (k in 2:K) { theta_star &lt;- rnorm(1, theta_sim[k-1], sqrt(sigma2_tune)) a &lt;- prod(dnorm(y, theta_star, 1)) * dnorm(theta_star, 0, sqrt(1000)) / prod(dnorm(y, theta_sim[k-1], 1)) * dnorm(theta_sim[k-1], 0, sqrt(1000)) u &lt;- runif(1, 0, 1) if (u &lt; a) { ## accept the proposed theta_star theta_sim[k] &lt;- theta_star ## keep running total of acceptance rate accept_prob &lt;- accept_prob + 1 / K } else { ## reject the proposed theta_star theta_sim[k] &lt;- theta_sim[k-1] } } ## Error in if (u &lt; a) {: missing value where TRUE/FALSE needed Oops! We get an error! When working with \\(a\\) directly, we get a product of small likelihoods. Instead, let’s work with the logarithm of the probabilities \\[\\begin{align*} a &amp; = \\exp \\left( \\log \\left( [\\mathbf{y} | \\theta^{\\star}] [\\theta^{\\star}] \\right) - \\log \\left( [\\mathbf{y} | \\theta^{(k-1)}] [\\theta^{(k-1)}] \\right) \\right) \\\\ &amp; = \\exp \\left( \\log [\\mathbf{y} | \\theta^{\\star}] + \\log [\\theta^{\\star}] - \\log [\\mathbf{y} | \\theta^{(k-1)}] - \\log [\\theta^{(k-1)}] \\right), \\end{align*}\\] where all we did was re-write the likelihood as the exponential of a log (an identity transformation). What this does is reduce the effect of multiplying small values that go to computer zero (numeric underflow) and cause problems. For example prod(dnorm(y, 8, 1)) ## [1] 3.779e-270 is very very small (close to zero) and gets closer to numeric underflow as N increase, whereas sum(dnorm(y, 8, 1, log=TRUE)) all.equal(prod(dnorm(y, 8, 1)), exp(sum(dnorm(y, 8, 1, log=TRUE)))) ## [1] -620.4 ## [1] TRUE is a more reasonable number (nowhere near computer zero), but produces the same answer when exponentiated. Writing the Metropolis algorithm on the log-probability scale is K &lt;- 5000 theta_sim &lt;- rep(0, K) sigma2_tune &lt;- 0.025 accept_prob &lt;- 0 for (k in 2:K) { theta_star &lt;- rnorm(1, theta_sim[k-1], sqrt(sigma2_tune)) ## calcuate the numerator of the ratio mh1 &lt;- sum(dnorm(y, theta_star, 1, log=TRUE)) + dnorm(theta_star, 0, sqrt(1000), log=TRUE) ## calcuate the denomiator of the ratio mh2 &lt;- sum(dnorm(y, theta_sim[k-1], 1, log=TRUE)) + dnorm(theta_sim[k-1], 0, sqrt(1000), log=TRUE) a &lt;- exp(mh1 - mh2) u &lt;- runif(1, 0, 1) if (u &lt; a) { ## accept the proposed theta_star theta_sim[k] &lt;- theta_star ## keep running total of acceptance rate accept_prob &lt;- accept_prob + 1 / K } else { ## reject the proposed theta_star theta_sim[k] &lt;- theta_sim[k-1] } } Plotting the output below looks much, much better. All thanks to avoiding numeric underflow! The lesson: always evaluate Metropolis ratios on the log scale. ## Plot the output plot(theta_sim, type=&#39;l&#39;, main=paste0(&quot;acceptance prob = &quot;, round(accept_prob, digits=2)), xlab=&quot;MCMC iteration&quot;, ylab=expression(theta)) abline(h=theta, col=&quot;red&quot;) We can also explore the effects of the tuning parameter on the behavior of the model Let’s examine these plots, throwing away the first 500 samples as “burn-in.” Notice that the smallest choice of \\(\\sigma^2_{\\mbox{tune}}\\) produces a chain with high autocorrelation that doesn’t die off at large lags. The choice of \\(\\sigma^2_{\\mbox{tune}} = 0.05\\) produces the quickest decay in the ACF plot below, and has an acceptance rate of about 0.46 (0.44 is optimal for a univariate Metropolis update). The trace plots for the larger values of the tuning parameter have low acceptance rates and get “stuck” at certain parameter values for long periods of time. library(coda) effectiveSize(theta_sim1[-c(1:500)]) effectiveSize(theta_sim2[-c(1:500)]) effectiveSize(theta_sim3[-c(1:500)]) effectiveSize(theta_sim4[-c(1:500)]) ## var1 ## 227.2 ## var1 ## 1103 ## var1 ## 335.6 ## var1 ## 117.9 "],
["day-14.html", "14 Day 14 14.1 Announcements 14.2 The Metropolis-Hastings Algorithm 14.3 The Metropolis-Hastings algorithm 14.4 Example 14.5 Posterior 14.6 Conditional posterior for \\(\\phi\\) 14.7 Conditional posterior for \\(\\sigma^2\\) 14.8 MCMC using a symmetric proposal (Metropolis)", " 14 Day 14 14.1 Announcements 14.2 The Metropolis-Hastings Algorithm Last time, we discussed the Metropolis algorithm for sampling from a distribution where the conditional posterior distribution is not known in an analytic form. The Metropolis algorithm assumes the proposal distribution is symmetric. The Metropolis-Hastings algorithm allows for asymmetric proposal distributions. A good intuition for how the Metropolis Hastings algorithm works is presented by Chib and Greenberg in Understanding the Metropolis-Hastings Algorithm. This begs the question: Why would one want to use an asymmetric proposal distribution? There are many reasons, but the most compelling and most common is to guarantee that the proposed value of the parameter is in the support of the parameter space. For example, a variance parameter must be positive and a symmetric normal proposal could generate negative proposals that must be rejected with probability 1. This can result in inefficient MCMC samplers that have very low acceptance rates and therefor exhibit poor mixing. Instead, we can use an asymmetric proposal that guarantees a positive proposal and should lead to higer acceptance rates. 14.3 The Metropolis-Hastings algorithm Recall for a symmetric proposal, the Metropolis ratio is \\[\\begin{align*} a &amp; = \\frac{[\\mathbf{y} | \\theta^{\\star}] [\\theta^{\\star}] } { [\\mathbf{y} | \\theta^{(k-1)}] [\\theta^{(k-1)}] }. \\end{align*}\\] Given an asymmetric proposal \\([\\theta^{\\star}|\\theta^{(k-1)}]\\), the Metropolis-Hastings ratio is \\[\\begin{align*} a &amp; = \\frac{[\\mathbf{y} | \\theta^{\\star}] [\\theta^{\\star}] } { [\\mathbf{y} | \\theta^{(k-1)}] [\\theta^{(k-1)}] } \\frac{[\\theta^{(k-1)} | \\theta^{\\star}]} {[\\theta^{\\star} | \\theta^{(k-1)}]}, \\end{align*}\\] where \\(\\frac{[\\theta^{(k-1)} | \\theta^{\\star}]} {[\\theta^{\\star} | \\theta^{(k-1)}]}\\) is a correction factor that accounts for the asymmetric proposal. The Metropolis algorithm can be shown to be a special case of the Metropolis-Hastings algorithm by noting that \\(\\frac{[\\theta^{(k-1)} | \\theta^{\\star}]} {[\\theta^{\\star} | \\theta^{(k-1)}]} = 1\\) when the proposal is symmetric. 14.4 Example Let’s consider an AR(1) model \\[\\begin{align*} y_t &amp; = \\phi y_{t-1} + \\varepsilon_t \\end{align*}\\] where \\(\\varepsilon_t \\stackrel{iid} \\sim \\operatorname{N}(0, \\sigma^2)\\) and both \\(\\phi\\) and \\(\\sigma^2\\) are unknown. Simulated data are shown below. N &lt;- 500 phi_sim &lt;- 0.99 sigma2_sim &lt;- 1.2 y &lt;- rep(0, N) for (t in 2:N) { y[t] &lt;- phi_sim * y[t-1] + rnorm(1, 0, sqrt(sigma2_sim)) } plot(y, type=&#39;l&#39;, xlab=&quot;time&quot;, main=&quot;simulated AR(1) data&quot;) To complete the model, we must assign prior distributions for \\(\\phi\\) and \\(\\sigma^2\\). A simple choice of prior for \\(\\phi\\) is Uniform(-1, 1) prior and for \\(\\sigma^2\\) we assign a half-Cauchy prior where \\[\\begin{align*} \\sigma^2 &amp; \\sim \\operatorname{Cauchy^+}(s^2) \\end{align*}\\] with \\(s^2\\)=25 where the half-Cauchy has the pdf \\[\\begin{align*} [\\sigma^2] = \\frac{2 * I(\\sigma^2 &gt; 0)}{\\pi s (1 + \\frac{\\sigma^2}{s^2})}. \\end{align*}\\] 14.5 Posterior The posterior distribution is \\[\\begin{align*} [\\phi, \\sigma^2 | \\mathbf{y}] &amp; = \\prod_{t=1}^N [y_t | y_{t-1}, \\phi, \\sigma^2] [\\phi] [\\sigma^2] \\end{align*}\\] 14.6 Conditional posterior for \\(\\phi\\) The conditional posterior for \\(\\phi\\) is \\[\\begin{align*} [\\phi | \\cdot] &amp; \\propto \\prod_{t=1^N} [y_t | y_{t-1}, \\phi, \\sigma^2] [\\phi] \\\\ &amp; \\propto \\prod_{t=1^N} N(y_t | \\phi y_{t-1}, \\sigma^2 ) I(-1 &lt; \\phi &lt; 1) \\end{align*}\\]} which has no known analytic distribution so we will sample using Metropolis-Hastings. 14.7 Conditional posterior for \\(\\sigma^2\\) The conditional posterior for \\(\\phi\\) is \\[\\begin{align*} [\\phi | \\cdot] &amp; \\propto \\prod_{t=1^N} [y_t | y_{t-1}, \\phi, \\sigma^2] [\\sigma^2] \\\\ &amp; \\propto \\prod_{t=1^N} N(y_t | \\phi y_{t-1}, \\sigma^2 ) \\operatorname{Cauchy^+}(\\sigma^2 | s^2) \\end{align*}\\] which has no known analytic distribution so we will sample using Metropolis-Hastings. 14.8 MCMC using a symmetric proposal (Metropolis) library(LaplacesDemon) ## ## Attaching package: &#39;LaplacesDemon&#39; ## The following objects are masked from &#39;package:geoR&#39;: ## ## dinvchisq, rinvchisq ## The following objects are masked from &#39;package:MCMCpack&#39;: ## ## BayesFactor, ddirichlet, dinvgamma, rdirichlet, ## rinvgamma ## The following objects are masked from &#39;package:mvnfast&#39;: ## ## dmvn, dmvt, rmvn, rmvt ## The following objects are masked from &#39;package:spatstat&#39;: ## ## is.data, is.stationary ## The following object is masked from &#39;package:purrr&#39;: ## ## partial K &lt;- 5000 phi &lt;- rep(0, N) sigma2 &lt;- rep(0, N) ## initialize phi and sigma2 phi[1] &lt;- runif(1, -1, 1) sigma2[1] &lt;- runif(1, 1, 5) ## set the tuning parameters phi_tune &lt;- 0.0095 accept_phi &lt;- 0 sigma2_tune &lt;- 0.125 accept_sigma2 &lt;- 0 ## ## MCMC loop ## for (k in 2:K) { ## sample phi phi_star &lt;- rnorm(1, phi[k-1], phi_tune) ## check to make sure phi_star is reasonable, if not, we reject and move on if (phi_star &gt; -1 &amp; phi_star &lt; 1) { mh1 &lt;- sum(dnorm(y[2:N], phi_star * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) mh2 &lt;- sum(dnorm(y[2:N], phi[k-1] * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) a &lt;- exp(mh1-mh2) if (runif(1) &lt; a) { ## accept the proposed value phi[k] &lt;- phi_star accept_phi &lt;- accept_phi + 1/K } else { phi[k] &lt;- phi[k-1] } } else { ## phi_star was outside the support, keep at the previous value phi[k] &lt;- phi[k-1] } ## sample sigma2 sigma2_star &lt;- rnorm(1, sigma2[k-1], sigma2_tune) ## check to make sure sigma2_star is reasonable, if not, we reject and move on if (sigma2_star &gt; 0) { mh1 &lt;- sum(dnorm(y[2:N], phi[k] * y[1:(N-1)], sqrt(sigma2_star), log=TRUE)) + dhalfcauchy(sigma2_star, sqrt(25), log=TRUE) mh2 &lt;- sum(dnorm(y[2:N], phi[k] * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) + dhalfcauchy(sigma2[k-1], sqrt(25), log=TRUE) a &lt;- exp(mh1-mh2) if (runif(1) &lt; a) { ## accept the proposed value sigma2[k] &lt;-sigma2_star accept_sigma2 &lt;- accept_sigma2 + 1/K } else { sigma2[k] &lt;- sigma2[k-1] } } else { ## sigma2_star was outside the support, keep at the previous value sigma2[k] &lt;- sigma2[k-1] } } The trace plots showing the parameter estimates, post burn-in, are below. From these, we see that the MCMC model is estimating the parameters reasonably. burnin &lt;- 1000 samples &lt;- cbind(phi[-c(1:burnin)], sigma2[-c(1:burnin)]) colnames(samples) &lt;- c(&quot;phi&quot;, &quot;sigma2&quot;) layout(matrix(1:2, 2, 1)) plot(samples[, &quot;phi&quot;], type=&#39;l&#39;, main=paste0(&quot;Trace plot for phi, acceptance=&quot;, round(accept_phi, digits=2))) abline(h=phi_sim, col=&quot;red&quot;) plot(samples[, &quot;sigma2&quot;], type=&#39;l&#39;, main=paste0(&quot;Trace plot for sigma2, acceptance=&quot;, round(accept_sigma2, digits=2))) abline(h=sigma2_sim, col=&quot;red&quot;) library(coda) effectiveSize(samples) ## phi sigma2 ## 618.4 826.8 In the model above, we used symmetric proposals and threw out parameter values that were outside the allowable range. Because \\(\\phi\\) has a true value close to the limit of allowable values, the Metropolis algorithm could be very inefficient. To improve efficiency, let’s rewrite the algorithm using truncated normal proposals (which are asymmetric). library(LaplacesDemon) library(truncnorm) K &lt;- 25000 phi &lt;- rep(0, N) sigma2 &lt;- rep(0, N) ## initialize phi and sigma2 phi[1] &lt;- runif(1, -1, 1) sigma2[1] &lt;- runif(1, 1, 5) ## set the tuning parameters phi_tune &lt;- 0.0175 accept_phi &lt;- 0 sigma2_tune &lt;- 0.175 accept_sigma2 &lt;- 0 ## ## MCMC loop ## for (k in 2:K) { ## sample phi ## use truncated normal proposal phi_star &lt;- rtruncnorm(1, a=-1, b=1, phi[k-1], phi_tune) ## No need to check if phi_star is between -1 and 1 mh1 &lt;- sum(dnorm(y[2:N], phi_star * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) + log(dtruncnorm(phi[k-1], a=-1, b=1, phi_star, phi_tune)) mh2 &lt;- sum(dnorm(y[2:N], phi[k-1] * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) + log(dtruncnorm(phi_star, a=-1, b=1, phi[k-1], phi_tune)) a &lt;- exp(mh1-mh2) if (runif(1) &lt; a) { ## accept the proposed value phi[k] &lt;- phi_star accept_phi &lt;- accept_phi + 1/K } else { phi[k] &lt;- phi[k-1] } ## sample sigma2 sigma2_star &lt;- rtruncnorm(1, a=0, b=Inf, sigma2[k-1], sigma2_tune) ## No need to check to make sure sigma2_star is positive mh1 &lt;- sum(dnorm(y[2:N], phi[k] * y[1:(N-1)], sqrt(sigma2_star), log=TRUE)) + dhalfcauchy(sigma2_star, sqrt(25), log=TRUE) + log(dtruncnorm(sigma2[k-1], a=0, b=Inf, sigma2_star, sigma2_tune)) mh2 &lt;- sum(dnorm(y[2:N], phi[k] * y[1:(N-1)], sqrt(sigma2[k-1]), log=TRUE)) + dhalfcauchy(sigma2[k-1], sqrt(25), log=TRUE) + log(dtruncnorm(sigma2_star, a=0, b=Inf, sigma2[k-1], sigma2_tune)) a &lt;- exp(mh1-mh2) if (runif(1) &lt; a) { ## accept the proposed value sigma2[k] &lt;-sigma2_star accept_sigma2 &lt;- accept_sigma2 + 1/K } else { sigma2[k] &lt;- sigma2[k-1] } } The trace plots below show the increased efficiency in the acceptance rate at the same tuning parameter value and show better effective sample size. burnin &lt;- 1000 samples &lt;- cbind(phi[-c(1:burnin)], sigma2[-c(1:burnin)]) colnames(samples) &lt;- c(&quot;phi&quot;, &quot;sigma2&quot;) layout(matrix(1:2, 2, 1)) plot(samples[, &quot;phi&quot;], type=&#39;l&#39;, main=paste0(&quot;Trace plot for phi, acceptance=&quot;, round(accept_phi, digits=2))) abline(h=phi_sim, col=&quot;red&quot;) plot(samples[, &quot;sigma2&quot;], type=&#39;l&#39;, main=paste0(&quot;Trace plot for sigma2, acceptance=&quot;, round(accept_sigma2, digits=2))) abline(h=sigma2_sim, col=&quot;red&quot;) hist(samples[,&quot;phi&quot;], breaks=100) abline(v=phi_sim, col=&quot;red&quot;) abline(v=mean(samples[, &quot;phi&quot;]), col=&quot;blue&quot;) abline(v=quantile(samples[, &quot;phi&quot;], prob=c(0.025, 0.975)), col=&quot;green&quot;, lwd=2) hist(samples[,&quot;sigma2&quot;], breaks=100) abline(v=sigma2_sim, col=&quot;red&quot;) abline(v=mean(samples[, &quot;sigma2&quot;]), col=&quot;blue&quot;) abline(v=quantile(samples[, &quot;sigma2&quot;], prob=c(0.025, 0.975)), col=&quot;green&quot;, lwd=2) library(coda) effectiveSize(samples) ## phi sigma2 ## 7381 5362 "],
["day-15.html", "15 Day 15 15.1 Announcements", " 15 Day 15 15.1 Announcements "],
["day-16.html", "16 Day 16 16.1 Announcements", " 16 Day 16 16.1 Announcements "],
["day-17.html", "17 Day 17 17.1 Announcements 17.2 Gaussian process representations", " 17 Day 17 17.1 Announcements 17.2 Gaussian process representations Karhunen-Loève expansion "],
["day-18.html", "18 Day 18 18.1 Announcements 18.2 Gaussian process representations continued", " 18 Day 18 18.1 Announcements Reading Assignment 3 is due Monday 3/4 in class. Data Camp HW 3 - Nonlinear modeling in R with GAMs due Monday 3/4 at 11:59PM CST. HW 3 - Bayesian spatial models is due Wednesday 3/4 by 5PM. The raw Rmarkdown file is available here 18.2 Gaussian process representations continued "],
["day-19.html", "19 Day 19 19.1 Announcements 19.2 Stochastic Integration", " 19 Day 19 19.1 Announcements 19.2 Stochastic Integration https://en.wikipedia.org/wiki/It%C3%B4_calculus "],
["day-20.html", "20 Day 20 20.1 Announcements 20.2 spBayes example 20.3 Global vs. Local Basis function 20.4 example local basis 20.5 Fitting spatial models many ways 20.6 Empirical Orthogonal Functions 20.7 Spatial modeling with mgcv package 20.8 Spatial modeling with Kernels 20.9 Sparse vs. Dense matrix computation", " 20 Day 20 library(tidyverse) library(STRbook) library(fields) library(mvnfast) library(fda) library(spBayes) library(coda) library(mgcv) library(glmnet) library(microbenchmark) 20.1 Announcements 20.2 spBayes example Fitting a spatial model using the NOAA_df_1990 data.frame from the STRbook package. We will focus on the average max temperature for July 1990. data(&quot;NOAA_df_1990&quot;) ## add a factor variable for left_join NOAA_df_1990$id_factor &lt;- factor(NOAA_df_1990$id) dat &lt;- NOAA_df_1990 %&gt;% subset(year == 1990 &amp; month == 7 &amp; proc == &quot;Tmax&quot;) %&gt;% group_by(id_factor) %&gt;% summarize(mean_Tmax = mean(z)) # ## add back in the lat/lon variables dat &lt;- NOAA_df_1990 %&gt;% subset(year == 1990 &amp; month == 7 &amp; proc == &quot;Tmax&quot; &amp; day == 1) %&gt;% left_join(dat, by = &quot;id_factor&quot;) n.samples &lt;- 5000 starting &lt;- list(phi = 3/100, sigma.sq = 2, tau.sq = 2) tuning &lt;- list(phi = 2.5, sigma.sq = 0.1, tau.sq = 0.1) p &lt;- 3 ## use lat and lon as predictors (plus an intercept) coords &lt;- as.matrix(cbind(dat$lon, dat$lat)) max_dist &lt;- max(rdist(coords)) priors &lt;- list( beta.Norm = list(rep(0, p), diag(1e+06, p)), phi.Unif = c(0.01, 100), sigma.sq.IG = c(2, 2), tau.sq.IG = c(2, 2) ) cov.model &lt;- &quot;exponential&quot; fit_e &lt;- spLM( dat$z ~ dat$lon + dat$lat, coords = coords, starting = starting, tuning = tuning, priors = priors, cov.model = cov.model, n.samples = n.samples, n.report = 1000 ) ## ---------------------------------------- ## General model description ## ---------------------------------------- ## Model fit with 136 observations. ## ## Number of covariates 3 (including intercept if specified). ## ## Using the exponential spatial correlation model. ## ## Number of MCMC samples 5000. ## ## Priors and hyperpriors: ## beta normal: ## mu: 0.000 0.000 0.000 ## cov: ## 1000000.000 0.000 0.000 ## 0.000 1000000.000 0.000 ## 0.000 0.000 1000000.000 ## ## sigma.sq IG hyperpriors shape=2.00000 and scale=2.00000 ## tau.sq IG hyperpriors shape=2.00000 and scale=2.00000 ## phi Unif hyperpriors a=0.01000 and b=100.00000 ## ------------------------------------------------- ## Sampling ## ------------------------------------------------- ## Sampled: 1000 of 5000, 20.00% ## Report interval Metrop. Acceptance rate: 12.20% ## Overall Metrop. Acceptance rate: 12.20% ## ------------------------------------------------- ## Sampled: 2000 of 5000, 40.00% ## Report interval Metrop. Acceptance rate: 8.70% ## Overall Metrop. Acceptance rate: 10.45% ## ------------------------------------------------- ## Sampled: 3000 of 5000, 60.00% ## Report interval Metrop. Acceptance rate: 11.00% ## Overall Metrop. Acceptance rate: 10.63% ## ------------------------------------------------- ## Sampled: 4000 of 5000, 80.00% ## Report interval Metrop. Acceptance rate: 10.40% ## Overall Metrop. Acceptance rate: 10.57% ## ------------------------------------------------- ## Sampled: 5000 of 5000, 100.00% ## Report interval Metrop. Acceptance rate: 11.50% ## Overall Metrop. Acceptance rate: 10.76% ## ------------------------------------------------- The above fits the model. However, sometimes it is hard to “tune” the model to get the desired Metropolis acceptance rate. spLM has an option for adaptive MCMC tuning. To enable this option, we add an amcmc option. We are also going to change to a Matern covarince function in the next code as well. n.samples &lt;- 5000 batch.length &lt;- 50 n.batch &lt;- n.samples / batch.length ## must add in values for the smoothness paramter nu starting &lt;- list(phi = 3/100, sigma.sq = 2, tau.sq = 2, nu = 1) tuning &lt;- list(phi = 2.5, sigma.sq = 0.1, tau.sq = 0.1, nu = 0.1) p &lt;- 3 ## use lat and lon as predictors (plus an intercept) coords &lt;- as.matrix(cbind(dat$lon, dat$lat)) max_dist &lt;- max(rdist(coords)) priors &lt;- list( beta.Norm = list(rep(0, p), diag(1e+06, p)), phi.Unif = c(0.01, 100), nu.Unif = c(0.01, 100), sigma.sq.IG = c(2, 2), tau.sq.IG = c(2, 2) ) cov.model &lt;- &quot;matern&quot; fit_m &lt;- spLM( dat$z ~ dat$lon + dat$lat, coords = coords, starting = starting, tuning = tuning, priors = priors, cov.model = cov.model, n.samples = n.samples, amcmc = list( n.batch = n.batch, batch.length = batch.length, accept.rate = 0.44 ), n.report = 10 ## report every 10 adaptive batches ) ## can update the initial tuning parameters given the adapted values ## ---------------------------------------- ## General model description ## ---------------------------------------- ## Model fit with 136 observations. ## ## Number of covariates 3 (including intercept if specified). ## ## Using the matern spatial correlation model. ## ## Using adaptive MCMC. ## ## Number of batches 100. ## Batch length 50. ## Target acceptance rate 0.44000. ## ## Priors and hyperpriors: ## beta normal: ## mu: 0.000 0.000 0.000 ## cov: ## 1000000.000 0.000 0.000 ## 0.000 1000000.000 0.000 ## 0.000 0.000 1000000.000 ## ## sigma.sq IG hyperpriors shape=2.00000 and scale=2.00000 ## tau.sq IG hyperpriors shape=2.00000 and scale=2.00000 ## phi Unif hyperpriors a=0.01000 and b=100.00000 ## nu Unif hyperpriors a=0.01000 and b=100.00000 ## ------------------------------------------------- ## Sampling ## ------------------------------------------------- ## Batch: 10 of 100, 10.00% ## parameter acceptance tuning ## sigma.sq 46.0 0.33578 ## tau.sq 52.0 0.34949 ## phi 6.0 1.43067 ## nu 44.0 0.28613 ## ------------------------------------------------- ## Batch: 20 of 100, 20.00% ## parameter acceptance tuning ## sigma.sq 58.0 0.36375 ## tau.sq 48.0 0.36375 ## phi 6.0 1.29453 ## nu 86.0 0.30997 ## ------------------------------------------------- ## Batch: 30 of 100, 30.00% ## parameter acceptance tuning ## sigma.sq 48.0 0.39404 ## tau.sq 60.0 0.37859 ## phi 8.0 1.17134 ## nu 40.0 0.32913 ## ------------------------------------------------- ## Batch: 40 of 100, 40.00% ## parameter acceptance tuning ## sigma.sq 48.0 0.37859 ## tau.sq 54.0 0.41841 ## phi 12.0 1.05987 ## nu 40.0 0.29781 ## ------------------------------------------------- ## Batch: 50 of 100, 50.00% ## parameter acceptance tuning ## sigma.sq 42.0 0.39404 ## tau.sq 40.0 0.43549 ## phi 18.0 0.95901 ## nu 64.0 0.30997 ## ------------------------------------------------- ## Batch: 60 of 100, 60.00% ## parameter acceptance tuning ## sigma.sq 58.0 0.42686 ## tau.sq 38.0 0.41013 ## phi 18.0 0.86775 ## nu 56.0 0.34257 ## ------------------------------------------------- ## Batch: 70 of 100, 70.00% ## parameter acceptance tuning ## sigma.sq 34.0 0.43549 ## tau.sq 64.0 0.39404 ## phi 26.0 0.78517 ## nu 66.0 0.37859 ## ------------------------------------------------- ## Batch: 80 of 100, 80.00% ## parameter acceptance tuning ## sigma.sq 54.0 0.45326 ## tau.sq 40.0 0.37859 ## phi 22.0 0.71045 ## nu 84.0 0.41841 ## ------------------------------------------------- ## Batch: 90 of 100, 90.00% ## parameter acceptance tuning ## sigma.sq 38.0 0.44428 ## tau.sq 40.0 0.38624 ## phi 22.0 0.64284 ## nu 56.0 0.46241 ## ------------------------------------------------- ## Batch: 100 of 100, 100.00% ## parameter acceptance tuning ## sigma.sq 42.0 0.43549 ## tau.sq 56.0 0.39404 ## phi 32.0 0.58167 ## nu 80.0 0.51105 ## ------------------------------------------------- tuning &lt;- list(phi = 0.5, sigma.sq = 0.38, tau.sq = 0.6, nu = 0.3) fit_m &lt;- spLM( dat$z ~ dat$lon + dat$lat, coords = coords, starting = starting, tuning = tuning, priors = priors, cov.model = cov.model, n.samples = n.samples, amcmc = list( n.batch = n.batch, batch.length = batch.length, accept.rate = 0.44 ), n.report = 10 ## report every 10 adaptive batches ) ## can update the initial tuning parameters given the adapted values ## ---------------------------------------- ## General model description ## ---------------------------------------- ## Model fit with 136 observations. ## ## Number of covariates 3 (including intercept if specified). ## ## Using the matern spatial correlation model. ## ## Using adaptive MCMC. ## ## Number of batches 100. ## Batch length 50. ## Target acceptance rate 0.44000. ## ## Priors and hyperpriors: ## beta normal: ## mu: 0.000 0.000 0.000 ## cov: ## 1000000.000 0.000 0.000 ## 0.000 1000000.000 0.000 ## 0.000 0.000 1000000.000 ## ## sigma.sq IG hyperpriors shape=2.00000 and scale=2.00000 ## tau.sq IG hyperpriors shape=2.00000 and scale=2.00000 ## phi Unif hyperpriors a=0.01000 and b=100.00000 ## nu Unif hyperpriors a=0.01000 and b=100.00000 ## ------------------------------------------------- ## Sampling ## ------------------------------------------------- ## Batch: 10 of 100, 10.00% ## parameter acceptance tuning ## sigma.sq 40.0 0.55778 ## tau.sq 28.0 0.79024 ## phi 12.0 0.63982 ## nu 24.0 0.49560 ## ------------------------------------------------- ## Batch: 20 of 100, 20.00% ## parameter acceptance tuning ## sigma.sq 30.0 0.51490 ## tau.sq 30.0 0.75926 ## phi 22.0 0.57893 ## nu 24.0 0.44844 ## ------------------------------------------------- ## Batch: 30 of 100, 30.00% ## parameter acceptance tuning ## sigma.sq 42.0 0.47531 ## tau.sq 44.0 0.71504 ## phi 32.0 0.52384 ## nu 36.0 0.42232 ## ------------------------------------------------- ## Batch: 40 of 100, 40.00% ## parameter acceptance tuning ## sigma.sq 34.0 0.46590 ## tau.sq 40.0 0.70088 ## phi 26.0 0.47399 ## nu 28.0 0.38213 ## ------------------------------------------------- ## Batch: 50 of 100, 50.00% ## parameter acceptance tuning ## sigma.sq 34.0 0.43008 ## tau.sq 58.0 0.71504 ## phi 18.0 0.43755 ## nu 24.0 0.34577 ## ------------------------------------------------- ## Batch: 60 of 100, 60.00% ## parameter acceptance tuning ## sigma.sq 40.0 0.41321 ## tau.sq 38.0 0.71504 ## phi 30.0 0.39591 ## nu 42.0 0.31286 ## ------------------------------------------------- ## Batch: 70 of 100, 70.00% ## parameter acceptance tuning ## sigma.sq 34.0 0.39701 ## tau.sq 50.0 0.74422 ## phi 26.0 0.35823 ## nu 34.0 0.28881 ## ------------------------------------------------- ## Batch: 80 of 100, 80.00% ## parameter acceptance tuning ## sigma.sq 50.0 0.39701 ## tau.sq 32.0 0.75926 ## phi 34.0 0.33069 ## nu 46.0 0.26661 ## ------------------------------------------------- ## Batch: 90 of 100, 90.00% ## parameter acceptance tuning ## sigma.sq 30.0 0.38915 ## tau.sq 46.0 0.74422 ## phi 52.0 0.30527 ## nu 38.0 0.24611 ## ------------------------------------------------- ## Batch: 100 of 100, 100.00% ## parameter acceptance tuning ## sigma.sq 48.0 0.39701 ## tau.sq 32.0 0.72949 ## phi 28.0 0.29922 ## nu 54.0 0.26133 ## ------------------------------------------------- To get the fitted parameters, we can recover these with composition sampling. ## discard the first half of the samples as burn-in burn.in &lt;- 0.5 * n.samples ## trace plots for the exponential model fit_e &lt;- spRecover(fit_e, start = burn.in, thin = 2, verbose = FALSE) theta.samps &lt;- mcmc.list(fit_e$p.theta.samples, fit_e$p.theta.samples) plot(theta.samps, density = FALSE) beta.samps &lt;- mcmc.list(fit_e$p.beta.recover.samples, fit_e$p.beta.recover.samples) plot(beta.samps, density = FALSE) ## trace plots for the matern model fit_m &lt;- spRecover(fit_m, start = burn.in, thin = 2, verbose = FALSE) theta.samps &lt;- mcmc.list(fit_m$p.theta.samples, fit_m$p.theta.samples) plot(theta.samps, density = FALSE) beta.samps &lt;- mcmc.list(fit_m$p.beta.recover.samples, fit_m$p.beta.recover.samples) plot(beta.samps, density = FALSE) round(summary(theta.samps)$quantiles, 3) ## 2.5% 25% 50% 75% 97.5% ## sigma.sq 14.437 20.865 25.640 32.910 62.267 ## tau.sq 0.538 1.029 1.430 1.917 3.239 ## phi 0.167 0.335 0.472 0.648 1.495 ## nu 0.602 0.819 1.033 1.425 4.846 ## recover the spatial random effects fit_e_w &lt;- fit_e$p.w.recover.samples fit_m_w &lt;- fit_m$p.w.recover.samples ## recover the spatial random effects (eta&#39;s in our class notation) w.samps &lt;- cbind(fit_e_w, fit_m_w) w.summary &lt;- apply(w.samps, 1, function(x) { quantile(x, prob = c(0.025, 0.5, 0.975)) }) Next we can genereate predictions for the exponential model states &lt;- map_data(&quot;state&quot;) pred_coords &lt;- as.data.frame( expand.grid( seq(min(dat$lon), max(dat$lon), length = 100), seq(min(dat$lat), max(dat$lat), length = 100) ) ) colnames(pred_coords) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) ## predict using the tuned Matern fit preds &lt;- spPredict( fit_m, start = burn.in, thin = 10, pred.coords = cbind(pred_coords$lon, pred_coords$lat), pred.covars = cbind(1, pred_coords$lon, pred_coords$lat) ) ## Calculate the prediction means and variances pred_coords$pred_mean &lt;- apply(preds$p.y.predictive.samples, 1, mean) pred_coords$pred_var &lt;- apply(preds$p.y.predictive.samples, 1, var) ## calculate 95% credible intervals for the predictions pred.summary &lt;- apply(preds$p.y.predictive.samples, 1, function(x) { quantile(x, prob = c(0.025, 0.5, 0.975)) }) ggplot(data = pred_coords, aes(x = lon, y = lat, fill = pred_mean)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) ggplot(data = pred_coords, aes(x = lon, y = lat, fill = pred_var)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature prediction variance in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) ## ---------------------------------------- ## General model description ## ---------------------------------------- ## Model fit with 136 observations. ## ## Prediction at 10000 locations. ## ## Number of covariates 3 (including intercept if specified). ## ## Using the matern spatial correlation model. ## ## ------------------------------------------------- ## Sampling ## ------------------------------------------------- ## Sampled: 100 of 251, 39.44% ## Sampled: 200 of 251, 79.28% Note these variances are higher than the MLE estimates (potentially by a lot – link to MLE fits day 11) 20.3 Global vs. Local Basis function simulate some data set.seed(44) n &lt;- 1000 sigma &lt;- 0.2 x &lt;- seq(0, 1, length.out = n) Sigma &lt;- exp( - rdist(x)^2 / 0.02) ## Gaussian covariance with a small amount of &quot;error&quot; mu &lt;- c(rmvn(1, rep(0, n), Sigma + 1e-10 * diag(n))) y &lt;- mu + rnorm(n, 0, sigma) dat &lt;- data.frame( x = x, y = y, mu = mu ) ggplot(data = dat, aes(x = x, y = mu)) + geom_line(color = &quot;red&quot;) + geom_point(aes(x = x, y = y)) 20.3.1 An example global basis: Fourier Basis make_fourier_basis &lt;- function(x, num_freq) { # if (!is.integer(num_freq)) { # stop(&quot;num_freq must be an odd integer&quot;) # } # if (num_freq %% 2 == 0) { # stop(&quot;num_freq must be an odd integer&quot;) # } X_fourier &lt;- cbind( 1, do.call( cbind, sapply( 1:((num_freq - 1) / 2), function(i) { cbind(sin(2 * i * pi * x), cos(2 * i * pi * x)) }, simplify = FALSE ) ) ) return(X_fourier) } X_fourier &lt;- make_fourier_basis(x, num_freq = 5) dat &lt;- data.frame( x = x, y = y, mu = mu, X_fourier = c(X_fourier), basis = factor(rep(1:ncol(X_fourier), each = n)) ) ggplot(data = dat, aes(x = x, y = y)) + # geom_point() + geom_line(aes(x = x, y = X_fourier, group = basis, color = basis)) + scale_color_viridis_d(option = &quot;magma&quot;) + theme_dark() ## fit a model using the Fourier basis fit &lt;- lm(y ~ X_fourier - 1) ## plot the basis with fitted coefficients dat$fitted_basis &lt;- c( sapply(1:ncol(X_fourier), function(i) X_fourier[, i] * fit$coefficients[i]) ) ggplot(data = dat, aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = fitted_basis, color = basis, group = basis)) + ggtitle(&quot;Weighted Fourier Basis functions&quot;) ## calculate the fitted function -- the sum of the bases dat$mu_hat &lt;- apply(sapply(1:ncol(X_fourier), function(i) X_fourier[, i] * fit$coefficients[i]), 1, sum) ggplot(data = dat, aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = mu_hat), color = &quot;red&quot;, lwd = 2) + geom_line(aes(x = x, y = mu), color = &quot;blue&quot;, lwd = 2) + ggtitle(&quot;Fitted function vs. simulated function&quot;) The Fourier basis is global ## only a small percentage of these values are zero mean(X_fourier == 0) ## [1] 4e-04 20.4 example local basis B-splines are piecewise polynomial functions degrees of freedom are the number of functions More degrees of freedom – more “wiggly” fit – potential for overfitting ## B-splines X_bs &lt;- bs(x, intercept = TRUE, df = 6) dat &lt;- data.frame( x = x, y = y, mu = mu, X_bs = c(X_bs), basis = factor(rep(1:ncol(X_bs), each = n)) ) ggplot(data = dat, aes(x = x, y = y)) + # geom_point() + geom_line(aes(x = x, y = X_bs, group = basis, color = basis)) + scale_color_viridis_d(option = &quot;magma&quot;) + theme_dark() ## fit a model using the B-spline basis fit &lt;- lm(y ~ X_bs - 1) ## plot the basis with fitted coefficients dat$fitted_basis &lt;- c( sapply(1:ncol(X_bs), function(i) X_bs[, i] * fit$coefficients[i]) ) ggplot(data = dat, aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = fitted_basis, color = basis, group = basis)) + ggtitle(&quot;Weighted B-spline Basis functions&quot;) ## calculate the fitted function -- the sum of the bases dat$mu_hat &lt;- apply(sapply(1:ncol(X_bs), function(i) X_bs[, i] * fit$coefficients[i]), 1, sum) ggplot(data = dat, aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = mu_hat), color = &quot;red&quot;, lwd = 2) + geom_line(aes(x = x, y = mu), color = &quot;blue&quot;, lwd = 2) + ggtitle(&quot;Fitted function vs. simulated function&quot;) The B-spline basis is local ## a much larger percentage of these values are zero -- can use sparse matrix ## routines to solve these equations much faster mean(X_bs == 0) ## [1] 0.3347 20.5 Fitting spatial models many ways 20.6 Empirical Orthogonal Functions Recall the Karhunen-Loève expansion Analog for PCA for spatio-temporal data Use the sea surface temperature (SST) data in Wikle, Zammit-Mangion, and Cressie (2019) data(&quot;SSTlandmask&quot;, package = &quot;STRbook&quot;) data(&quot;SSTlonlat&quot;, package = &quot;STRbook&quot;) data(&quot;SSTdata&quot;, package = &quot;STRbook&quot;) Delete the values of SST that are over land (e.g. SSTlandmask is 1) delete_rows &lt;- which(SSTlandmask == 1) SSTdata &lt;- SSTdata[-delete_rows, 1:396] The eigen decomposition of the sample covariance of the data is equivalent to the singular value decomposition of the scaled and detrended data \\(\\mathbf{y}\\). \\[\\begin{align*} \\tilde{\\mathbf{y}} &amp; \\equiv \\frac{1}{\\sqrt{T - 1}} \\left( \\right) \\end{align*}\\] To get a spatial EOF, we need to transform the data into space-wide format y &lt;- t(SSTdata) dim(y) ## [1] 396 2261 To use the equation above, we need to calculate the spatial mean and the number of spatial replicates and scale and detrend the data spatial_mean &lt;- apply(SSTdata, 1, mean) nT &lt;- ncol(SSTdata) y_tilde &lt;- 1 / sqrt(nT - 1) * (y - outer(rep(1, nT), spatial_mean)) We carry out the SVD on this scaled and detrended data svd_y &lt;- svd(y_tilde) which returns a list with three elements \\(\\mathbf{U}\\) \\(\\mathbf{D}\\) \\(\\mathbf{V}\\) \\[\\begin{align*} \\tilde{\\mathbf{y}} &amp; = \\mathbf{U} \\mathbf{D} \\mathbf{V} \\end{align*}\\] V &lt;- svd_y$v colnames(V) &lt;- paste0(&quot;EOF&quot;, 1:ncol(SSTdata)) EOFs &lt;- cbind(SSTlonlat[ - delete_rows, ], V) glimpse(EOFs[, 1:6, ]) ## Rows: 2,261 ## Columns: 6 ## $ lon &lt;dbl&gt; 154, 156, 158, 160, 162, 164, 166, 168, 170,… ## $ lat &lt;dbl&gt; -29, -29, -29, -29, -29, -29, -29, -29, -29,… ## $ EOF1 &lt;dbl&gt; -0.0049151, -0.0014123, 0.0002459, 0.0014550… ## $ EOF2 &lt;dbl&gt; -1.213e-02, -2.276e-03, 2.298e-03, 2.304e-03… ## $ EOF3 &lt;dbl&gt; -0.02882, -0.02553, -0.01933, -0.01906, -0.0… ## $ EOF4 &lt;dbl&gt; 8.541e-05, 6.726e-03, 8.591e-03, 1.026e-02, … Plot the EOFs ggplot(data = EOFs, aes(x = lon, y = lat, fill = EOF1)) + geom_raster() + scale_fill_viridis_c(option = &quot;magma&quot;) + coord_fixed(ratio = 1.3) + theme_bw() + xlab(&quot;Longitude (deg)&quot;) + ylab(&quot;Latitude (deg)&quot;) + ggtitle(&quot;First EOF&quot;) ggplot(data = EOFs, aes(x = lon, y = lat, fill = EOF2)) + geom_raster() + scale_fill_viridis_c(option = &quot;magma&quot;) + coord_fixed(ratio = 1.3) + theme_bw() + xlab(&quot;Longitude (deg)&quot;) + ylab(&quot;Latitude (deg)&quot;) + ggtitle(&quot;Second EOF&quot;) Let’s assume I have data for January 1970. dat &lt;- SST_df %&gt;% subset(Year == 1970 &amp; Month == &quot;Jan&quot;) EOFs$y &lt;- dat$sst[ - delete_rows] Model the data using the first few EOFs as basis functions mod &lt;- lm(y ~ EOF1, data = EOFs) summary(mod) plot(EOFs$y, predict(mod), main = &quot;Fitted vs. Predicted&quot;) abline(0, 1, col = &quot;red&quot;) ## ## Call: ## lm(formula = y ~ EOF1, data = EOFs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.0443 -0.2771 0.0635 0.3283 1.0750 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.2824 0.0111 25.5 &lt;2e-16 *** ## EOF1 -7.9894 0.5259 -15.2 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.455 on 2259 degrees of freedom ## Multiple R-squared: 0.0927, Adjusted R-squared: 0.0923 ## F-statistic: 231 on 1 and 2259 DF, p-value: &lt;2e-16 mod &lt;- lm(y ~ EOF1 + EOF2 + EOF3 + EOF4 + EOF5, data = EOFs) summary(mod) plot(EOFs$y, predict(mod), main = &quot;Fitted vs. Predicted&quot;) abline(0, 1, col = &quot;red&quot;) ## ## Call: ## lm(formula = y ~ EOF1 + EOF2 + EOF3 + EOF4 + EOF5, data = EOFs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.6775 -0.2141 0.0033 0.2330 0.8828 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2095 0.0217 -9.67 &lt;2e-16 *** ## EOF1 -19.7556 0.6232 -31.70 &lt;2e-16 *** ## EOF2 -12.3421 0.6165 -20.02 &lt;2e-16 *** ## EOF3 -11.8236 0.6720 -17.59 &lt;2e-16 *** ## EOF4 11.3313 0.3695 30.67 &lt;2e-16 *** ## EOF5 13.5727 0.4285 31.67 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.346 on 2255 degrees of freedom ## Multiple R-squared: 0.474, Adjusted R-squared: 0.473 ## F-statistic: 407 on 5 and 2255 DF, p-value: &lt;2e-16 mod &lt;- lm(y ~ EOF1 + EOF2 + EOF3 + EOF4 + EOF5 + EOF6 + EOF7 + EOF8 + EOF9 + EOF10, data = EOFs) summary(mod) plot(EOFs$y, predict(mod), main = &quot;Fitted vs. Predicted&quot;) abline(0, 1, col = &quot;red&quot;) ## ## Call: ## lm(formula = y ~ EOF1 + EOF2 + EOF3 + EOF4 + EOF5 + EOF6 + EOF7 + ## EOF8 + EOF9 + EOF10, data = EOFs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.505 -0.210 0.009 0.228 0.968 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.2330 0.0229 -10.16 &lt; 2e-16 *** ## EOF1 -20.3188 0.6398 -31.76 &lt; 2e-16 *** ## EOF2 -12.8966 0.6325 -20.39 &lt; 2e-16 *** ## EOF3 -12.4496 0.6930 -17.97 &lt; 2e-16 *** ## EOF4 11.4713 0.3565 32.18 &lt; 2e-16 *** ## EOF5 13.8471 0.4241 32.65 &lt; 2e-16 *** ## EOF6 -4.1098 0.3349 -12.27 &lt; 2e-16 *** ## EOF7 1.5233 0.3294 4.62 4.0e-06 *** ## EOF8 -2.5908 0.3538 -7.32 3.4e-13 *** ## EOF9 0.4330 0.3308 1.31 0.191 ## EOF10 0.5836 0.3365 1.73 0.083 . ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.329 on 2250 degrees of freedom ## Multiple R-squared: 0.525, Adjusted R-squared: 0.523 ## F-statistic: 249 on 10 and 2250 DF, p-value: &lt;2e-16 mod &lt;- lm(y ~ EOF1 + EOF2 + EOF3 + EOF4 + EOF5 + EOF6 + EOF7 + EOF8 + EOF9 + EOF10 + EOF11 + EOF12 + EOF13 + EOF14 + EOF15 + EOF16 + EOF17 + EOF18 + EOF19 + EOF20, data = EOFs) summary(mod) plot(EOFs$y, predict(mod), main = &quot;Fitted vs. Predicted&quot;) abline(0, 1, col = &quot;red&quot;) ## ## Call: ## lm(formula = y ~ EOF1 + EOF2 + EOF3 + EOF4 + EOF5 + EOF6 + EOF7 + ## EOF8 + EOF9 + EOF10 + EOF11 + EOF12 + EOF13 + EOF14 + EOF15 + ## EOF16 + EOF17 + EOF18 + EOF19 + EOF20, data = EOFs) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.9760 -0.1509 0.0125 0.1632 0.6419 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -0.1694 0.0228 -7.44 1.4e-13 *** ## EOF1 -18.7965 0.5955 -31.56 &lt; 2e-16 *** ## EOF2 -11.3979 0.5879 -19.39 &lt; 2e-16 *** ## EOF3 -10.7575 0.6516 -16.51 &lt; 2e-16 *** ## EOF4 11.0929 0.2766 40.10 &lt; 2e-16 *** ## EOF5 13.1056 0.3585 36.55 &lt; 2e-16 *** ## EOF6 -4.2775 0.2486 -17.21 &lt; 2e-16 *** ## EOF7 1.5256 0.2412 6.32 3.1e-10 *** ## EOF8 -2.2324 0.2732 -8.17 5.0e-16 *** ## EOF9 0.3487 0.2431 1.43 0.152 ## EOF10 0.3939 0.2506 1.57 0.116 ## EOF11 -0.4529 0.2419 -1.87 0.061 . ## EOF12 -1.6805 0.2417 -6.95 4.7e-12 *** ## EOF13 1.4899 0.2412 6.18 7.8e-10 *** ## EOF14 -1.2705 0.2442 -5.20 2.1e-07 *** ## EOF15 -1.9499 0.2422 -8.05 1.3e-15 *** ## EOF16 3.8331 0.2975 12.88 &lt; 2e-16 *** ## EOF17 -3.7016 0.2470 -14.99 &lt; 2e-16 *** ## EOF18 3.7071 0.2597 14.27 &lt; 2e-16 *** ## EOF19 -5.5411 0.2420 -22.90 &lt; 2e-16 *** ## EOF20 5.5816 0.2487 22.45 &lt; 2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.241 on 2240 degrees of freedom ## Multiple R-squared: 0.747, Adjusted R-squared: 0.744 ## F-statistic: 330 on 20 and 2240 DF, p-value: &lt;2e-16 Disadvantages Calculation of the SVD scales with \\(O(n^3)\\) Each of the EOFs are global (almost always nonzero) 20.7 Spatial modeling with mgcv package Use the same temperature data data(&quot;NOAA_df_1990&quot;) ## add a factor variable for left_join NOAA_df_1990$id_factor &lt;- factor(NOAA_df_1990$id) dat &lt;- NOAA_df_1990 %&gt;% subset(year == 1990 &amp; month == 7 &amp; proc == &quot;Tmax&quot;) %&gt;% group_by(id_factor) %&gt;% summarize(mean_Tmax = mean(z)) # ## add back in the lat/lon variables dat &lt;- NOAA_df_1990 %&gt;% subset(year == 1990 &amp; month == 7 &amp; proc == &quot;Tmax&quot; &amp; day == 1) %&gt;% left_join(dat, by = &quot;id_factor&quot;) fit &lt;- gam(z ~ lon + lat + s(lon, lat, bs = &quot;tp&quot;, k = 30), data = dat) # tensor product of a 2-d thin plate regression spline and 1-d cr spline ## explore model residuals par(mfrow = c(2, 2)) gam.check(fit) ## ## Method: GCV Optimizer: magic ## Smoothing parameter selection converged after 5 iterations. ## The RMS GCV score gradient at convergence was 0.0005041 . ## The Hessian was positive definite. ## Model rank = 30 / 32 ## ## Basis dimension (k) checking results. Low p-value (k-index&lt;1) may ## indicate that k is too low, especially if edf is close to k&#39;. ## ## k&#39; edf k-index p-value ## s(lon,lat) 29 24 0.97 0.3 ## generate model predictions pred_coords &lt;- as.data.frame( expand.grid( seq(min(dat$lon), max(dat$lon), length = 100), seq(min(dat$lat), max(dat$lat), length = 100) ) ) colnames(pred_coords) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) preds &lt;- predict(fit, newdata = pred_coords, se.fit = TRUE) pred_coords$pred_mean &lt;- preds$fit pred_coords$pred_var &lt;- preds$se.fit^2 ggplot(data = pred_coords, aes(x = lon, y = lat, fill = pred_mean)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) ggplot(data = pred_coords, aes(x = lon, y = lat, fill = pred_var)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature prediction variance in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) 20.8 Spatial modeling with Kernels Kernel functions can be either local or global Gaussian, exponential kernels are global Truncated Gaussian and truncated exponential are local Kernel functions require specifying a kernel shape, a “bandwith”, and the number of “knots”. Use the same temperature data with Gaussian Kernels data(&quot;NOAA_df_1990&quot;) ## add a factor variable for left_join NOAA_df_1990$id_factor &lt;- factor(NOAA_df_1990$id) dat &lt;- NOAA_df_1990 %&gt;% subset(year == 1990 &amp; month == 7 &amp; proc == &quot;Tmax&quot;) %&gt;% group_by(id_factor) %&gt;% summarize(mean_Tmax = mean(z)) # ## add back in the lat/lon variables dat &lt;- NOAA_df_1990 %&gt;% subset(year == 1990 &amp; month == 7 &amp; proc == &quot;Tmax&quot; &amp; day == 1) %&gt;% left_join(dat, by = &quot;id_factor&quot;) ## For simplicity, assume this is a square number n_knots &lt;- 25 knots &lt;- expand.grid( seq(min(dat$lon), max(dat$lon), length = sqrt(n_knots)), seq(min(dat$lat), max(dat$lat), length = sqrt(n_knots)) ) make_kernel_basis &lt;- function(coords, knots, kernel = &quot;gaussian&quot;, bandwith = 1, threshold = NULL) { if (!(kernel %in% c(&quot;gaussian&quot;, &quot;exponential&quot;))) { stop(&quot;only kernels available are gaussian and exponential&quot;) } D &lt;- fields::rdist(as.matrix(coords), as.matrix(knots)) X &lt;- matrix(0, nrow(D), ncol(D)) if (kernel == &quot;exponential&quot;) { X &lt;- exp (- D / bandwith) } else if (kernel == &quot;gaussian&quot;) { X &lt;- exp (- D^2 / bandwith) } ## add in a minimum distance threshold if (!is.null(threshold)) { X[D &gt; threshold] &lt;- 0 } return(X) } We can plot the kernels over the domain of interest using the code below. Note that it is useful to change the bandwith parameter so that the kernel covers multiple other knots coords &lt;- cbind(dat$lon, dat$lat) coords_plot &lt;- expand.grid( seq(min(dat$lon), max(dat$lon), length = 100), seq(min(dat$lat), max(dat$lat), length = 100) ) colnames(coords_plot) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) X_plot &lt;- make_kernel_basis(coords_plot, knots, kernel = &quot;gaussian&quot;, bandwith = 20) ## plot the kernel basis dat_plot &lt;- data.frame( x = coords_plot[, 1], y = coords_plot[, 2], X = c(X_plot), basis = factor(rep(1:ncol(X_plot), each = nrow(coords_plot))) ) dat_knots &lt;- data.frame( x = knots[, 1], y = knots[, 2] ) ggplot(data = dat_plot, aes(x = x, y = y, fill = X)) + geom_raster() + # geom(aes(x = x, y = X_fourier, group = basis, color = basis)) + scale_color_viridis_d(option = &quot;magma&quot;) + theme_bw() + facet_wrap( ~ basis, ncol = sqrt(n_knots)) + ggtitle(&quot;Kernel Basis&quot;) + geom_point(data = dat_knots, aes(x = x, y = y), color = &quot;red&quot;, size = 0.25, inherit.aes = FALSE) ## apply the kernel basis to the data bw &lt;- c(1, 10, 20, 30, 40, 50, 75, 100, 1000) check_AICc &lt;- matrix(0, 8, length(bw)) for (j in 1:8) { for (k in 1:length(bw)) { n_knots &lt;- j^2 knots &lt;- expand.grid( seq(min(dat$lon), max(dat$lon), length = sqrt(n_knots)), seq(min(dat$lat), max(dat$lat), length = sqrt(n_knots)) ) X &lt;- make_kernel_basis(coords, knots, kernel = &quot;gaussian&quot;, bandwith = bw[k]) colnames(X) &lt;- paste0(&quot;X&quot;, 1:ncol(X)) X &lt;- data.frame(X) fit &lt;- lm(dat$z ~ ., data = X) check_AICc[j, k] &lt;- AIC(fit) + (2 * (j^2 + 1)^2 + 2 * (j^2 + 1)) / (nrow(X) - j^2) } } matplot(check_AICc, type = &#39;p&#39;) best_idx &lt;- which(check_AICc == min(check_AICc), arr.ind = TRUE) ## the best fitting model has 5 knots and bandwidth 20 best_idx ## row col ## [1,] 5 3 n_knots &lt;- best_idx[1]^2 knots &lt;- expand.grid( seq(min(dat$lon), max(dat$lon), length = sqrt(n_knots)), seq(min(dat$lat), max(dat$lat), length = sqrt(n_knots)) ) X &lt;- make_kernel_basis(coords, knots, kernel = &quot;gaussian&quot;, bandwith = bw[best_idx[2]]) colnames(X) &lt;- paste0(&quot;X&quot;, 1:ncol(X)) X &lt;- data.frame(X) fit &lt;- lm(dat$z ~ ., data = X) ## generate model predictions pred_coords &lt;- as.data.frame( expand.grid( seq(min(dat$lon), max(dat$lon), length = 100), seq(min(dat$lat), max(dat$lat), length = 100) ) ) colnames(pred_coords) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) X_pred &lt;- make_kernel_basis(pred_coords, knots, kernel = &quot;gaussian&quot;, bandwith = bw[best_idx[2]]) colnames(X_pred) &lt;- paste0(&quot;X&quot;, 1:ncol(X)) X_pred &lt;- data.frame(X_pred) preds &lt;- predict(fit, newdata = X_pred, se.fit = TRUE) dat_pred &lt;- data.frame( lon = pred_coords$lon, lat = pred_coords$lat, ## prediction mean pred_mean = preds$fit, ## prediction variance pred_var = preds$se.fit^2 ) ggplot(data = dat_pred, aes(x = lon, y = lat, fill = pred_mean)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) ggplot(data = dat_pred, aes(x = lon, y = lat, fill = pred_var)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Average July max Temperature prediction variance in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) Notice that the above model is overfit – see the large variance surface. This can be resolved with penalized models (ridge, lasso, elastic net, etc). Many basis-function models show “edge effects” in the model fit. This is because at the edges of the domain there are one or two basis functions that see little data and therefore have high variability in the estimate. One can either add duplicate knots or extend the spatial domain of the knots and add a penalty term to the fit. 20.8.1 Kernel regression with a ridge penalty Note: glmnet doesn’t have a theoretically-motivated method for calculating prediction standard errors ## set alpha = 0 for ridge regression X &lt;- make_kernel_basis(coords, knots, kernel = &quot;gaussian&quot;, bandwith = bw[best_idx[2]]) colnames(X) &lt;- paste0(&quot;X&quot;, 1:ncol(X)) X &lt;- data.frame(X) cv_fit &lt;- cv.glmnet(as.matrix(X), dat$z, alpha = 0) fit &lt;- glmnet(as.matrix(X), dat$z, alpha = 0, lambda = cv_fit$lambda.min) ## generate model predictions pred_coords &lt;- as.data.frame( expand.grid( seq(min(dat$lon), max(dat$lon), length = 100), seq(min(dat$lat), max(dat$lat), length = 100) ) ) colnames(pred_coords) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) X_pred &lt;- make_kernel_basis(pred_coords, knots, kernel = &quot;gaussian&quot;, bandwith = bw[best_idx[2]]) colnames(X_pred) &lt;- paste0(&quot;X&quot;, 1:ncol(X)) X_pred &lt;- data.frame(X_pred) preds &lt;- predict(fit, newx = as.matrix(X_pred)) dat_pred &lt;- data.frame( lon = pred_coords$lon, lat = pred_coords$lat, pred_mean = c(preds) ) ggplot(data = dat_pred, aes(x = lon, y = lat, fill = pred_mean)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) 20.8.2 Kernel regression with a lasso penalty Note: glmnet doesn’t have a theoretically-motivated method for calculating prediction standard errors ## set alpha = 1 for ridge regression X &lt;- make_kernel_basis(coords, knots, kernel = &quot;gaussian&quot;, bandwith = bw[best_idx[2]]) colnames(X) &lt;- paste0(&quot;X&quot;, 1:ncol(X)) X &lt;- data.frame(X) cv_fit &lt;- cv.glmnet(as.matrix(X), dat$z, alpha = 1) fit &lt;- glmnet(as.matrix(X), dat$z, alpha = 1, lambda = cv_fit$lambda.min) ## generate model predictions pred_coords &lt;- as.data.frame( expand.grid( seq(min(dat$lon), max(dat$lon), length = 100), seq(min(dat$lat), max(dat$lat), length = 100) ) ) colnames(pred_coords) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) X_pred &lt;- make_kernel_basis(pred_coords, knots, kernel = &quot;gaussian&quot;, bandwith = bw[best_idx[2]]) colnames(X_pred) &lt;- paste0(&quot;X&quot;, 1:ncol(X)) X_pred &lt;- data.frame(X_pred) preds &lt;- predict(fit, newx = as.matrix(X_pred)) dat_pred &lt;- data.frame( lon = pred_coords$lon, lat = pred_coords$lat, pred_mean = c(preds) ) ggplot(data = dat_pred, aes(x = lon, y = lat, fill = pred_mean)) + geom_raster() + geom_polygon(data = states, aes(x = long, y = lat, group = group), colour = &quot;black&quot;, fill = NA) + scale_fill_viridis_c(option = &quot;plasma&quot;) + ggtitle(&quot;Predicted Average July max Temperature in 1990&quot;) + coord_fixed(xlim = range(dat$lon), ylim = range(dat$lat), ratio = 1.3) 20.8.3 Truncated Kernels - local support The kernels constructed above are globally supported (dense) mean(X_pred == 0) ## [1] 0 Can use traditional kernels and truncate using a threshold X_threshold &lt;- make_kernel_basis(pred_coords, knots, kernel = &quot;gaussian&quot;, bandwith = bw[best_idx[2]], threshold = 10) mean(X_threshold == 0) ## [1] 0.473 Can use other functions as a kernel: - Wendland basis \\[\\begin{align*} \\frac{(1 - d)^6 (35 d^2 + 18d + 3)}{3} I\\{d &lt; 1\\} \\end{align*}\\] wendland_basis &lt;- function(d, radius) { if (any(d &lt; 0)) { stop(&quot;d must be nonnegative&quot;) } d &lt;- d / radius return(((1 - d)^6 * (35 * d^2 + 18 * d + 3)) / 3 * (d &lt; 1)) } layout(matrix(1:4, 2, 2, byrow = TRUE)) curve(wendland_basis(abs(x - 0.25), radius = 0.15), n = 1000) curve(wendland_basis(abs(x - 0.75), radius = 0.15), n = 1000) curve(wendland_basis(abs(x - 0.25), radius = 0.5), n = 1000) curve(wendland_basis(abs(x - 0.75), radius = 0.5), n = 1000) 20.9 Sparse vs. Dense matrix computation uses the spam64 package compute time of \\(\\mathbf{X}&#39; \\mathbf{X}\\) for dense vs. sparse matrices coords &lt;- as.data.frame( expand.grid( seq(min(dat$lon), max(dat$lon), length = 50), seq(min(dat$lat), max(dat$lat), length = 50) ) ) colnames(coords) &lt;- c(&quot;lon&quot;, &quot;lat&quot;) n_knots &lt;- 25^2 knots &lt;- expand.grid( seq(min(dat$lon), max(dat$lon), length = sqrt(n_knots)), seq(min(dat$lat), max(dat$lat), length = sqrt(n_knots)) ) X_dense &lt;- make_kernel_basis(coords, knots, kernel = &quot;gaussian&quot;, bandwith = 5) X_sparse &lt;- make_kernel_basis(coords, knots, kernel = &quot;gaussian&quot;, bandwith = 5, threshold = 5) mean(X_dense == 0) mean(X_sparse == 0) ## convert to a sparse matrix format X_sparse &lt;- as.spam(X_sparse) ## calcuate t(X) %*% X for dense and sparse matrices bm &lt;- microbenchmark( crossprod(X_dense, X_dense), crossprod.spam(X_sparse, X_sparse), times = 10 ) bm plot(bm) ## [1] 0 ## [1] 0.7948 ## Unit: milliseconds ## expr min lq mean ## crossprod(X_dense, X_dense) 1029.7 1030.7 1034.5 ## crossprod.spam(X_sparse, X_sparse) 139.1 139.6 140.3 ## median uq max neval ## 1032.1 1034.8 1055.2 10 ## 140.5 140.7 141.2 10 almost a 10-fold speedup in computation time by using sparse matrix operations References "],
["day-21.html", "21 Day 21 21.1 Announcements", " 21 Day 21 21.1 Announcements "],
["day-22.html", "22 Day 22 22.1 Announcements", " 22 Day 22 22.1 Announcements "],
["day-23.html", "23 Day 23 23.1 Announcements", " 23 Day 23 23.1 Announcements "],
["day-24-scaling-gps-for-large-data.html", "24 Day 24 – Scaling GPs for large data 24.1 Announcements 24.2 Local analysis 24.3 Low rank approimxations 24.4 Spectral basis functions 24.5 Kernel convolutions", " 24 Day 24 – Scaling GPs for large data library(tidyverse) library(mvnfast) library(fields) library(spBayes) library(spNNGP) ## Loading required package: Formula ## Loading required package: RANN ## ## Attaching package: &#39;spNNGP&#39; ## The following object is masked from &#39;package:spBayes&#39;: ## ## spDiag library(LatticeKrig) 24.1 Announcements review EOF example from previous lecture spatial data can often be large – thousands to even millions of sites the likelihood has \\(n \\times n\\) covariance matrices which have to be evaluated – impossible to do directly Prediction isn’t too bad – can use local prediction using \\(k\\) nearest locations 24.2 Local analysis goal: make prediction at a location \\(\\mathbf{s}_0\\) only use data within a distance \\(d_0\\) to make predictions slide the window across the domain draw picture Advantages local inference results in many small matrices parallelization/GPUs nonstationary model by construction Disadvantages inefficient estimation – especially if process is stationary boundary effects – need to resolve predictions along the boundaries how to choose the distance \\(d_0\\) 24.3 Low rank approimxations In general, approximate the true process using \\(M &lt; &lt; n\\) basis functions \\[\\begin{align*} y(\\mathbf{s}) &amp; = \\mathbf{X}(\\mathbf{s}) \\boldsymbol{\\beta} + \\sum_{m=1}^M B_m(\\mathbf{s}) \\alpha_m + \\varepsilon(\\mathbf{s}) \\end{align*}\\] where \\(\\mathbf{x}(\\mathbf{s})\\) are covariates with fixed effect coefficients \\(\\boldsymbol{\\beta}\\), B_m() are basis functions evaluated at \\(\\mathbf{s}\\), \\(\\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_M)&#39; \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma}_\\alpha)\\) are random coefficients, and \\(\\varepsilon(\\mathbf{s}) \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) is the nugget (observation error). There are many choices of basis functions where as \\(M \\rightarrow \\infty\\) can approximate any stationary process 24.4 Spectral basis functions \\[\\begin{align*} y(\\mathbf{s}) &amp; = \\mathbf{X}(\\mathbf{s}) \\boldsymbol{\\beta} + \\sum_{m=1}^{M/2} sin(\\boldsymbol{\\omega}_m&#39; \\mathbf{s}) \\alpha_{1m} + \\sum_{m=1}^{M/2} cos(\\boldsymbol{\\omega}_m&#39; \\mathbf{s}) \\alpha_{2m} + \\varepsilon(\\mathbf{s}) \\end{align*}\\] where \\(\\alpha_{jm} \\stackrel{independent}{\\sim} N(0, \\tau^2 g(\\boldsymbol{\\omega}_m))\\). 24.5 Kernel convolutions \\[\\begin{align*} y(\\mathbf{s}) &amp; = \\mathbf{X}(\\mathbf{s}) \\boldsymbol{\\beta} + \\sum_{m=1}^M K(\\mathbf{s}, \\boldsymbol{\\kappa}_m) \\alpha_m + \\varepsilon(\\mathbf{s}) \\end{align*}\\] where \\(\\boldsymbol{\\alpha} = (\\alpha_1, \\ldots, \\alpha_M)&#39; \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma}_\\alpha)\\) – typically \\(\\boldsymbol{\\Sigma}_\\alpha = \\tau^2 \\mathbf{I}\\) draw picture Multi-resolution extensions \\[\\begin{align*} y(\\mathbf{s}) &amp; = \\mathbf{X}(\\mathbf{s}) \\boldsymbol{\\beta} + \\sum_{\\ell=1}^L \\sum_{m=1}^M K(\\mathbf{s}, \\boldsymbol{\\kappa}_{\\ell m}) \\alpha_{\\ell m} + \\varepsilon(\\mathbf{s}) \\end{align*}\\] Multiresolution Gaussian Process Model for the Analysis of Large Spatial Datasets ## Make some data set.seed(1) n &lt;- 1000 coords &lt;- cbind(runif(n, 0, 1), runif(n, 0, 1)) X &lt;- cbind(1, rnorm(n)) beta &lt;- as.matrix(c(1, 5)) sigma.sq &lt;- 5 tau.sq &lt;- 1 phi &lt;- 3 / 0.5 D &lt;- as.matrix(rdist(coords)) R &lt;- exp(-phi * D) w &lt;- c(rmvn(1, rep(0,n), sigma.sq * R)) y &lt;- rnorm(n, X %*% beta + w, sqrt(tau.sq)) ## x are the spatial locations, y is the response variable, Z are the fixed effects ## note that X has an intercept term that needs to be removed fit &lt;- LatticeKrig(x = coords, y = y, Z = X[, 2]) summary(fit) preds &lt;- predict(fit) plot(y, preds) abline(0, 1, col = &quot;red&quot;) ## $call ## LatticeKrig(x = coords, y = y, Z = X[, 2]) ## ## $inverseModel ## [1] FALSE ## ## $parameters ## ## 1 Number of Observations: 1.000e+03 ## 2 Number of parameters in the fixed component 4.000e+00 ## 3 Number of covariates 1.000e+00 ## 4 Effective degrees of freedom (EDF) 1.146e+02 ## 5 Standard Error of EDF estimate: 2.961e+00 ## 6 Smoothing parameter (lambda) 4.720e-03 ## 7 MLE sigma 1.352e+00 ## 8 MLE rho 3.874e+02 ## 9 Total number of basis functions 1.968e+03 ## 10 Multiresolution levels 3.000e+00 ## 11 log Profile Likelihood -1.881e+03 ## 12 log Likelihood (if applicable) NA ## 13 Nonzero entries in Ridge regression matrix 3.710e+05 ## ## $timingLKrig ## user.self sys.self elapsed ## timewX 1.657 0.004 1.662 ## timeQ 0.006 0.000 0.006 ## timeM 0.036 0.001 0.036 ## timeChol 0.038 0.000 0.038 ## timeCoef 0.012 0.000 0.013 ## timeLike 0.003 0.000 0.003 ## timeTrA 0.043 0.000 0.044 ## 1.795 0.005 1.802 ## ## $LKinfo ## Classes for this object are: LKinfo LKRectangle ## The second class usually will indicate the geometry ## e.g. 2-d rectangle is LKRectangle ## ## Some details on spatial autoregression flags: ## stationary: TRUE TRUE TRUE ## first order (by level): TRUE TRUE TRUE ## isotropic: TRUE TRUE TRUE ## ## Ranges of locations in raw scale: ## [,1] [,2] ## [1,] 0.001315 0.0006053 ## [2,] 0.999931 0.9988775 ## ## Logical (collapseFixedEffect) if fixed effects will be pooled: ## FALSE ## ## Number of levels: 3 ## delta scalings: 0.1664 0.08322 0.04161 ## with an overlap parameter of 2.5 ## alpha: 0.7619 0.1905 0.04762 ## based on smoothness nu = 1 ## ## a.wght: 4.01 4.01 4.01 ## ## Basis type: Radial using WendlandFunction and Euclidean ## distance. ## Basis functions will be normalized ## ## Total number of basis functions 1968 ## Level Basis size ## 1 272 17 16 ## 2 506 23 22 ## 3 1190 35 34 ## ## Lambda value: 0.00472 ## ## $MLE ## $MLE$summary ## EffDf lnProfLike GCV sigma.MLE ## 1.121e+02 -1.881e+03 2.058e+00 1.352e+00 ## rho.MLE lambda.MLE llambda.MLE lnLike ## 3.874e+02 4.720e-03 -5.356e+00 NA ## counts value grad ## 1.200e+01 NA ## ## $MLE$LKinfo ## Classes for this object are: LKinfo LKRectangle ## The second class usually will indicate the geometry ## e.g. 2-d rectangle is LKRectangle ## ## Some details on spatial autoregression flags: ## stationary: TRUE TRUE TRUE ## first order (by level): TRUE TRUE TRUE ## isotropic: TRUE TRUE TRUE ## ## Ranges of locations in raw scale: ## [,1] [,2] ## [1,] 0.001315 0.0006053 ## [2,] 0.999931 0.9988775 ## ## Logical (collapseFixedEffect) if fixed effects will be pooled: ## FALSE ## ## Number of levels: 3 ## delta scalings: 0.1664 0.08322 0.04161 ## with an overlap parameter of 2.5 ## alpha: 0.7619 0.1905 0.04762 ## based on smoothness nu = 1 ## ## a.wght: 4.01 4.01 4.01 ## ## Basis type: Radial using WendlandFunction and Euclidean ## distance. ## Basis functions will be normalized ## ## Total number of basis functions 1968 ## Level Basis size ## 1 272 17 16 ## 2 506 23 22 ## 3 1190 35 34 ## ## Lambda value: NA ## ## $MLE$llambda.start ## [1] -1 ## ## $MLE$lambda.MLE ## [1] 0.00472 ## ## $MLE$lnLike.eval ## lambda rho.MLE sigma.MLE lnProfileLike.FULL ## 0.367879 8.252 1.742 -2005 ## rowForCapture 0.017696 119.442 1.454 -1893 ## rowForCapture 0.380771 8.008 1.746 -2007 ## rowForCapture 0.002656 644.635 1.308 -1884 ## rowForCapture 0.004700 388.795 1.352 -1881 ## rowForCapture 0.004829 379.626 1.354 -1881 ## rowForCapture 0.004720 387.365 1.352 -1881 ## rowForCapture 0.004712 387.938 1.352 -1881 ## rowForCapture 0.004728 386.792 1.352 -1881 ## rowForCapture 0.004720 387.365 1.352 -1881 "],
["day-25-scaling-gps-for-large-data.html", "25 Day 25 – Scaling GPs for large data 25.1 Announcements 25.2 Gaussian Predictive Process (spBayes package) 25.3 Sparse matrix routines 25.4 The stochastic PDE approach 25.5 Approximate likelihood methods 25.6 Vecchia Approximations", " 25 Day 25 – Scaling GPs for large data 25.1 Announcements library(tidyverse) library(mvnfast) library(fields) library(spBayes) library(spNNGP) 25.2 Gaussian Predictive Process (spBayes package) projection idea the predictive (child) process has the same covariance function \\(C(\\cdot | \\boldsymbol{\\theta})\\) as the desired parent process optimal predictor of dimension \\(M &lt; &lt; n\\) given the covariance function \\(C(\\cdot | \\boldsymbol{\\theta})\\) Define a set of knots \\(\\boldsymbol{\\kappa} = (\\kappa_1, \\ldots, \\kappa_M)&#39;\\) The predictive process \\(\\boldsymbol{\\eta}^{\\star} \\sim N(\\mathbf{0}, \\mathbf{C}^{\\star})\\) where \\(\\mathbf{C}^{\\star}\\) is the covariance matrix of the process over the \\(M\\) knots given parameters \\(\\boldsymbol{\\theta}\\) The basis functions are the \\(M\\)-vector \\(\\mathbf{c}^{\\star}(\\mathbf{s} | \\boldsymbol{\\theta}) {\\mathbf{C}^{\\star}}^{-1}\\) where \\(\\mathbf{c}^{\\star}(\\mathbf{s} | \\boldsymbol{\\theta}) = \\left( Cov(\\mathbf{s}, \\mathbf{\\kappa}_1), \\ldots, Cov(\\mathbf{s}, \\mathbf{\\kappa}_M) \\right)\\) which is the cross-covariance between the observed locations \\(\\mathbf{s}\\) and the knot locations \\(\\boldsymbol{\\kappa}\\) \\[\\begin{align*} y(\\mathbf{s}) &amp; = \\mathbf{X}(\\mathbf{s}) \\boldsymbol{\\beta} + \\mathbf{c}^{\\star} (\\mathbf{s}) {\\mathbf{C}^{\\star}}^{-1} \\boldsymbol{\\eta}^{\\star} + \\varepsilon(\\mathbf{s}) \\end{align*}\\] Thus, the spatial process has covariance \\[\\begin{align*} Cov \\left( \\mathbf{c}^{\\star} {\\mathbf{C}^{\\star}}^{-1} \\boldsymbol{\\eta}^{\\star} \\right) &amp; = \\left( \\mathbf{c}^{\\star} {\\mathbf{C}^{\\star}}^{-1} \\right) Cov \\left( \\boldsymbol{\\eta}^{\\star} \\right) \\left( \\mathbf{c}^{\\star} {\\mathbf{C}^{\\star}}^{-1} \\right)&#39; \\\\ &amp; = \\mathbf{c}^{\\star} (\\mathbf{s}) {\\mathbf{C}^{\\star}}^{-1} \\mathbf{C}^{\\star} {\\mathbf{C}^{\\star}}^{-1} {\\mathbf{c}^{\\star}}&#39; \\\\ &amp; = \\mathbf{c}^{\\star} (\\mathbf{s}) {\\mathbf{C}^{\\star}}^{-1} {\\mathbf{c}^{\\star}}&#39; \\end{align*}\\] which is an \\(n \\times n\\) matrix that has rank \\(M\\) (not full rank) The Sherman-Morrison-Woodbury matrix inverse formula for invertible matrices \\(\\mathbf{A}\\) and \\(\\mathbf{D}\\) is \\[\\begin{align*} \\left( \\mathbf{A} + \\mathbf{U} \\mathbf{D} \\mathbf{V}&#39; \\right)^{-1} &amp; = \\mathbf{A}^{-1} - \\mathbf{A}^{-1} \\mathbf{U} \\left( \\mathbf{D}^{-1} + \\mathbf{V}&#39; \\mathbf{A}^{-1} \\mathbf{U} \\right)^{-1} \\mathbf{V}&#39; \\mathbf{A}^{-1} \\end{align*}\\] and the matrix determinant formula is \\[\\begin{align*} \\left| \\mathbf{A} + \\mathbf{U} \\mathbf{D} \\mathbf{V}&#39; \\right| &amp; = \\left| \\mathbf{D}^{-1} + \\mathbf{V}&#39; \\mathbf{A}^{-1} \\mathbf{U} \\right| \\left| \\mathbf{D} \\right| \\left| \\mathbf{A} \\right| \\end{align*}\\] thus, the model can be represented in the integrated representation \\[\\begin{align*} \\mathbf{y} &amp; \\sim N \\left( \\mathbf{X} \\boldsymbol{\\beta}, \\mathbf{c}^{\\star} (\\mathbf{s}) {\\mathbf{C}^{\\star}}^{-1} {\\mathbf{c}^{\\star}}&#39; + \\sigma^2 \\mathbf{I} \\right) \\end{align*}\\] the matrix inverse is \\[\\begin{align*} \\left( \\mathbf{c}^{\\star} (\\mathbf{s}) {\\mathbf{C}^{\\star}}^{-1} {\\mathbf{c}^{\\star}}&#39; + \\sigma^2 \\mathbf{I} \\right) &amp; = \\left( \\sigma^2 \\mathbf{I} \\right)^{-1} - \\left( \\sigma^2 \\mathbf{I} \\right)^{-1} {\\mathbf{c}^{\\star}} \\left(\\mathbf{C}^{\\star} + {\\mathbf{c}^{\\star}}&#39; \\left( \\sigma^2 \\mathbf{I} \\right)^{-1} {\\mathbf{c}^{\\star}} \\right)^{-1} {\\mathbf{c}^{\\star}}&#39; \\left( \\sigma^2 \\mathbf{I} \\right)^{-1} \\end{align*}\\] which requires inversion of a diagonal \\(n \\times n\\) matrix \\(\\sigma^2 \\mathbf{I}\\) which is trivial and the inversion of the \\(M \\times M\\) matrix \\(\\mathbf{C}^{\\star} + {\\mathbf{c}^{\\star}}&#39; \\left( \\sigma^2 \\mathbf{I} \\right)^{-1} {\\mathbf{c}^{\\star}}\\) where you have control over \\(M\\) and hence the computational complexity as this is \\(\\approx O(M^3) &lt;&lt; O(n^3)\\) advantages fast if \\(M\\) is small easy to embed in traditional models - mixed models, hierarchical models easy to extend to non-stationarity (predictive process is nonstationary) disadvantages fits are overly smooth (unless \\(M \\approx n\\) or the process itself is very smooth) variance estimates are too small modified predictive process ## Make some data set.seed(1) n &lt;- 1000 coords &lt;- cbind(runif(n, 0, 1), runif(n, 0, 1)) X &lt;- cbind(1, rnorm(n)) beta &lt;- as.matrix(c(1, 5)) sigma.sq &lt;- 5 tau.sq &lt;- 1 phi &lt;- 3 / 0.5 D &lt;- as.matrix(rdist(coords)) R &lt;- exp(-phi * D) w &lt;- c(rmvn(1, rep(0,n), sigma.sq * R)) y &lt;- rnorm(n, X %*% beta + w, sqrt(tau.sq)) ## Fit Predictive Process n.samples &lt;- 1000 batch.length &lt;- 50 n.batch &lt;- n.samples / batch.length starting &lt;- list(&quot;phi&quot; = 3 / 0.5, &quot;sigma.sq&quot; = 5, &quot;tau.sq&quot; = 1) tuning &lt;- list(&quot;phi&quot; = 0.5, &quot;sigma.sq&quot; = 0.5, &quot;tau.sq&quot; = 0.5) priors &lt;- list(&quot;phi.Unif&quot; = c(3/10, 3 / 0.001), &quot;sigma.sq.IG&quot; = c(2, 5), &quot;tau.sq.IG&quot; = c(2, 1) ) cov.model &lt;- &quot;exponential&quot; ## specify the number and location of knots n_knots &lt;- 8^2 ## this works best as a squre knots &lt;- as.matrix( expand.grid( seq(0, 1, length.out = sqrt(n_knots)), seq(0, 1, length.out = sqrt(n_knots)) ) ) if (file.exists(here::here(&quot;results&quot;, &quot;PP.RData&quot;))) { load(here::here(&quot;results&quot;, &quot;PP.RData&quot;)) } else { fit &lt;- spLM( y ~ X - 1, coords = coords, knots = knots, starting = starting, tuning = tuning, priors = priors, cov.model = cov.model, n.samples = n.samples, amcmc = list( n.batch = n.batch, batch.length = batch.length, accept.rate = 0.44 ), n.report = 10 ## report every 10 adaptive batches ) save(fit, file = here::here(&quot;results&quot;, &quot;PP.RData&quot;)) } To get the fitted parameters, we can recover these with composition sampling. ## discard the first half of the samples as burn-in burn.in &lt;- 0.5 * n.samples ## trace plots for the exponential model recovered_params &lt;- spRecover(fit, start = burn.in, thin = 2, verbose = FALSE) theta_post &lt;- mcmc.list(recovered_params$p.theta.samples) plot(theta_post, density = FALSE) round(summary(theta_post)$quantiles, 3) beta_post &lt;- mcmc.list(recovered_params$p.beta.recover.samples) plot(beta_post, density = FALSE) round(summary(beta_post)$quantiles, 3) ## 2.5% 25% 50% 75% 97.5% ## sigma.sq 4.367 6.283 7.073 8.686 13.703 ## tau.sq 0.607 0.832 0.943 1.033 1.269 ## phi 1.495 2.592 3.344 3.766 5.114 ## 2.5% 25% 50% 75% 97.5% ## X1 -3.315 -0.865 -0.078 0.98 3.056 ## X2 4.882 4.929 4.960 4.99 5.048 ## recover the spatial random effects estimated_w &lt;- recovered_params$p.w.recover.samples plot(apply(estimated_w, 1, mean), w) abline(0, 1, col = &quot;red&quot;) 25.3 Sparse matrix routines If most of the elements are 0, sparse matrix routines can speed up computation Most analytic covariance functions are not sparse However, most covariance functions are \\(\\approx\\) 0 at large spatial distances truncate these values to be exactly 0 introduce a tapering function \\(C_t(d | \\gamma) = (1 - \\frac{d}{\\gamma})(4 \\frac{d}{\\gamma} + 1) I\\{d &lt; \\gamma\\}\\) then, the tapered covariance is \\[\\begin{align*} C(d) = C_t(d | \\gamma) C(d | \\boldsymbol{\\theta}) \\end{align*}\\] where \\(C(d | \\boldsymbol{\\theta})\\) is a Matern covariance function 25.4 The stochastic PDE approach The Matern covariance was originally derived using SPDEs Lindgren et. al. (2011) showed the computational benefits of solving the SPDE Defintion: the Laplace operator is \\[\\begin{align*} {}\\!\\mathbin\\bigtriangleup y(\\mathbf{s}) &amp; = \\frac{{ \\partial^2 y(\\mathbf{s})} }{ { \\partial s_1}^2 } + \\frac{{ \\partial^2 y(\\mathbf{s})} }{ { \\partial s_2}^2 } \\end{align*}\\] for a positive integer \\(n\\), \\({}\\!\\mathbin\\bigtriangleup^n y(\\mathbf{s}) = {}\\!\\mathbin\\bigtriangleup\\stackrel{\\times n}{\\cdots} {}\\!\\mathbin\\bigtriangleup y(\\mathbf{s})\\) Thus, a GP with mean \\(\\mathbf{0}\\), variance \\(\\tau^2\\) and Matern covariance with range parameter \\(\\phi\\) and smoothness parameter \\(\\nu\\) can be shown to solve the SPDE \\[\\begin{align*} (\\phi^2 - {}\\!\\mathbin\\bigtriangleup)^{\\frac{\\nu + 1}{2}} \\sqrt{\\frac{\\phi^{2 \\nu} \\Gamma(\\nu)}{4 \\pi \\tau^2 \\Gamma(\\nu + 1)}} y(\\mathbf{s}) &amp; = z(\\mathbf{s}) \\end{align*}\\] where \\(z \\sim\\) WNGP The Laplacian is a decorrelating operator The Matern smoothness parameter determines the number of derivatives until you get to white noise Consider the spatial case where \\(\\nu = 1\\) \\[\\begin{align*} \\sqrt{\\frac{\\phi^{2 \\nu} \\Gamma(\\nu)}{4 \\pi \\tau^2 \\Gamma(\\nu + 1)}} (\\frac{1}{\\phi^2} - {}\\!\\mathbin\\bigtriangleup) y(\\mathbf{s}) &amp; = z (\\mathbf{s}) \\\\ \\Rightarrow \\\\ \\frac{y(\\mathbf{s})}{\\phi^2} - \\frac{{ \\partial^2 y(\\mathbf{s})} }{ { \\partial s_1}^2 } - \\frac{{ \\partial^2 y(\\mathbf{s})} }{ { \\partial s_2}^2 } &amp; = \\frac{1}{\\sqrt{\\frac{\\phi^{2 \\nu} \\Gamma(\\nu)}{4 \\pi \\tau^2 \\Gamma(\\nu + 1)}}} z(\\mathbf{s}) \\end{align*}\\] represent the process as a SPDE gives a sparse precision matrix (the precision matrix is the inverse of the covariance) If the data are on a regular gird, the SPDE approximation to a Matern covariance with smoothness parameter \\(\\nu = 1\\) is derived as Say \\(\\mathcal{D} = \\{ , \\ldots, -2, -1, 0, 1, 2, \\ldots \\}^2\\) is a regular rectangular grid. Then, we can approximate the partial derivatives as \\[\\begin{align*} \\frac{ {\\partial^2 y(s_1, s_2)}}{ {\\partial s_1}^2 } &amp; = y(s_1 + 1, s_2) - 2 y(s_1, s_2) + y(s_1 - 1,s_2) \\end{align*}\\] and \\[\\begin{align*} \\frac{ {\\partial^2 y(s_1, s_2)}}{ {\\partial s_2}^2 } &amp; = y(s_1, s_2 + 1) - 2 y(s_1, s_2) + y(s_1, s_2 - 1). \\end{align*}\\] Then, the SPDE is approximated by \\[\\begin{align*} \\left( 4 + \\frac{1}{\\phi^2} \\right) y(\\mathbf{s}_1, \\mathbf{s}_2) - 4 \\bar{y}(\\mathbf{s}_1, \\mathbf{s}_2) = \\frac{1}{\\sqrt{\\frac{\\phi^{2 \\nu} \\Gamma(\\nu)}{4 \\pi \\tau^2 \\Gamma(\\nu + 1)}}} z(\\mathbf{s}) \\end{align*}\\] where \\(\\bar{y}(\\mathbf{s}_1, \\mathbf{s}_2)\\) is the mean of the 4 neighbors of the location (s_1, s_2). In matrix notation, this is \\[\\begin{align*} \\left( 4 + \\frac{1}{\\phi^2} \\right) \\mathbf{y}- \\mathbf{A} \\mathbf{y} = \\frac{1}{\\sqrt{\\frac{\\phi^{2 \\nu} \\Gamma(\\nu)}{4 \\pi \\tau^2 \\Gamma(\\nu + 1)}}} \\mathbf{z} \\end{align*}\\] so that \\[\\begin{align*} \\mathbf{y} &amp; \\sim N(\\mathbf{0}, \\boldsymbol{\\Sigma}) \\end{align*}\\] where \\(\\boldsymbol{\\Sigma}^{-1} = \\frac{1}{\\tau^2} \\mathbf{Q}\\mathbf{Q}&#39;\\) with \\(\\tau^2 = \\frac{1}{\\left( \\frac{\\phi^{2 \\nu} \\Gamma(\\nu)}{4 \\pi \\tau^2 \\Gamma(\\nu + 1)} \\right)}\\) (notice that this is another reason why the estimates of \\(\\phi\\) is spatial models is inconsistent) where \\[\\begin{align*} \\mathbf{Q} &amp; = (4 + \\frac{1}{\\phi^2})\\mathbf{I} - \\mathbf{A} \\end{align*}\\] where \\(\\mathbf{A}\\) is an adjacency matrix with \\(ij\\)th element \\(a_{ij} = 1\\) if locations \\(i\\) and \\(j\\) are neighbors and 0 otherwise because this model is sparse in the precision (and the precision is needed to evaluate the likelihood) this representation are fast What if the data aren’t on a regular grid? construct a triangular mesh and use linear interpolation along the grid 25.5 Approximate likelihood methods twice the negative log likelihood is \\[\\begin{align*} - 2 log [\\mathbf{y} | \\boldsymbol{\\beta}, \\boldsymbol{\\theta}] &amp; = log |\\boldsymbol{\\Sigma}(\\boldsymbol{\\theta}) | + \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right) \\boldsymbol{\\Sigma}(\\boldsymbol{\\theta})^{-1} \\left( \\mathbf{y} - \\mathbf{X} \\boldsymbol{\\beta} \\right)&#39; \\end{align*}\\] Many approximations have been proposed Parameters that maximize the approximate likelihood are approximates for the ful likelihood MLE examples: pairwise likelihood: define \\(\\boldsymbol{\\delta}_{ij} = \\begin{pmatrix} y_i - X_i \\boldsymbol{\\beta} \\\\ y_j - X_j \\boldsymbol{\\beta} \\end{pmatrix}\\) \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\beta}, \\boldsymbol{\\theta}] \\approx \\prod_{i &lt; j} | \\boldsymbol{\\Sigma}_{ij} | \\exp\\{ - \\frac{1}{2} \\boldsymbol{\\delta}_{ij}&#39; \\boldsymbol{\\Sigma}_{ij}^{-1} \\boldsymbol{\\delta}_{ij} \\} \\\\ &amp; = \\prod_{i &lt; j} [y_i, j_y | \\boldsymbol{\\beta}, \\boldsymbol{\\theta}] \\end{align*}\\] independent blocks \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\beta}, \\boldsymbol{\\theta}] \\approx \\prod_{b = 1}^B [\\mathbf{y}_b | \\boldsymbol{\\beta}, \\boldsymbol{\\theta}] \\end{align*}\\] where \\(\\mathbf{y}_b\\) are the data in block \\(b = 1, \\ldots, B\\) draw picture each of these are unbiased estimating equations and standard errors can be derived using a sandwich formula 25.6 Vecchia Approximations Any joint PDF can be written as: \\[\\begin{align*} [y_1, \\ldots, y_n] &amp; = [y_1] [y_2 | y_1] [y_3 | y1, y2] \\cdots [y_n | y_{n-1}, \\ldots, y_1] \\\\ &amp; = \\prod_{i=1}^n [y_i | y_j \\hspace{0.25cm} \\forall j &lt; i] \\end{align*}\\] Note: The converse is not true! A set of conditional PDFs when multiplied together does not always guarantee a valid joint probability distribution - a theorem called the Hammersly Clifford Theorem defines under what conditions a joint distribution defines a valid joint PDF. The Vecchia approximation decomposes an \\(n\\)-dimensional PDF into \\(n\\) one-dimensional PDFs However, the last term requires an \\(n-1 \\times n-1\\) matrix inverse must be calculated (in this formulation) Outlines a formula for more useful approximations: Pick a conditioning sequence of sets that defines a Markov Chain (independent samples, nearest neighbor groupings, etc.) How to pick the conditioning set? order of conditioning matters, but is not super important how to compute standard errors: sandwhich estimator Used in nearest neighbor Gaussian Processes ## Make some data set.seed(1) n &lt;- 1000 coords &lt;- cbind(runif(n,0,1), runif(n,0,1)) x &lt;- cbind(1, rnorm(n)) B &lt;- as.matrix(c(1,5)) sigma.sq &lt;- 5 tau.sq &lt;- 1 phi &lt;- 3/0.5 D &lt;- as.matrix(rdist(coords)) R &lt;- exp(-phi * D) w &lt;- c(rmvn(1, rep(0,n), sigma.sq*R)) y &lt;- rnorm(n, x%*%B + w, sqrt(tau.sq)) ##Fit a Response and Latent NNGP model n.samples &lt;- 500 starting &lt;- list(&quot;phi&quot;=phi, &quot;sigma.sq&quot;=5, &quot;tau.sq&quot;=1) tuning &lt;- list(&quot;phi&quot;=0.5, &quot;sigma.sq&quot;=0.5, &quot;tau.sq&quot;=0.5) priors &lt;- list(&quot;phi.Unif&quot;=c(3/1, 3/0.01), &quot;sigma.sq.IG&quot;=c(2, 5), &quot;tau.sq.IG&quot;=c(2, 1)) cov.model &lt;- &quot;exponential&quot; if (file.exists(here::here(&quot;results&quot;, &quot;NNGP.RData&quot;))) { load(here::here(&quot;results&quot;, &quot;NNGP.RData&quot;)) } else { fit &lt;- spNNGP(y ~ x - 1, coords = coords, starting = starting, method = &quot;latent&quot;, # method = &quot;response&quot;, ## use for large n, doesn&#39;t save spatial random effects n.neighbors = 10, tuning = tuning, priors = priors, cov.model = cov.model, n.samples = n.samples, n.omp.threads = 1 ) save(fit, file = here::here(&quot;results&quot;, &quot;NNGP.RData&quot;)) } summary(fit) plot(apply(fit$p.w.samples, 1, median), w) abline(a = 0, b = 1, col = &quot;red&quot;) ## ## Call: ## spNNGP(formula = y ~ x - 1, coords = coords, method = &quot;latent&quot;, ## n.neighbors = 10, starting = starting, tuning = tuning, ## priors = priors, cov.model = cov.model, n.samples = n.samples, ## n.omp.threads = 1) ## ## Model class is NNGP, method latent, family gaussian. ## ## Model object contains 500 MCMC samples. ## ## Chain sub.sample: ## start = 250 ## end = 500 ## thin = 1 ## samples size = 251 ## 2.5% 25% 50% 75% 97.5% ## x1 0.134 0.270 0.360 0.478 0.627 ## x2 4.897 4.955 4.979 5.003 5.049 ## sigma.sq 4.327 5.044 5.554 6.275 7.159 ## tau.sq 0.947 1.076 1.149 1.205 1.335 ## phi 3.952 4.360 4.757 5.223 7.611 Used in mutliresolution predictive process approximation Advantages: very good approximation GPUs available Disadvantages: hard for non-Gaussian data "],
["day-26.html", "26 Day 26 26.1 Announcements 26.2 Multivariate Data 26.3 Classical Multivariate Geostatistics", " 26 Day 26 library(tidyverse) library(fields) library(mvnfast) library(LaplacesDemon) library(patchwork) library(GGally) 26.1 Announcements 26.2 Multivariate Data At each location \\(\\mathbf{s} \\in \\mathcal{D}\\) there are \\(p\\) variables \\(y_1(\\mathbf{s}), \\ldots, y_p(\\mathbf{s})\\) measured Examples: temperature and ozone precipitation and humidity Advantages of a joint model – why not just model each variable independently? Learn about any potential dependence between the variables Use cross-correlation to “borrow strength” among the variables to improve prediction and estimation Sometimes the processes are only partially observed (i.e., if there are two varaibles of interest, you might only observe one variable at a handful of sites, the other variable at other sites, and both variables at a subset to sites) draw picture can even fit a model if none of the variables are co-located if you make assumptions about the spatial co-dependence 26.3 Classical Multivariate Geostatistics 26.3.1 cross-variograms and cross-covariance functions The cross-variograms between variables \\(i\\) and \\(j\\) at lag \\(\\mathbf{h}\\) is \\[\\begin{align*} \\gamma_{ij}(\\mathbf{h}) = \\frac{1}{2} E\\left( \\left( y_i(\\mathbf{s} + \\mathbf{h}) - y_i(\\mathbf{s}) \\right) \\left( y_j(\\mathbf{s} + \\mathbf{h}) - y_j(\\mathbf{s}) \\right) \\right) \\end{align*}\\] where we assume that \\(E\\left( y_i(\\mathbf{s} + \\mathbf{h}) - y_i(\\mathbf{s}) \\right) = 0\\) for all \\(\\mathbf{s}\\) and \\(\\mathbf{s} + \\mathbf{h} \\in \\mathcal{D}\\) The cross-covariance function is \\[\\begin{align*} C_{ij}(\\mathbf{h}) = E\\left( \\left( y_i(\\mathbf{s} + \\mathbf{h}) - \\mu_i \\right) \\left( y_j(\\mathbf{s} + \\mathbf{h}) - \\mu_j \\right) \\right) \\end{align*}\\] where a constant mean \\(\\mu_i\\) is assumed. Note: cross-covariace function satisfies \\(| C_{ij}(\\mathbf{h}) |^2 \\leq C_{ii}(\\mathbf{0})C_{jj}(\\mathbf{0})\\) but \\(| C_{ij}(\\mathbf{h}) |\\) need not be \\(\\leq C_{ij}(\\mathbf{0})\\) 26.3.2 Cokriging use the multivariate vector for prediction by “borrowing strength” assume valid mean and cross-covariance functions, then cokriging can be done using multivariate normal distributions Let \\(\\mathbf{y} = \\left(y_1(\\mathbf{s_1}), \\ldots, y_1(\\mathbf{s_n}), y_2(\\mathbf{s_1}), \\ldots, y_2(\\mathbf{s_n}), \\ldots, y_p(\\mathbf{s_1}), \\ldots, y_p(\\mathbf{s_n}) \\right)&#39;\\) \\[\\begin{align*} \\mathbf{y} &amp; \\sim N(\\mathbf{X} \\boldsymbol{\\beta}, \\boldsymbol{\\Sigma}) \\end{align*}\\] in general, it is hard to choose a valid cross-covariance function so the matrix \\(\\boldsymbol{\\Sigma}\\) is hard to specify Instead, one can use a “seperable” covariance function \\[\\begin{align*} C_{ij}(\\mathbf{h}) &amp; = \\sigma_{ij} C(\\mathbf{h}) \\end{align*}\\] where \\(\\sigma_{ij}\\) is a cross-correlation between variable \\(i\\) and \\(j\\) that is independent of spatial location and \\(C(\\mathbf{h})\\) is a purely spatial covariance function This implies: Variance \\(Var(y_i(\\mathbf{s})) = \\sigma_{ii}\\) Cross-covariance \\(Cov(y_i(\\mathbf{s}), y_j(\\mathbf{s})) = \\sigma_{ij}\\) Spatial covariance \\(Cov(y_i(\\mathbf{s}), y_i(\\mathbf{s}&#39;)) = \\sigma_{ii} C(\\mathbf{s} - \\mathbf{s}&#39;)\\) Cross spatial covariance \\(Cov(y_i(\\mathbf{s}), y_j(\\mathbf{s}&#39;)) = \\sigma_{ij} C(\\mathbf{s} - \\mathbf{s}&#39;)\\) Hence, seperability implies the cross-covariance between responses is the same from site to site (the interrelationships between the variables are constant across space) and the spatial correlation is the same for each of the variable types Then, \\(\\boldsymbol{\\Sigma} = \\boldsymbol{\\Sigma}_p \\otimes \\mathbf{R}_n\\) where \\(\\boldsymbol{\\Sigma}_p\\) is the \\(p \\times p\\) covariance matrix among the \\(p\\) variables and \\(\\mathbf{R}_n\\) is the \\(n \\times n\\) spatial correlation matrix. ## Make some data set.seed(1) n &lt;- 20^2 p &lt;- 4 coords &lt;- expand.grid( seq(0, 1, length.out = sqrt(n)), seq(0, 1, length.out = sqrt(n)) ) X &lt;- cbind(1, rnorm(n, 0, 0.5)) beta &lt;- list(p) for (j in 1:p) { beta[[j]] &lt;- as.matrix(c(1, rnorm(1, 0, 0.5))) } Xbeta &lt;- rep(0, n * p) for (j in 1:p) { Xbeta[1:n + (j-1)*n] &lt;- X %*% beta[[j]] } sigma2 &lt;- 1 phi &lt;- 3 / 0.5 D &lt;- as.matrix(rdist(coords)) R_n &lt;- exp(-phi * D) ## simulate a 4 by 4 covariance matrix for the nu &lt;- p+1 Sigma_p &lt;- nu * rinvwishart(nu, diag(p)) ggcorr(data = NULL, cor_matrix = cov2cor(Sigma_p), label = TRUE) + scale_fill_viridis_c() + ggtitle(&quot;Correlation among processes&quot;) Note: we need to evaluate the Cholesky of \\(\\boldsymbol{\\Sigma}_p \\otimes \\mathbf{R}_n\\) for both simulation and estimation of the process. However, this is enabled computationally by noticing that \\[\\begin{align*} \\boldsymbol{\\Sigma} &amp; = \\boldsymbol{\\Sigma}_p \\otimes \\mathbf{R}_n \\\\ &amp; = \\mathbf{L} \\mathbf{L}&#39; \\\\ \\end{align*}\\] is the Cholesky decomposition which is \\(O(n^3 p^3)\\). If we take the Cholesky decomposition of the individual component matrices \\(\\boldsymbol{\\Sigma}_p = \\mathbf{L}_p \\mathbf{L}_p&#39;\\) and \\(\\mathbf{R}_n = \\mathbf{L}_n \\mathbf{L}_p &#39;\\) which are \\(O(p^3)\\) and \\(O(n^3)\\), respectively. Then, we can use the fact that Kronecker product of the Cholesky decomposition of the matrices is the Cholesky of the Kronecker product \\[\\begin{align*} \\mathbf{L}_p \\otimes \\mathbf{L}_n &amp; = \\mathbf{L} \\end{align*}\\] to greatly reduce computation time. This is commonly called “tensor products” or “tensor multiplication” – Google’s Tensorflow all.equal( kronecker(chol(Sigma_p), chol(R_n)), chol(kronecker(Sigma_p, R_n)) ) ## calcuate the timings of the different operations bm &lt;- microbenchmark( kronecker(chol(Sigma_p), chol(R_n)), chol(kronecker(Sigma_p, R_n)), times = 10 ) bm plot(bm) ## [1] TRUE ## Unit: milliseconds ## expr min lq mean ## kronecker(chol(Sigma_p), chol(R_n)) 27.62 40.36 45.43 ## chol(kronecker(Sigma_p, R_n)) 712.43 736.55 770.63 ## median uq max neval ## 46.15 50.0 63.66 10 ## 755.13 799.5 861.41 10 ## Note: this is slow # w &lt;- c(mvnfast::rmvn(1, rep(0, n*p), kronecker(Sigma_p, R_n))) ## instead use the decomposed Cholesky represntation w &lt;- c(mvnfast::rmvn(1, rep(0, n*p), kronecker(chol(Sigma_p), chol(R_n)), isChol = TRUE)) y &lt;- rnorm(n * p, Xbeta + w, sqrt(sigma2)) dat &lt;- data.frame( lon = coords[, 1], lat = coords[, 2], mu = Xbeta, w = w, y = y, var = rep(1:p, each = n) ) p1 &lt;- ggplot(data = dat, aes(x = lon, y = lat, fill = w)) + geom_raster() + facet_grid(~ var) + ggtitle(&quot;spatial random effects&quot;) p2 &lt;- ggplot(data = dat, aes(x = lon, y = lat, fill = mu)) + geom_raster() + facet_grid(~ var) + ggtitle(&quot;fixed effects&quot;) p3 &lt;- ggplot(data = dat, aes(x = lon, y = lat, fill = y)) + geom_raster() + facet_grid(~ var) + ggtitle(&quot;observed data&quot;) ## plot using patchwork package p1 / p2 / p3 26.3.3 Linear Model of Coregionalization essentially a spatial version of factor analysis the responses are a \\(p\\)-vector \\((y_1(\\mathbf{s}), \\ldots, y_p(\\mathbf{s}))&#39;\\) at each location (maybe only partially observed) Idea: instead of there being \\(p\\) different processes, these can be represented by \\(L &lt; p\\) latent (unobserved) sources \\(f_1(\\mathbf{s}), \\ldots, f_L(\\mathbf{s})\\) very common in pollution / air quality monitoring each of the \\(f_\\ell(\\mathbf{s})\\) can be thought of as different sources of pollutions (e.g., cars, power plants, manufacturing plants, etc.) The factor analysis model (linear model of coregionalization) is \\[\\begin{align*} y_i(\\mathbf{s}) &amp; = \\sum_{\\ell = 1}^L A_{i \\ell} f_{\\ell}(\\mathbf{s}) + \\varepsilon{s} \\end{align*}\\] where \\(\\varepsilon(\\mathbf{s}) \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) The joint model is \\[\\begin{align*} \\mathbf{y}(\\mathbf{s}) = \\begin{pmatrix} y_1(\\mathbf{s}) \\\\ \\vdots \\\\ y_p(\\mathbf{s}) \\end{pmatrix} &amp; \\sim N(\\mathbf{A} \\mathbf{f}(\\mathbf{s}), \\mathbf{D}) , \\end{align*}\\] where \\(\\mathbf{A} = \\begin{pmatrix} \\mathcal{R}^+ &amp; 0 &amp; \\cdots &amp; 0\\\\ \\mathcal{R} &amp; \\mathcal{R}^+ &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\mathcal{R} &amp; \\cdots &amp; \\mathcal{R} &amp; \\mathcal{R}^+ \\\\ \\vdots &amp; \\cdots &amp; \\vdots &amp; \\vdots \\\\ \\mathcal{R} &amp; \\cdots &amp; \\mathcal{R} &amp; \\mathcal{R} \\end{pmatrix}\\) is a \\(p \\times L\\) lower triangular matrix with positive diagonal elements (\\(\\mathcal{R}^+\\)) and unconstrained values in the lower triangle (\\(\\mathcal{R}\\)). These requirements are to ensure the model is identifiable. The \\(\\mathbf{f}(\\mathbf{s}) = (f_1(\\mathbf{s}), \\ldots, f_L(\\mathbf{s}))&#39;\\) is a \\(L\\)-vector of spatially correlated latent factors, and \\(\\mathbf{D} = \\begin{pmatrix} \\sigma^2_{11} &amp; 0 &amp; \\cdots &amp; 0\\\\ 0 &amp; \\sigma^2_{22} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^2_{pp} \\end{pmatrix}\\) is a diagonal matrix with positive diagonals representing the nugget variances for each of the \\(p\\) variables Seperable model: If the latent processes \\(f_\\ell\\) are \\(iid\\) GPs with mean zero, variance 1 and correlation \\(C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta})\\), then the covariance is \\[\\begin{align*} Cov \\left( y(\\mathbf{s}), y(\\mathbf{s}&#39;) \\right) &amp; = Cov \\left( \\mathbf{A} \\mathbf{f}(\\mathbf{s}), \\mathbf{A} \\mathbf{f}(\\mathbf{s}&#39;) \\right) \\\\ &amp; = \\mathbf{A} Cov \\left( \\mathbf{f}(\\mathbf{s}), \\mathbf{f}(\\mathbf{s}&#39;)\\right) \\mathbf{A}&#39;\\\\ &amp; = \\mathbf{A} \\begin{pmatrix} Cov(f_1(\\mathbf{s}), f_1(\\mathbf{s}&#39;)) &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; Cov(f_2(\\mathbf{s}), f_2(\\mathbf{s}&#39;)) &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; Cov(f_L(\\mathbf{s}), f_L(\\mathbf{s}&#39;)) \\end{pmatrix} \\mathbf{A}&#39; \\\\ &amp; = \\mathbf{A} \\begin{pmatrix} Cov(C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta}) &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta}) &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta})) \\end{pmatrix} \\mathbf{A}&#39; \\\\ &amp; \\mathbf{A} \\left( C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta}) \\mathbf{I} \\right) \\mathbf{A}&#39; , &amp; \\mathbf{A} \\mathbf{A}&#39; C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta}), \\end{align*}\\] where \\(\\mathbf{A} \\mathbf{A}&#39;\\) is the cross-covariance and \\(C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\|)\\) is the spatial correlation function. ## Make some data set.seed(1) n &lt;- 30^2 p &lt;- 4 L &lt;- 3 ## three latent factors coords &lt;- expand.grid( seq(0, 1, length.out = sqrt(n)), seq(0, 1, length.out = sqrt(n)) ) X &lt;- cbind(1, rnorm(n, 0, 0.5)) beta &lt;- list(p) for (j in 1:p) { beta[[j]] &lt;- as.matrix(c(1, rnorm(1, 0, 0.5))) } Xbeta &lt;- rep(0, n * p) for (j in 1:p) { Xbeta[1:n + (j-1)*n] &lt;- X %*% beta[[j]] } sigma2 &lt;- c(1, 2, 0.5, 1) phi &lt;- 3 / 0.5 D &lt;- as.matrix(rdist(coords)) ## constant spatial correlation function R_n &lt;- exp(-phi * D) R_n_chol &lt;- chol(R_n) f &lt;- matrix(0, L, n) for (l in 1:L) { f[l, ] &lt;- mvnfast::rmvn(1, rep(0, n), R_n_chol, isChol = TRUE) } dat &lt;- data.frame( lon = rep(coords[, 1], each = 3), lat = rep(coords[, 2], each = 3), f = c(f), factor = rep(1:3, times = n) ) ggplot(data = dat, aes(x = lon, y = lat, fill = f)) + geom_raster() + facet_grid(~ factor) + ggtitle(&quot;spatial random effects&quot;) + scale_fill_viridis_c() generate the correlation matrix \\(\\mathbf{A}\\) A &lt;- matrix(0, p, L) for (i in 1:p) { for (l in 1:L) { if (i == l){ A[i, l] &lt;- exp(rnorm(1)) } else if (i &gt; l) { A[i, l] &lt;- rnorm(1) } } } A ## simulate a 4 by 4 covariance matrix for the ggcorr(data = NULL, cor_matrix = cov2cor(A %*% t(A)), label = TRUE) + scale_fill_viridis_c() + ggtitle(&quot;Correlation among processes&quot;) ## Scale for &#39;fill&#39; is already present. Adding another scale ## for &#39;fill&#39;, which will replace the existing scale. ## [,1] [,2] [,3] ## [1,] 0.4729 0.0000 0.0000 ## [2,] -1.6391 0.3597 0.0000 ## [3,] 2.5958 0.3032 2.4812 ## [4,] 0.2078 0.1780 -0.1658 y &lt;- rep(0, n*p) for (j in 1:p) { idx &lt;- 1:n + (j-1) * n y[idx] &lt;- rnorm(n, Xbeta[idx] + (A %*% f)[j, ], sqrt(sigma2[j])) } dat &lt;- data.frame( lon = coords[, 1], lat = coords[, 2], mu = Xbeta, Af = c(t(A %*% f)), y = y, var = rep(1:p, each = n) ) p1 &lt;- ggplot(data = dat, aes(x = lon, y = lat, fill = Af)) + geom_raster() + facet_grid(~ var) + ggtitle(&quot;spatial random effects&quot;) + scale_fill_viridis_c() p2 &lt;- ggplot(data = dat, aes(x = lon, y = lat, fill = mu)) + geom_raster() + facet_grid(~ var) + ggtitle(&quot;fixed effects&quot;) + scale_fill_viridis_c() p3 &lt;- ggplot(data = dat, aes(x = lon, y = lat, fill = y)) + geom_raster() + facet_grid(~ var) + ggtitle(&quot;observed data&quot;) + scale_fill_viridis_c() ## plot using patchwork package p1 / p2 / p3 Non-seperable model: If the latent processes \\(f_\\ell\\) are \\(iid\\) GPs with mean zero, variance 1 but the latent process \\(j\\) has correlation function \\(C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta}_\\ell)\\), then the covariance is \\[\\begin{align*} Cov \\left( y(\\mathbf{s}), y(\\mathbf{s}&#39;) \\right) &amp; = Cov \\left( \\mathbf{A} \\mathbf{f}(\\mathbf{s}), \\mathbf{A} \\mathbf{f}(\\mathbf{s}&#39;) \\right) \\\\ &amp; = \\mathbf{A} Cov \\left( \\mathbf{f}(\\mathbf{s}), \\mathbf{f}(\\mathbf{s}&#39;)\\right) \\mathbf{A}&#39;\\\\ &amp; = \\mathbf{A} \\begin{pmatrix} Cov(f_1(\\mathbf{s}), f_1(\\mathbf{s}&#39;)) &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; Cov(f_2(\\mathbf{s}), f_2(\\mathbf{s}&#39;)) &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; Cov(f_L(\\mathbf{s}), f_L(\\mathbf{s}&#39;)) \\end{pmatrix} \\mathbf{A}&#39; // &amp; = \\mathbf{A} \\begin{pmatrix} C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta}_\\1) &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta}_\\2) &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; C(\\|\\mathbf{s} - \\mathbf{s}&#39;\\| | \\boldsymbol{\\theta}_L) \\end{pmatrix} \\mathbf{A}&#39; \\end{align*}\\] is non-seperable because we can’t write the covariance function as a product of “cross-covariance” times “spatial correlation” Non-stationarity: to account for spatially varying response, one can model the \\(A_{ij}\\)s as spatially varying by \\(A_{ij}(\\mathbf{s})\\). Therefore, the cross-covariance matrix \\(\\mathbf{A}(\\mathbf{s}) \\mathbf{A}(\\mathbf{s}&#39;)\\) is spatially varying ## Make some data set.seed(1) n &lt;- 30^2 p &lt;- 4 L &lt;- 3 ## three latent factors coords &lt;- expand.grid( seq(0, 1, length.out = sqrt(n)), seq(0, 1, length.out = sqrt(n)) ) X &lt;- cbind(1, rnorm(n, 0, 0.5)) beta &lt;- list(p) for (j in 1:p) { beta[[j]] &lt;- as.matrix(c(1, rnorm(1, 0, 0.5))) } Xbeta &lt;- rep(0, n * p) for (j in 1:p) { Xbeta[1:n + (j-1)*n] &lt;- X %*% beta[[j]] } sigma2 &lt;- c(1, 2, 0.5, 1) phi &lt;- 3 / 0.5 D &lt;- as.matrix(rdist(coords)) ## constant spatial correlation function R_n &lt;- exp(-phi * D) R_n_chol &lt;- chol(R_n) f &lt;- matrix(0, L, n) for (l in 1:L) { f[l, ] &lt;- mvnfast::rmvn(1, rep(0, n), R_n_chol, isChol = TRUE) } dat &lt;- data.frame( lon = rep(coords[, 1], each = 3), lat = rep(coords[, 2], each = 3), f = c(f), factor = rep(1:3, times = n) ) ggplot(data = dat, aes(x = lon, y = lat, fill = f)) + geom_raster() + facet_grid(~ factor) + ggtitle(&quot;spatial random effects&quot;) + scale_fill_viridis_c() generate the spatially-varying correlation matrix \\(\\mathbf{A}(\\mathbf{s})\\) where each of the parameters of the matrix are assigned the same spatial exponential covariace function \\(\\exp \\left(-\\mathbf{D} / \\phi \\right)\\) with respect to the required support. A &lt;- array(0, dim = c(p, L, n)) for (i in 1:p) { for (l in 1:L) { if (i == l){ A[i, l, ] &lt;- exp(mvnfast::rmvn(1, rep(0, n), exp(-D))) } else if (i &gt; l) { A[i, l, ] &lt;- mvnfast::rmvn(1, rep(0, n), exp(-D)) } } } str(A) ## simulate a 4 by 4 covariance matrix for the g1 &lt;- ggcorr(data = NULL, cor_matrix = cov2cor(A[, , 1] %*% t(A[, , 1])), label = TRUE) + scale_fill_viridis_c() + ggtitle(&quot;Correlation among processes&quot;) g10 &lt;- ggcorr(data = NULL, cor_matrix = cov2cor(A[, , 10] %*% t(A[, , 10])), label = TRUE) + scale_fill_viridis_c() + ggtitle(&quot;Correlation among processes&quot;) g100 &lt;- ggcorr(data = NULL, cor_matrix = cov2cor(A[, , 100] %*% t(A[, , 100])), label = TRUE) + scale_fill_viridis_c() + ggtitle(&quot;Correlation among processes&quot;) g500 &lt;- ggcorr(data = NULL, cor_matrix = cov2cor(A[, , 500] %*% t(A[, , 500])), label = TRUE) + scale_fill_viridis_c() + ggtitle(&quot;Correlation among processes&quot;) (g1 | g10) / (g100 | g500) ## num [1:4, 1:3, 1:900] 0.476 0.931 0.95 -0.959 0 ... y &lt;- rep(0, n*p) for (j in 1:p) { idx &lt;- (j-1) * n for (i in 1:n) { y[idx + i] &lt;- rnorm(1, Xbeta[i + idx] + (A[, , i] %*% f[, i])[j, ], sqrt(sigma2[j])) } } Af &lt;- matrix(0, n, p) for (i in 1:n) { Af[i, ] &lt;- (A[, , i] %*% f[, i]) } dat &lt;- data.frame( lon = coords[, 1], lat = coords[, 2], mu = Xbeta, Af = c(Af), y = y, var = rep(1:p, each = n) ) p1 &lt;- ggplot(data = dat, aes(x = lon, y = lat, fill = Af)) + geom_raster() + facet_grid(~ var) + ggtitle(&quot;non-stationary spatial random effects&quot;) + scale_fill_viridis_c() p2 &lt;- ggplot(data = dat, aes(x = lon, y = lat, fill = mu)) + geom_raster() + facet_grid(~ var) + ggtitle(&quot;fixed effects&quot;) + scale_fill_viridis_c() p3 &lt;- ggplot(data = dat, aes(x = lon, y = lat, fill = y)) + geom_raster() + facet_grid(~ var) + ggtitle(&quot;non-stationary observed data&quot;) + scale_fill_viridis_c() ## plot using patchwork package p1 / p2 / p3 "],
["day-27.html", "27 Day 27 27.1 Announcements", " 27 Day 27 27.1 Announcements reading assignment due Wednesday 4/1 HW 4 assignment due Monday 4/6 "],
["day-28.html", "28 Day 28 28.1 Announcements 28.2 Intro to stan 28.3 stan Hints and tips", " 28 Day 28 library(tidyverse) library(fields) library(mvnfast) library(patchwork) library(rstan) ## use recommended rstan settings options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) library(bayesplot) 28.1 Announcements 28.2 Intro to stan Needs a c++ compiler toolchain. See here for instructions on how to install the rstan library instructions for your OS (Windows, MacOC, Linux, and support for different languages, etc. on the bar to the right) probabilistic programming allows you to focus on modelling rather than algorithms for fitting Many alternatives (NIMBLE, JAGS, PyMC3, PyMC4 - experimental) 28.2.1 Example: Linear regression set.seed(444) n &lt;- 1000 X &lt;- cbind(1, runif(n)) beta &lt;- c(3, 2) sigma &lt;- 0.25 y &lt;- as.vector(X %*% beta + rnorm(n, 0, sigma)) data.frame(y = y, x = X[, 2], mu = X %*% beta) %&gt;% ggplot(aes(x = x, y = y)) + geom_point() + geom_line(aes(x = x, y = mu), color = &quot;red&quot;) we need to define a stan model create a stan model in a folder named stan_models in the Rstudio project folder print the output of the model linear-regression.stan cat(read_lines(here::here(&quot;stan_models&quot;, &quot;linear-regression.stan&quot;)), sep = &quot;\\n&quot;) // // This Stan program defines a simple model, with a // vector of values &#39;y&#39; modeled as normally distributed // with mean &#39;mu&#39; = &#39;X beta` and standard deviation &#39;sigma&#39;. // // Learn more about model development with Stan at: // // http://mc-stan.org/users/interfaces/rstan.html // https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started // // The input data is a vector &#39;y&#39; of length &#39;n&#39; and an // &#39;n&#39; times &#39;p&#39; matrix of covariates (including the intercept) data { int&lt;lower=0&gt; n; int&lt;lower=0&gt; p; vector[n] y; matrix[n, p] X; } // The parameters accepted by the model. Our model // accepts two parameters &#39;beta&#39; and &#39;sigma&#39;. parameters { vector[p] beta; real&lt;lower=0&gt; sigma; vector[n] y_rep; } // The transformed parameters X %*% beta for the model. transformed parameters { vector[n] mu; mu = X * beta; } // The model to be estimated. We model the output // &#39;y&#39; to be normally distributed with mean &#39;mu&#39; // and standard deviation &#39;sigma&#39;. model { y ~ normal(mu, sigma); // Note that this is vectorized and equivalent to // for (i in 1:n) { // y[i] ~ normal(mu[i], sigma); // } // posterior predictive distribution y_rep ~ normal(mu, sigma); } // Note: the stan file must end in a blank (new) line What are the priors implied by this model on \\(\\boldsymbol{\\beta}\\) and \\(\\sigma\\)? Fitting the model fit &lt;- stan( file = here::here(&quot;stan_models&quot;, &quot;linear-regression.stan&quot;), data = list(y = y, n = n, X = X, p = ncol(X)), iter = 1000 ) with priors cat(read_lines(here::here(&quot;stan_models&quot;, &quot;linear-regression-priors.stan&quot;)), sep = &quot;\\n&quot;) // // This Stan program defines a simple model, with a // vector of values &#39;y&#39; modeled as normally distributed // with mean &#39;mu&#39; = &#39;X beta` and standard deviation &#39;sigma&#39;. // // Learn more about model development with Stan at: // // http://mc-stan.org/users/interfaces/rstan.html // https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started // // The input data is a vector &#39;y&#39; of length &#39;n&#39; and an // &#39;n&#39; times &#39;p&#39; matrix of covariates (including the intercept) data { int&lt;lower=0&gt; n; int&lt;lower=0&gt; p; vector[n] y; matrix[n, p] X; } // The parameters accepted by the model. Our model // accepts two parameters &#39;beta&#39; and &#39;sigma&#39;. parameters { vector[p] beta; real&lt;lower=0&gt; sigma; vector[n] y_rep; } // The transformed parameters X %*% beta for the model. transformed parameters { vector[n] mu; mu = X * beta; } // The model to be estimated. We model the output // &#39;y&#39; to be normally distributed with mean &#39;mu&#39; // and standard deviation &#39;sigma&#39;. model { y ~ normal(mu, sigma); beta ~ normal(0, 5); sigma ~ cauchy(0, 5); // Note that this is vectorized and equivalent to // for (i in 1:n) { // y[i] ~ normal(mu[i], sigma); // } // posterior predictive distribution y_rep ~ normal(mu, sigma); } // Note: the stan file must end in a blank (new) line fit_prior &lt;- stan( file = here::here(&quot;stan_models&quot;, &quot;linear-regression-priors.stan&quot;), data = list(y = y, n = n, X = X, p = ncol(X)), iter = 1000 ) examine the output from the MCMC fit ## only plot the regression parameters print(fit, probs = c(0.1, 0.9), pars = c(&quot;beta&quot;, &quot;sigma&quot;, &quot;lp__&quot;)) ## trace plots mcmc_trace(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma&quot;, &quot;lp__&quot;)) ## area plots of posterior mcmc_areas(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma&quot;)) ## Warning: `expand_scale()` is deprecated; use `expansion()` ## instead. ## acf plots mcmc_acf(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma&quot;)) ## violin plots of posterior mcmc_violin(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma&quot;)) ## Inference for Stan model: linear-regression. ## 4 chains, each with iter=1000; warmup=500; thin=1; ## post-warmup draws per chain=500, total post-warmup draws=2000. ## ## mean se_mean sd 10% 90% n_eff Rhat ## beta[1] 3.00 0.00 0.02 2.98 3.02 1141 1.00 ## beta[2] 1.98 0.00 0.03 1.94 2.02 1165 1.00 ## sigma 0.26 0.00 0.01 0.25 0.26 901 1.00 ## lp__ 1717.79 1.44 30.48 1679.23 1756.59 447 1.01 ## ## Samples were drawn using NUTS(diag_e) at Tue Apr 7 20:44:57 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). compare these estimates to the lm estimtes summary(lm(y ~ X - 1)) ## ## Call: ## lm(formula = y ~ X - 1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.6696 -0.1733 0.0016 0.1705 0.8912 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## X1 2.9962 0.0165 181.4 &lt;2e-16 *** ## X2 1.9802 0.0285 69.4 &lt;2e-16 *** ## --- ## Signif. codes: ## 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.256 on 998 degrees of freedom ## Multiple R-squared: 0.996, Adjusted R-squared: 0.996 ## F-statistic: 1.24e+05 on 2 and 998 DF, p-value: &lt;2e-16 28.2.1.1 Posterior predictive checks How do we know if the model is fitting the data well? posterior predictive checks examples here and here \\[\\begin{align*} [\\tilde{\\mathbf{y}} | \\mathbf{y}] &amp; = \\int [\\tilde{\\mathbf{y}} | \\boldsymbol{\\theta} ] [\\boldsymbol{\\theta} | \\mathbf{y}] \\,d\\boldsymbol{\\theta} \\end{align*}\\] where the integral over \\(\\boldsymbol{\\theta}\\) is performed using MCMC sampling. Using the bayesplot package, we can explre the posterior predictive checks (ppcs) using a variety of functions ## extract the posterior predictive samples y_rep &lt;- rstan::extract(fit, pars = &quot;y_rep&quot;)$y_rep ## only plot for 25 randomly selected samples plot_idx &lt;- sample(1:n, 25) ppc_data(y, y_rep) ## # A tibble: 2,001,000 x 6 ## y_id rep_id rep_label is_y is_y_label value ## &lt;int&gt; &lt;int&gt; &lt;fct&gt; &lt;lgl&gt; &lt;fct&gt; &lt;dbl&gt; ## 1 1 1 italic(y)[rep] ( … FALSE italic(y)[re… 3.31 ## 2 1 2 italic(y)[rep] ( … FALSE italic(y)[re… 3.25 ## 3 1 3 italic(y)[rep] ( … FALSE italic(y)[re… 3.36 ## 4 1 4 italic(y)[rep] ( … FALSE italic(y)[re… 3.17 ## 5 1 5 italic(y)[rep] ( … FALSE italic(y)[re… 3.72 ## 6 1 6 italic(y)[rep] ( … FALSE italic(y)[re… 2.80 ## 7 1 7 italic(y)[rep] ( … FALSE italic(y)[re… 3.22 ## 8 1 8 italic(y)[rep] ( … FALSE italic(y)[re… 2.95 ## 9 1 9 italic(y)[rep] ( … FALSE italic(y)[re… 3.43 ## 10 1 10 italic(y)[rep] ( … FALSE italic(y)[re… 3.32 ## # … with 2,000,990 more rows ppc_hist(y, y_rep[1:5, ]) ## only use a subset of rows ## `stat_bin()` using `bins = 30`. Pick better value with ## `binwidth`. ppc_intervals(y[plot_idx], y_rep[, plot_idx], x = X[, 2][plot_idx]) ppc_ribbon(y[plot_idx], y_rep[, plot_idx], x = X[, 2][plot_idx]) ppc_dens_overlay(y, y_rep) ppc_ecdf_overlay(y, y_rep) 28.2.1.2 Optimization in stan We can also fit the model using optimization: max posterior mode, also known as max a posteriori (MAP). max penalized likelihood (MLE). model &lt;- stan_model( file = here::here(&quot;stan_models&quot;, &quot;linear-regression.stan&quot;) ) mle &lt;- optimizing(model, data = list(y = y, n = n, X = X, p = ncol(X))) print(mle$par[1:3], digits = 4) ## beta[1] beta[2] sigma ## 2.9964 1.9799 0.1811 28.2.2 Example: Spatial model in stan cat(read_lines(here::here(&quot;stan_models&quot;, &quot;spatial-regression.stan&quot;)), sep = &quot;\\n&quot;) // // This Stan program defines a simple model, with a // vector of values &#39;y&#39; modeled as normally distributed // with mean &#39;mu&#39; = &#39;X beta` and standard deviation &#39;sigma&#39;. // // Learn more about model development with Stan at: // // http://mc-stan.org/users/interfaces/rstan.html // https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started // // The input data is a vector &#39;y&#39; of length &#39;n&#39;, an // &#39;n&#39; times &#39;p&#39; matrix of covariates (including the intercept), // and &#39;coords&#39; is an &#39;n&#39; times &#39;2&#39; matrix of spatial locations data { int&lt;lower=0&gt; n; int&lt;lower=0&gt; p; vector[n] y; matrix[n, p] X; matrix[n, 2] coords; } // calculate the Euclidean distance between observed locations transformed data { matrix[n, n] D; for (i in 1:n) { for (j in 1:(i-1)) { D[i, j] = sqrt(sum(square(coords[i, ] - coords[j, ]))); D[j, i] = D[i, j]; } D[i, i] = 0.0; } } // The parameters accepted by the model. Our model // accepts the parameters &#39;beta&#39;, &#39;sigma2&#39;, &#39;tau2&#39; and &#39;phi&#39;. parameters { vector[p] beta; real&lt;lower=0&gt; sigma2; real&lt;lower=0&gt; tau2; real&lt;lower=0&gt; phi; } // The transformed parameters X %*% beta for the model, // Sigma for the covariance matrix, and L_Sigma for the // Cholesky decomposition of the covariance matrix transformed parameters { vector[n] mu; cov_matrix[n] Sigma; matrix[n, n] L_Sigma; mu = X * beta; for (i in 1:(n-1)) { for (j in (i+1):n) { Sigma[i, j] = tau2 * exp(- D[i, j] / phi); Sigma[j, i] = Sigma[i, j]; } } // equivalent to // Sigma = exp( - D / phi) for (i in 1:n) { Sigma[i, i] = sigma2 + tau2; } L_Sigma = cholesky_decompose(Sigma); } // The model to be estimated. We model the output // &#39;y&#39; to be normally distributed with mean &#39;mu&#39; // and Cholesky of covariance matrix L_Sigma. model { // priors phi ~ normal(0, 10); tau2 ~ normal(0, 5); sigma2 ~ normal(0, 5); beta ~ normal(0, 5); // likelihood y ~ multi_normal_cholesky(mu, L_Sigma); } // Note: the stan file must end in a blank (new) line ## Make some data set.seed(1) n &lt;- 100 coords &lt;- cbind(runif(n, 0, 1), runif(n, 0, 1)) X &lt;- cbind(1, rnorm(n)) beta &lt;- as.matrix(c(1, 5)) sigma2 &lt;- 1 tau2 &lt;- 5 phi &lt;- 3 / 0.5 D &lt;- as.matrix(rdist(coords)) R &lt;- exp(- phi * D) eta &lt;- c(rmvn(1, rep(0,n), tau2 * R)) y &lt;- rnorm(n, X %*% beta + eta, sqrt(sigma2)) Fit the stan model this can be a little slow for fitting relatively small data however, stan uses Hamiltonian Monte Carlo (HMC) which is highly efficient often can get by with much shorter MCMC chains (500-1000 samples – monitor the effective sample size) fit &lt;- stan( file = here::here(&quot;stan_models&quot;, &quot;spatial-regression.stan&quot;), data = list(y = y, n = n, X = X, p = ncol(X), coords = coords), iter = 1000 ) ## Warning: The largest R-hat is NA, indicating chains have not mixed. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#r-hat ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#bulk-ess ## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#tail-ess examine the output from the MCMC fit ## only plot the regression parameters print(fit, probs = c(0.1, 0.9), pars = c(&quot;beta&quot;, &quot;sigma2&quot;, &quot;tau2&quot;, &quot;phi&quot;)) ## trace plots mcmc_trace(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma2&quot;, &quot;tau2&quot;, &quot;phi&quot;)) ## area plots of posterior mcmc_areas(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma2&quot;, &quot;tau2&quot;, &quot;phi&quot;)) ## Warning: `expand_scale()` is deprecated; use `expansion()` ## instead. ## acf plots mcmc_acf(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma2&quot;, &quot;tau2&quot;, &quot;phi&quot;)) ## Inference for Stan model: spatial-regression. ## 4 chains, each with iter=1000; warmup=500; thin=1; ## post-warmup draws per chain=500, total post-warmup draws=2000. ## ## mean se_mean sd 10% 90% n_eff Rhat ## beta[1] 1.83 0.03 1.20 0.42 3.33 1228 1 ## beta[2] 4.82 0.00 0.17 4.60 5.02 1619 1 ## sigma2 1.06 0.01 0.48 0.48 1.68 1186 1 ## tau2 7.63 0.07 2.40 4.95 11.03 1084 1 ## phi 0.30 0.01 0.15 0.15 0.49 832 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Apr 7 20:48:05 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 28.2.3 Example: Predictive process model in stan use the same data from the previous example cat(read_lines(here::here(&quot;stan_models&quot;, &quot;predictive-process-regression.stan&quot;)), sep = &quot;\\n&quot;) // // This Stan program defines a simple model, with a // vector of values &#39;y&#39; modeled as normally distributed // with mean &#39;mu&#39; = &#39;X beta` and standard deviation &#39;sigma&#39;. // // Learn more about model development with Stan at: // // http://mc-stan.org/users/interfaces/rstan.html // https://github.com/stan-dev/rstan/wiki/RStan-Getting-Started // // The input data is a vector &#39;y&#39; of length &#39;n&#39;, an // &#39;n&#39; times &#39;p&#39; matrix of covariates (including the intercept), // an &#39;n&#39; times &#39;2&#39; matrix &#39;coords&#39; of spatial locations, and // an &#39;n_knots&#39; times &#39;2&#39; matrix of &#39;knots&#39; for the predictive process data { int&lt;lower=0&gt; n; int&lt;lower=0&gt; p; int&lt;lower=0&gt; n_knots; vector[n] y; matrix[n, p] X; matrix[n, 2] coords; matrix[n_knots, 2] knots; } transformed data { matrix[n, n_knots] D; matrix[n_knots, n_knots] D_star; for (i in 1:n) { for (j in 1:n_knots) { D[i, j] = sqrt(sum(square(coords[i, ] - knots[j, ]))); } } for (i in 1:n_knots) { for (j in 1:(i-1)) { D_star[i, j] = sqrt(sum(square(knots[i, ] - knots[j, ]))); D_star[j, i] = D_star[i, j]; } D_star[i, i] = 0.0; } } // The parameters accepted by the model. Our model // accepts two parameters &#39;beta&#39; and &#39;sigma&#39;. parameters { vector[p] beta; real&lt;lower=0&gt; sigma2; real&lt;lower=0&gt; tau2; real&lt;lower=0&gt; phi; vector[n_knots] eta_centered; } // The transformed parameters X %*% beta for the model, // Sigma for the covariance matrix, and L_Sigma for the // Cholesky decomposition of the covariance matrix transformed parameters { vector[n] mu; vector[n_knots] eta_star; vector[n] eta; cov_matrix[n_knots] Sigma_star; cov_matrix[n_knots] Sigma_star_inv; matrix[n, n_knots] c; // fixed effects mu = X * beta; // latent GP covariance matrix at the knots Sigma_star = tau2 * exp(-D_star / phi); Sigma_star_inv = inverse(Sigma_star); // predictive process interpolation matrix c = tau2 * exp(- D / phi); // eta_centered is N(0, 1) // eta_star = L * eta_centered ~ N(0, LL&#39;) eta_star = cholesky_decompose(Sigma_star) * eta_centered; // predictive process interpolator eta = c * Sigma_star_inv * eta_star; } model { // priors phi ~ normal(0, 10); tau2 ~ normal(0, 5); sigma2 ~ normal(0, 5); beta ~ normal(0, 5); eta_centered ~ normal(0, 1); y ~ normal(mu + eta, sqrt(sigma2)); // Note that this is vectorized and equivalent to // for (i in 1:n) { // y[i] ~ normal(mu[i] + eta[i], sqrt(sigma2)); // } } // Note: the stan file must end in a blank (new) line Fit the stan model this can be a little slow for fitting relatively small data however, stan uses Hamiltonian Monte Carlo (HMC) which is highly efficient often can get by with much shorter MCMC chains (500-1000 samples – monitor the effective sample size) n_knots &lt;- 5^2 knots &lt;- expand.grid( seq(min(coords[, 1]), max(coords[, 1]), length.out = sqrt(n_knots)), seq(min(coords[, 2]), max(coords[, 2]), length.out = sqrt(n_knots)) ) fit &lt;- stan( file = here::here(&quot;stan_models&quot;, &quot;predictive-process-regression.stan&quot;), data = list(y = y, n = n, X = X, p = ncol(X), coords = coords, n_knots = n_knots, knots = knots), iter = 1000 ) examine the output from the MCMC fit ## only plot the regression parameters print(fit, probs = c(0.1, 0.9), pars = c(&quot;beta&quot;, &quot;sigma2&quot;, &quot;tau2&quot;, &quot;phi&quot;)) ## trace plots mcmc_trace(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma2&quot;, &quot;tau2&quot;, &quot;phi&quot;)) ## area plots of posterior mcmc_areas(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma2&quot;, &quot;tau2&quot;, &quot;phi&quot;)) ## Warning: `expand_scale()` is deprecated; use `expansion()` ## instead. ## acf plots mcmc_acf(fit, regex_pars = c(&quot;beta&quot;, &quot;sigma2&quot;, &quot;tau2&quot;, &quot;phi&quot;)) ## Inference for Stan model: predictive-process-regression. ## 4 chains, each with iter=1000; warmup=500; thin=1; ## post-warmup draws per chain=500, total post-warmup draws=2000. ## ## mean se_mean sd 10% 90% n_eff Rhat ## beta[1] 1.75 0.04 1.48 0.00 3.58 1379 1 ## beta[2] 4.80 0.00 0.20 4.55 5.06 2896 1 ## sigma2 3.54 0.02 0.60 2.83 4.33 1308 1 ## tau2 9.27 0.08 2.95 5.72 13.24 1355 1 ## phi 0.43 0.01 0.32 0.19 0.74 1001 1 ## ## Samples were drawn using NUTS(diag_e) at Tue Apr 7 20:51:27 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). 28.3 stan Hints and tips if there are issues, run one chain using chains = 1 option in the stan() function pay careful attention to the error messages – once you learn to read these they can be very informative if you have multiple chains that have widely different run times, that suggests there are issues with your model explore advanced stan diagnostics examples here, here, and here. General stan best pracitices "],
["day-29.html", "29 Day 29 29.1 Announcements 29.2 Areal Data", " 29 Day 29 library(tidyverse) library(viridis) library(mvnfast) library(igraph) library(Matrix) library(patchwork) library(rstan) ## use recommended rstan settings options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) library(bayesplot) set.seed(101) 29.1 Announcements 29.2 Areal Data Areal data are data that are associated with a spatial region the actual data might have occured at point locations but are aggregated across space example: disease spread individuals either have a disease or don’t (point level data) but the data are reported at aggregate levels (number of infections in the county/state/country) some data are only meaninful at the area level: e.g., votes in a county/state areal data are models for discrete spatial domains (geostatistical models are for continuous spatial domains) partition the domain \\(\\mathcal{D}\\) into \\(n\\) discrete units define \\(\\mathbf{y} = (y_1, \\ldots, y_n)&#39;\\) spatial models: account for spatial autocorrelation between regions which improves the residual error model borrows strength across locations to improve inference Question: how to measure the “closeness” of irregulary spaced regions? draw figure here can use spatial centroid draw figure here can I just use the geostatistical methods introduced previously? geostatistical models produce a valid, positive definite covariance matrix using continuous (geostatistical) methods works if the regions are rectangular shaped and of the same general size can also model the covariance using adjacency matrices and graphical models "],
["day-30.html", "30 Day 30 30.1 Announcements 30.2 Gaussian Markov random fields (GMRFs) 30.3 Conditional Autoregressive models (CAR models)", " 30 Day 30 library(tidyverse) library(viridis) library(mvnfast) library(igraph) library(Matrix) library(patchwork) library(rstan) ## use recommended rstan settings options(mc.cores = parallel::detectCores()) rstan_options(auto_write = TRUE) library(bayesplot) set.seed(101) 30.1 Announcements s(x) + s(y) vs. s(x, y) in MGCV 30.2 Gaussian Markov random fields (GMRFs) A Markov random field is specified from a set of conditional probability distributions. Let \\(\\mathbf{y} = (y_1, \\ldots, y_n)&#39;\\) be the \\(n\\)-dimensional vector for each of the \\(i\\) observations in the spatial doamin \\(\\mathcal{D}\\). Let \\(\\mathcal{A} \\in \\mathcal{D}\\) be defined as a subset of regions in the domain, \\(\\mathbf{y}_{\\mathcal{A}} = \\{ y_i | i \\in \\mathcal{A}\\}\\) be the set of observation in the subdomain \\(\\mathcal{A}\\), and \\(\\mathbf{y}_{-\\mathcal{A}} = \\{ y_i | i \\notin \\mathcal{A}\\}\\) be the set of observation not in the subdomain \\(\\mathcal{A}\\) A GMRF is a Gaussian distributed random vector \\(\\mathbf{y}\\) which obeys some conditional independence properties. For some \\(i \\neq j\\), \\[\\begin{align*} y_i \\perp y_j | \\mathbf{y}_{-\\{i, j\\}}, \\end{align*}\\] where, conditioned on \\(\\mathbf{y}_{-\\{i, j\\}}\\), \\(y_i\\) and \\(y_j\\) are independent. Given a graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) where \\(\\mathcal{V} = \\{1, \\ldots, n\\}\\) is the set of vertices and \\(\\mathcal{E} = \\{ \\{i, j\\} | i, j \\in \\mathcal{V}\\}\\), the set of edges of the graph, the goal is to sepcify a GMRF that has conditional independence properties that are consistent with an underlying graph structure (typically one implied by the adjacency structure of the spatial domain) This turns out to be quite easy by using the precision matrix \\(\\mathbf{Q} = \\boldsymbol{\\Sigma}^{-1}\\) of the random variable \\(\\mathbf{y}\\) Definition: A random vector \\(\\mathbf{y}\\) is a GMRF with respect to the labeled graph \\(\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})\\) with mean \\(\\boldsymbol{\\mu}\\) and symmetric, positive definite precision matrix \\(\\mathbf{Q}\\) \\(\\Leftrightarrow\\) its density is \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\mu}, \\mathbf{Q}] &amp; = (2 \\pi)^{-n / 2} |\\mathbf{Q}|^{1/2} \\exp\\left( -1 / 2 \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right) \\mathbf{Q} \\left(\\mathbf{y} - \\boldsymbol{\\mu} \\right) \\right) \\end{align*}\\] and \\[\\begin{align*} Q_{i, j} \\neq 0 \\hspace{1em} \\Leftrightarrow \\hspace{1em} \\{i, j\\} \\in \\mathcal{E} \\hspace{1em} \\forall \\hspace{1em} i \\neq j \\end{align*}\\] Note: this implies that \\(Q_{i, j} = 0 \\hspace{1em} \\forall \\hspace{1em} i \\neq j\\) where \\(\\{i, j\\} \\notin \\mathcal{E}\\) 30.2.1 Example: A stationary autoregressive time series of order 1 \\[\\begin{align*} y_t = \\phi y_{t-1} + \\varepsilon_t \\end{align*}\\] where \\(|\\phi| &lt; 1\\), \\(\\varepsilon_t \\stackrel{iid}{\\sim} N(0, 1)\\), and \\(y_1 \\sim N(0, 1 / (1 - \\phi^2))\\). n &lt;- 500 phi &lt;- 0.9 ## autoregression model representation y &lt;- rep(0, n) y[1] &lt;- rnorm(1, 0, sqrt(1 / (1 - phi^2))) for (t in 2:n) { y[t] &lt;- rnorm(1, phi * y[t-1], 1) } plot(y, type = &#39;l&#39;) The set of edges is \\(\\mathcal{E} = \\{ \\{ 1, 2\\}, \\{2, 3\\}, \\ldots, \\{n-1, n\\} \\}\\) and the precision matrix \\(\\mathbf{Q}\\) has nonzero elements \\(Q_{i,j} = -\\phi\\) for \\(|i-j| = 1\\), \\(Q_{1,1} = Q_{n,n} = 1\\) and \\(Q_{i,i} = 1 + \\phi^2\\) for \\(i = 2, \\ldots, n-1\\). ## GMRF representation Q &lt;- matrix(0, n, n) Q &lt;- toeplitz(c(0, -phi, rep(0, n - 2))) diag(Q) &lt;- c(1, rep(1 + phi^2, n - 2), 1) Q[1:5, 1:5] y &lt;- as.vector(rmvn(1, rep(0, n), solve(Q))) plot(y, type = &#39;l&#39;) ## [,1] [,2] [,3] [,4] [,5] ## [1,] 1.0 -0.90 0.00 0.00 0.00 ## [2,] -0.9 1.81 -0.90 0.00 0.00 ## [3,] 0.0 -0.90 1.81 -0.90 0.00 ## [4,] 0.0 0.00 -0.90 1.81 -0.90 ## [5,] 0.0 0.00 0.00 -0.90 1.81 notice how sparse the matrix \\(\\mathbf{Q}\\) is mean(Q == 0) ## [1] 0.994 Recall the autocorrelation function of an AR(1) timeseries at lag \\(h\\): \\(\\gamma(h) = \\phi^h\\) Sigma &lt;- solve(Q) plot( Sigma[1, ] / Sigma[1, 1], type = &#39;p&#39;, main = &quot;GMRF ACF in points, theoretical ACF in red&quot; ) lines(c(1, phi^(1:(n-1))), col = &quot;red&quot;) where \\(\\boldsymbol{\\Sigma}\\) is a dense matrix mean(Sigma == 0) ## [1] 0 therefore, the GMRF formulation can be evaluated using \\(\\mathcal{O}(n)\\) algorithms 30.3 Conditional Autoregressive models (CAR models) spatial data have no inherent ordering, but we can still define a set of edges \\(\\mathcal{E}\\) based on spatial adjacency Many ways to define adjacency – typically if two regions share a border, they are adjacent \\(i \\sim j\\) implies that site \\(i\\) and \\(j\\) are adjacent (equivalent to \\(\\{i, j\\} \\in \\mathcal{E}\\)) \\(\\mathbf{A}\\) is an \\(n \\times n\\) adjacency matrix \\(A_{ij} = I\\{i \\sim j\\}\\) is an indicator of adjacency \\(N_i = \\{j | A_{ij} = 1\\}\\) is the set of neighbors of region \\(i\\) \\(d_i = \\sum_{j=1}^n A_{ij}\\) is the number of neighbors of region \\(i\\) \\(\\mathbf{D} = \\operatorname{diag}(d_1, \\ldots, d_n)\\) \\(\\bar{y}_i = \\frac{1}{d_i} \\sum_{j=1}^n A_{ij} y_i\\) is the mean of region \\(i\\)’s neighbors (note the similarity to a nearest neighbors type algorithm) 30.3.1 Models built based on conditional distributions: \\(y_i | \\mathbf{y}_{-i}\\) CAR model is a GMRF given by \\[\\begin{align*} y_i | \\mathbf{y}_{-i} &amp; \\sim N \\left( \\phi \\bar{y}_i, \\frac{\\sigma^2}{d_i} \\right) \\end{align*}\\] where each region \\(i\\) is conditionally independent from all other regions given its neighbors If there is a valid joint distribution that has these full conditional distributions, the conditional distributions are said to be compatible how do we know if these conditional distributions are compatible? Consider the joint distribution \\[\\begin{align*} \\mathbf{y} \\sim N \\left(0, \\sigma^2 \\left(\\mathbf{D} - \\phi \\mathbf{A} \\right)^{-1} \\right) \\end{align*}\\] the covariance is positive definite if \\(\\frac{1}{\\lambda_n} &lt; \\phi &lt; \\frac{1}{\\lambda_1}\\) where \\(\\lambda_n\\) is the smallest eigenvalue and \\(\\lambda_1\\) is the largest eigenvalue of \\(\\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}\\) First, let \\(\\boldsymbol{\\Gamma} \\boldsymbol{\\Lambda} \\boldsymbol{\\Gamma}&#39;\\) be the eigen-decomposition of \\(\\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}\\) where \\(\\boldsymbol{\\Lambda} = \\operatorname{diag}(\\lambda_1, \\ldots, \\lambda_n)\\) with \\(\\lambda_1 \\geq \\lambda_2 \\geq \\cdots \\geq \\lambda_n\\) \\[\\begin{align*} \\mathbf{D} - \\phi \\mathbf{A} &amp; = \\mathbf{D}^{1/2} (\\mathbf{I} - \\phi \\mathbf{D}^{-1/2}\\mathbf{A}\\mathbf{D}^{-1/2}) \\mathbf{D}^{1/2} \\\\ &amp; = \\mathbf{D}^{1/2} (\\mathbf{I} - \\phi \\boldsymbol{\\Gamma} \\boldsymbol{\\Lambda} \\boldsymbol{\\Gamma}&#39;) \\mathbf{D}^{1/2} \\\\ &amp; = \\mathbf{D}^{1/2} (\\boldsymbol{\\Gamma} \\boldsymbol{\\Gamma}&#39; - \\phi \\boldsymbol{\\Gamma} \\boldsymbol{\\Lambda} \\boldsymbol{\\Gamma}&#39;) \\mathbf{D}^{1/2} \\\\ &amp; = \\mathbf{D}^{1/2} \\boldsymbol{\\Gamma} (\\mathbf{I} - \\phi \\boldsymbol{\\Lambda}) \\boldsymbol{\\Gamma}&#39; \\mathbf{D}^{1/2} \\\\ \\end{align*}\\] which has positive eigenvlaues \\(\\Leftrightarrow\\) \\(\\frac{1}{\\lambda_n} &lt; \\phi &lt; \\frac{1}{\\lambda_1}\\) (if \\(\\phi\\) was outside this range, the \\(\\mathbf{I} - \\phi \\boldsymbol{\\Lambda}\\) would contain negative values which would violate the positive definite condition) To show the above model gives the desired conditional probability distributions \\[\\begin{align*} \\mathbf{y} &amp; \\sim N \\left(0, \\sigma^2 \\left(\\mathbf{D} - \\phi \\mathbf{A} \\right)^{-1} \\right) \\\\ &amp; \\propto \\exp \\left( -\\frac{1}{2 \\sigma^2} \\mathbf{y}&#39; \\left(\\mathbf{D} - \\phi \\mathbf{A} \\right) \\mathbf{y} \\right) \\\\ &amp; \\propto \\exp \\left( -\\frac{1}{2 \\sigma^2} \\left( \\sum_{i=1}^n d_i y_i^2 - 2 \\phi \\sum_{i \\sim j} y_{ij} \\right) \\right) \\end{align*}\\] where the conditional probability distribution \\([y_i | \\mathbf{y}_{-i}]\\) is \\[\\begin{align*} [y_i | \\mathbf{y}_{-i}] &amp; \\propto \\exp \\left( -\\frac{1}{2 \\sigma^2} \\left( d_i y_i^2 - 2 \\phi d_i \\bar{y}_i \\right) \\right) \\\\ &amp; \\propto \\exp \\left( -\\frac{1}{2 \\frac{\\sigma^2} {d_i}} \\left( y_i^2 - 2 \\phi \\bar{y}_i \\right) \\right) \\\\ &amp; \\propto \\exp \\left( -\\frac{1}{2 \\frac{\\sigma^2} {d_i}} \\left( y_i - \\phi \\bar{y}_i \\right)^2 \\right) \\\\ \\end{align*}\\] which is the kernel of the conditional probability distribution we started with \\[\\begin{align*} y_i | \\mathbf{y}_{-i} &amp; \\sim N \\left( \\phi \\bar{y}_i, \\frac{\\sigma^2}{d_i} \\right) \\end{align*}\\] The multivariate normal likelihood for the CAR model is: \\[\\begin{align*} [\\mathbf{y} | \\boldsymbol{\\mu}, \\mathbf{Q}] &amp; = (2 \\pi)^{-n / 2} |\\mathbf{D} - \\phi \\mathbf{A}|^{1/2} \\exp\\left( -1 / 2 \\left( \\mathbf{y} - \\boldsymbol{\\mu} \\right) \\left(\\mathbf{D} - \\phi \\mathbf{A} \\right) \\left(\\mathbf{y} - \\boldsymbol{\\mu} \\right) \\right) \\end{align*}\\] The kernel of the likelihood is trivially fast due to sparse matrix operations and due to the precision construction (no matrix inverses!) The determinant \\(|\\mathbf{D} - \\phi \\mathbf{A}|\\) is easy to calculate as \\[\\begin{align*} |\\mathbf{D} - \\phi \\mathbf{A}| &amp; = |\\mathbf{D}^{1/2} \\left( \\mathbf{I} - \\phi \\mathbf{A} \\right) \\mathbf{D}^{1/2}| \\\\ &amp; = |\\mathbf{D}^{1/2} \\boldsymbol{\\Gamma} \\left( \\mathbf{I} - \\phi \\boldsymbol{\\Lambda} \\right) \\boldsymbol{\\Gamma}&#39; \\mathbf{D}^{1/2}| \\\\ &amp; = |\\mathbf{D}^{1/2} \\boldsymbol{\\Gamma}| | \\mathbf{I} - \\phi \\boldsymbol{\\Lambda}| |\\boldsymbol{\\Gamma}&#39; \\mathbf{D}^{1/2}| \\\\ &amp; = | \\mathbf{I} - \\phi \\boldsymbol{\\Lambda}| \\\\ &amp; = \\prod_{i=1}^n| (1 - \\phi \\lambda_i) \\end{align*}\\] sparse routines can be used to calculate these eigenvalues if \\(\\phi \\in [0, 1)\\), this model is called the conditional autoregressive (CAR) model and the covariance is positive definite so that the joint distribution of \\(\\mathbf{y}\\) is proper (integrates to 1) if \\(\\phi = 1\\), this is called an intrinsic CAR (ICAR) model and the covariance is singular so this model is only useful as a prior very efficient model for Bayesian inference and MCMC needs an additional assumption that \\(\\sum_{i=1}^n y_i = 0\\) Note: \\(\\phi\\) is not a correlation! However, correlation increases as \\(\\phi\\) increases typicallly, we assume \\(\\phi\\) will be large A close look at the spatial structure implied by the CAR and SAR models if \\(\\phi = 0\\), then the observations are spatially independent n &lt;- 10^2 phi &lt;- 0.4 ## construct a sparse matrix adjacency matrix for a 2-d lattice A &lt;- igraph::as_adjacency_matrix(make_lattice(length = sqrt(n), dim = 2), sparse = TRUE) A[1:5, 1:5] D &lt;- Diagonal(x = colSums(A)) D[1:5, 1:5] Q &lt;- D - phi * A Q[1:5, 1:5] Sigma &lt;- solve(Q) ## calculate the correlation from a first order CAR Sigma[5, 6] / Sigma[5, 5] n_phi &lt;- 500 phi &lt;- seq(-1, 1, length.out = n_phi)[-c(1, n_phi)] rho &lt;- rep(0, n_phi - 2) ## do this as a loop for (i in 1:(n_phi-2)) { A &lt;- igraph::as_adjacency_matrix(make_lattice(length = sqrt(n), dim = 2), sparse = TRUE) D &lt;- Diagonal(x = colSums(A)) Q &lt;- D - phi[i] * A Sigma &lt;- solve(Q) ## calculate the correlation from a first order CAR rho[i] &lt;- Sigma[5, 6] / Sigma[5, 5] } data.frame(phi = phi, rho = rho) %&gt;% ggplot(aes(x = phi, y = rho)) + geom_line() + ylim(c(-1, 1)) + ylab(&quot;correlation&quot;) + ggtitle(&quot;CAR correlation (lag-1) vs phi parameter&quot;) + geom_abline(slope = 1, intercept = 0, color = &quot;red&quot;) ## 5 x 5 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] . 1 . . . ## [2,] 1 . 1 . . ## [3,] . 1 . 1 . ## [4,] . . 1 . 1 ## [5,] . . . 1 . ## 5 x 5 diagonal matrix of class &quot;ddiMatrix&quot; ## [,1] [,2] [,3] [,4] [,5] ## [1,] 2 . . . . ## [2,] . 3 . . . ## [3,] . . 3 . . ## [4,] . . . 3 . ## [5,] . . . . 3 ## 5 x 5 sparse Matrix of class &quot;dgCMatrix&quot; ## ## [1,] 2.0 -0.4 . . . ## [2,] -0.4 3.0 -0.4 . . ## [3,] . -0.4 3.0 -0.4 . ## [4,] . . -0.4 3.0 -0.4 ## [5,] . . . -0.4 3.0 ## [1] 0.1393 Plot how the \\(\\phi\\) parameter determines the correlation ## simulate some data simulate_data &lt;- function(n, phi) { locs &lt;- expand.grid( seq(0, 1, length.out = sqrt(n)), seq(0, 1, length.out = sqrt(n)) ) # plot(make_lattice(length = sqrt(n), dim = 2)) A &lt;- make_lattice(length = sqrt(n), dim = 2) %&gt;% as_adjacency_matrix(sparse = TRUE) D &lt;- Diagonal(x = colSums(A)) Q &lt;- D - phi * A ## cholesky (this is a sparse matrix that is upper diagonal) R &lt;- chol(Q) y &lt;- backsolve(R, rnorm(n)) ## generate the plot output &lt;- data.frame(x = locs[, 1], y = locs[, 2], z = y) %&gt;% ggplot(aes(x = x, y = y, fill = z)) + geom_raster() + scale_fill_viridis() + ggtitle(paste(&quot;Phi = &quot;, phi)) return(output) } p1 &lt;- simulate_data(n = 50^2, phi = 0.999) p2 &lt;- simulate_data(n = 50^2, phi = 0.9) p3 &lt;- simulate_data(n = 50^2, phi = 0.8) p4 &lt;- simulate_data(n = 50^2, phi = - 0.8) p5 &lt;- simulate_data(n = 50^2, phi = -0.9) p6 &lt;- simulate_data(n = 50^2, phi = - 0.999) (p1 | p2 | p3) / (p4 | p5 | p6) 30.3.1.1 Fitting a CAR model in stan ## simulate data for fitting the model n &lt;- 10^2 phi &lt;- 0.999 sigma2 &lt;- 0.5 locs &lt;- expand.grid( seq(0, 1, length.out = sqrt(n)), seq(0, 1, length.out = sqrt(n)) ) # plot(make_lattice(length = sqrt(n), dim = 2)) A &lt;- make_lattice(length = sqrt(n), dim = 2) %&gt;% as_adjacency_matrix(sparse = TRUE) D &lt;- Diagonal(x = colSums(A)) Q &lt;- D - phi * A ## cholesky (this is a sparse matrix that is upper diagonal) R &lt;- chol(Q) eta &lt;- backsolve(R, rnorm(n)) y &lt;- eta + rnorm(n, 0, sqrt(sigma2)) p1 &lt;- data.frame(x = locs[, 1], y = locs[, 2], z = eta) %&gt;% ggplot(aes(x = x, y = y, fill = z)) + geom_raster() + scale_fill_viridis() + ggtitle(&quot;Spatial random effect&quot;) p2 &lt;- data.frame(x = locs[, 1], y = locs[, 2], z = y) %&gt;% ggplot(aes(x = x, y = y, fill = z)) + geom_raster() + scale_fill_viridis() + ggtitle(&quot;Observed data&quot;) p1 + p2 we need to define a stan model create a stan model in a folder named stan_models in the Rstudio project folder print the output of the model spatial-car.stan cat(read_lines(here::here(&quot;stan_models&quot;, &quot;spatial-car.stan&quot;)), sep = &quot;\\n&quot;) data { int&lt;lower=0&gt; N; // the number of observations vector[N] y; // the obersved values matrix[N, N] A; // the N times N adjacency matrix matrix[N, N] D; // the N times N diagonal matrix of neighbors } parameters { vector[N] eta; // real mu; real&lt;lower=0&gt; sigma2; real&lt;lower=0&gt; tau2; real&lt;lower=0, upper=1&gt; phi; } // transformed parameters { // vector[N] y_pred = mu + w_s; // } model { matrix[N, N] Sigma_inv = (D - phi * A) / tau2; eta ~ multi_normal_prec(rep_vector(0, N), Sigma_inv); mu ~ normal(0, 1); sigma2 ~ cauchy(0, 5); tau2 ~ cauchy(0, 5); y ~ normal(mu + eta, sigma2); } Fitting the model note: this is very slow for even moderate sized data fit &lt;- stan( file = here::here(&quot;stan_models&quot;, &quot;spatial-car.stan&quot;), ## note: convert from a sparse matrix format to regular matrix ## for fitting in stan data = list(y = y, N = n, D = as.matrix(D), A = as.matrix(A)), iter = 5000 ) ## Warning: There were 111 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Warning: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See ## http://mc-stan.org/misc/warnings.html#bfmi-low ## Warning: Examine the pairs() plot to diagnose sampling problems ## Warning: The largest R-hat is 1.12, indicating chains have not mixed. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#r-hat ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#bulk-ess ## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#tail-ess Note: this is giving warinings – it’s best to pay attention to these in stan print(fit, probs = c(0.1, 0.9), pars = c(&quot;mu&quot;, &quot;tau2&quot;, &quot;sigma2&quot;, &quot;phi&quot;, &quot;lp__&quot;)) mcmc_trace(fit, regex_pars = c(&quot;mu&quot;, &quot;tau2&quot;, &quot;sigma2&quot;, &quot;phi&quot;)) mcmc_areas(fit, regex_pars = c(&quot;mu&quot;, &quot;tau2&quot;, &quot;sigma2&quot;, &quot;phi&quot;)) ## Warning: `expand_scale()` is deprecated; use `expansion()` ## instead. mcmc_acf(fit, regex_pars = c(&quot;mu&quot;, &quot;tau2&quot;, &quot;sigma2&quot;, &quot;phi&quot;)) ## Inference for Stan model: spatial-car. ## 4 chains, each with iter=5000; warmup=2500; thin=1; ## post-warmup draws per chain=2500, total post-warmup draws=10000. ## ## mean se_mean sd 10% 90% n_eff Rhat ## mu -0.34 0.01 0.15 -0.52 -0.16 350 1.00 ## tau2 2.14 0.07 0.76 1.06 3.07 104 1.05 ## sigma2 0.42 0.03 0.22 0.13 0.72 59 1.10 ## phi 0.58 0.01 0.22 0.27 0.85 919 1.01 ## lp__ 30.36 8.78 54.35 -18.03 114.13 38 1.16 ## ## Samples were drawn using NUTS(diag_e) at Tue Apr 7 20:56:24 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). plot fitted vs. simulated spatial random effect eta_post &lt;- extract(fit)$eta plot(apply(eta_post, 2, mean), eta) abline(0, 1, col = &quot;red&quot;) looks like there might be a mean shift (non-identifiability) mu_post &lt;- extract(fit)$mu plot(mean(mu_post) + apply(eta_post, 2, mean), eta) abline(0, 1, col = &quot;red&quot;) mean(eta) ## [1] -0.4367 Note: the above model is inefficient as it doesn’t leverage sparse matrix operations. More complicated (and efficient) code is available here Example where the CAR parameter \\(\\phi = 1\\) is available here 30.3.2 Simultaneous autoregressive (SAR) model \\[\\begin{align*} y_i &amp; = \\phi \\frac{1}{d_i} \\sum_{j \\in N_i} y_j + \\varepsilon_i \\end{align*}\\] where \\(\\varepsilon_i \\stackrel{iid}{\\sim} N(0, \\sigma^2)\\) Note: this is not a conditional distribution like the CAR model – this gives the definition of a joint distribution Some algebra gives \\[\\begin{align*} &amp; \\mathbf{y} = \\rho \\mathbf{A} \\mathbf{D}^{-1} \\mathbf{y} + \\boldsymbol{\\varepsilon} \\\\ \\Rightarrow &amp; \\left(\\mathbf{I} - \\rho \\mathbf{A} \\mathbf{D}^{-1} \\right) \\mathbf{y} = \\boldsymbol{\\varepsilon} \\\\ \\Rightarrow &amp; \\mathbf{y} = \\left(\\mathbf{I} - \\rho \\mathbf{A} \\mathbf{D}^{-1} \\right)^{-1} \\boldsymbol{\\varepsilon} \\\\ \\end{align*}\\] which implies the joint distribution \\[\\begin{align*} \\mathbf{y} &amp; \\sim \\operatorname{N} \\left(\\mathbf{0}, \\left(\\mathbf{I} - \\rho \\mathbf{A} \\mathbf{D}^{-1} \\right)^{-1}\\left(\\mathbf{I} - \\rho \\mathbf{A} \\mathbf{D}^{-1} \\right)^{-1} \\right) \\end{align*}\\] which has a similar form to the SPDE models Note: \\(\\phi\\) is not a correlation! However, correlation increases as \\(\\phi\\) increases typicallly, we assume \\(\\phi\\) will be large A close look at the spatial structure implied by the CAR and SAR models if \\(\\phi = 0\\), then the observations are spatially independent n &lt;- 10^2 phi &lt;- 0.4 ## construct a sparse matrix adjacency matrix for a 2-d lattice A &lt;- igraph::as_adjacency_matrix(make_lattice(length = sqrt(n), dim = 2), sparse = FALSE) D &lt;- diag(rowSums(A)) Sigma &lt;- solve(diag(n) - phi * A %*% solve(D)) %*% t(solve(diag(n) - phi * A %*% solve(D))) ## calculate the correlation from a first order SAR cov2cor(Sigma)[5, 6] ## do this as a loop n_phi &lt;- 500 phi &lt;- seq(-1, 1, length.out = n_phi)[-c(1, n_phi)] rho &lt;- rep(0, n_phi - 2) A &lt;- igraph::as_adjacency_matrix(make_lattice(length = sqrt(n), dim = 2), sparse = FALSE) D &lt;- diag(rowSums(A)) for (i in 1:(n_phi-2)) { Sigma &lt;- solve(diag(n) - phi[i] * A %*% solve(D)) %*% t(solve(diag(n) - phi[i] * A %*% solve(D))) ## calculate the correlation from a first order CAR rho[i] &lt;- cov2cor(Sigma)[5, 6] } data.frame(phi = phi, rho = rho) %&gt;% ggplot(aes(x = phi, y = rho)) + geom_line() + ylim(c(-1, 1)) + ylab(&quot;correlation&quot;) + ggtitle(&quot;SAR correlation (lag-1) vs. phi parameter&quot;) + geom_abline(slope = 1, intercept = 0, color = &quot;red&quot;) ## [1] 0.2755 Plot how the \\(\\phi\\) parameter determines the correlation ## simulate some data simulate_data &lt;- function(n, phi) { locs &lt;- expand.grid( seq(0, 1, length.out = sqrt(n)), seq(0, 1, length.out = sqrt(n)) ) # plot(make_lattice(length = sqrt(n), dim = 2)) A &lt;- make_lattice(length = sqrt(n), dim = 2) %&gt;% as_adjacency_matrix(sparse = TRUE) D &lt;- Diagonal(x = colSums(A)) Q &lt;- (diag(n) - phi * A %*% solve(D)) %*% t(diag(n) - phi * A %*% solve(D)) ## cholesky (this is a sparse matrix that is upper diagonal) R &lt;- chol(Q) y &lt;- backsolve(R, rnorm(n)) ## generate the plot output &lt;- data.frame(x = locs[, 1], y = locs[, 2], z = y) %&gt;% ggplot(aes(x = x, y = y, fill = z)) + geom_raster() + scale_fill_viridis() + ggtitle(paste(&quot;Phi = &quot;, phi)) return(output) } p1 &lt;- simulate_data(n = 50^2, phi = 0.999) p2 &lt;- simulate_data(n = 50^2, phi = 0.9) p3 &lt;- simulate_data(n = 50^2, phi = 0.8) p4 &lt;- simulate_data(n = 50^2, phi = - 0.8) p5 &lt;- simulate_data(n = 50^2, phi = -0.9) p6 &lt;- simulate_data(n = 50^2, phi = - 0.999) (p1 | p2 | p3) / (p4 | p5 | p6) 30.3.2.1 Fitting a SAR model in stan ## simulate data for fitting the model n &lt;- 10^2 phi &lt;- 0.9 sigma2 &lt;- 0.5 tau2 &lt;- 2 locs &lt;- expand.grid( seq(0, 1, length.out = sqrt(n)), seq(0, 1, length.out = sqrt(n)) ) # plot(make_lattice(length = sqrt(n), dim = 2)) A &lt;- make_lattice(length = sqrt(n), dim = 2) %&gt;% as_adjacency_matrix(sparse = TRUE) D &lt;- Diagonal(x = colSums(A)) Q &lt;- (diag(n) - phi * A %*% D) %*% t(diag(n) - phi * A %*% D) / tau2 ## cholesky (this is a sparse matrix that is upper diagonal) R &lt;- chol(Q) eta &lt;- backsolve(R, rnorm(n)) y &lt;- eta + rnorm(n, 0, sqrt(sigma2)) p1 &lt;- data.frame(x = locs[, 1], y = locs[, 2], z = eta) %&gt;% ggplot(aes(x = x, y = y, fill = z)) + geom_raster() + scale_fill_viridis() + ggtitle(&quot;Spatial random effect&quot;) p2 &lt;- data.frame(x = locs[, 1], y = locs[, 2], z = y) %&gt;% ggplot(aes(x = x, y = y, fill = z)) + geom_raster() + scale_fill_viridis() + ggtitle(&quot;Observed data&quot;) p1 + p2 we need to define a stan model print the output of the model spatial-sar.stan cat(read_lines(here::here(&quot;stan_models&quot;, &quot;spatial-sar.stan&quot;)), sep = &quot;\\n&quot;) data { int&lt;lower=0&gt; N; vector[N] y; matrix[N, N] A; matrix[N, N] D; } transformed data { matrix[N, N] I = diag_matrix(rep_vector(1, N)); matrix[N, N] A_D_inv; A_D_inv = A * inverse(D); } parameters { vector[N] eta; real mu; real&lt;lower=0&gt; sigma2; real&lt;lower=0&gt; tau2; real&lt;lower=0,upper=1&gt; phi; } transformed parameters { vector[N] y_pred = mu + eta; } model { matrix[N,N] C = I - phi * A_D_inv; matrix[N,N] Sigma_inv = C * C&#39; / tau2; eta ~ multi_normal_prec(rep_vector(0, N), Sigma_inv); mu ~ normal(0, 1); sigma2 ~ cauchy(0, 5); tau2 ~ cauchy(0, 5); y ~ normal(mu + eta, sigma2); } Fitting the model note: this is very slow for even moderate sized data fit &lt;- stan( file = here::here(&quot;stan_models&quot;, &quot;spatial-sar.stan&quot;), ## note: convert from a sparse matrix format to regular matrix ## for fitting in stan data = list(y = y, N = n, D = as.matrix(D), A = as.matrix(A)), iter = 5000 ) ## Warning: There were 12 divergent transitions after warmup. Increasing adapt_delta above 0.8 may help. See ## http://mc-stan.org/misc/warnings.html#divergent-transitions-after-warmup ## Warning: There were 4 chains where the estimated Bayesian Fraction of Missing Information was low. See ## http://mc-stan.org/misc/warnings.html#bfmi-low ## Warning: Examine the pairs() plot to diagnose sampling problems ## Warning: The largest R-hat is 1.13, indicating chains have not mixed. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#r-hat ## Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#bulk-ess ## Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. ## Running the chains for more iterations may help. See ## http://mc-stan.org/misc/warnings.html#tail-ess Note: this is giving warinings – it’s best to pay attention to these in stan print(fit, probs = c(0.1, 0.9), pars = c(&quot;mu&quot;, &quot;tau2&quot;, &quot;sigma2&quot;, &quot;phi&quot;, &quot;lp__&quot;)) mcmc_trace(fit, regex_pars = c(&quot;mu&quot;, &quot;tau2&quot;, &quot;sigma2&quot;, &quot;phi&quot;)) mcmc_areas(fit, regex_pars = c(&quot;mu&quot;, &quot;tau2&quot;, &quot;sigma2&quot;, &quot;phi&quot;)) ## Warning: `expand_scale()` is deprecated; use `expansion()` ## instead. mcmc_acf(fit, regex_pars = c(&quot;mu&quot;, &quot;tau2&quot;, &quot;sigma2&quot;, &quot;phi&quot;)) ## Inference for Stan model: spatial-sar. ## 4 chains, each with iter=5000; warmup=2500; thin=1; ## post-warmup draws per chain=2500, total post-warmup draws=10000. ## ## mean se_mean sd 10% 90% n_eff Rhat ## mu 0.07 0.01 0.20 -0.16 0.32 829 1.00 ## tau2 1.72 0.15 0.99 0.25 2.92 46 1.09 ## sigma2 0.95 0.09 0.51 0.23 1.61 35 1.13 ## phi 0.21 0.01 0.15 0.04 0.42 517 1.01 ## lp__ -84.45 8.55 57.41 -139.39 5.26 45 1.09 ## ## Samples were drawn using NUTS(diag_e) at Tue Apr 7 21:02:52 2020. ## For each parameter, n_eff is a crude measure of effective sample size, ## and Rhat is the potential scale reduction factor on split chains (at ## convergence, Rhat=1). plot fitted vs. simulated spatial random effect eta_post &lt;- extract(fit)$eta plot(apply(eta_post, 2, mean), eta) abline(0, 1, col = &quot;red&quot;) More on CAR and SAR models is available here ### Theoretical Results for more general model classes In general, the Hammersley-Clifford Theorem can be used to show that a set of conditional probability distributions specifies a compatible joint probability distribution If you start with compatible conditional probability distributions, Brooks’ Lemma is a formula for computing the joint distribution random variables \\(\\mathcal{X} = \\{ X_v \\}_{v \\in \\mathcal{V}}\\) form a Markov random field with respect to \\(\\mathcal{G}\\) if they satisfy - Pairwise Markov property: "],
["day-31.html", "31 Day 31 31.1 Announcements 31.2 Change of Spatial Support", " 31 Day 31 31.1 Announcements 31.2 Change of Spatial Support Also called the ecological fallcy Often data occur in continuous space, but are summarized in higher order summaries example: lightning strikes over a landscape occur in continuous space, could summarize these to strikes per county. example: temperature occurs at every point, global climate models operate on gridcells (spatial averages) draw picture sometimes data occur over different scales socio-economic and other data are collected by the US Census on the block, block-group, and tract scale infection data (coronavirus, flu) is often reported at the county level need to reconcile data that occur over different scales (census tract vs. county) and may be potentially mis-aligned Example: Gerrymandering Interactive Gerrymandering map details on the mapping "],
["day-32.html", "32 Day 32 32.1 Announcements 32.2 Generalized (non-Gaussian) spatial models", " 32 Day 32 32.1 Announcements 32.2 Generalized (non-Gaussian) spatial models "],
["day-33.html", "33 Day 33 33.1 Announcements 33.2 Spatio-temporal data", " 33 Day 33 33.1 Announcements 33.2 Spatio-temporal data Let \\(y(\\mathbf{s}, t)\\) be an observation at location \\(\\mathbf{s} \\in \\mathcal{D}\\) for time \\(t \\in \\mathcal{T}\\) the \\(\\mathbf{y} = (y(\\mathbf{s}_1, t_1), \\ldots, y(\\mathbf{s}_1, t_m), \\ldots, y(\\mathbf{s}_n, t_1), \\ldots, y(\\mathbf{s}_n, t_m))&#39; \\sim N(\\mathbf{X}\\boldsymbol{\\beta}, \\boldsymbol{\\Sigma})\\) In general, \\(\\mathcal{D}\\) and \\(\\mathcal{T}\\) could be either discrete or continuous. The same old story – It’s really hard to specify a positive-definite covariance function in space-time without making some assumptions "],
["day-34.html", "34 Day 34 34.1 Announcements", " 34 Day 34 34.1 Announcements "],
["day-35.html", "35 Day 35 35.1 Announcements", " 35 Day 35 35.1 Announcements "],
["day-36.html", "36 Day 36 36.1 Announcements", " 36 Day 36 36.1 Announcements "],
["day-37.html", "37 Day 37 37.1 Announcements", " 37 Day 37 37.1 Announcements "],
["day-38.html", "38 Day 38 38.1 Announcements", " 38 Day 38 38.1 Announcements "],
["day-39.html", "39 Day 39 39.1 Announcements", " 39 Day 39 39.1 Announcements "],
["day-40.html", "40 Day 40 40.1 Announcements", " 40 Day 40 40.1 Announcements "],
["day-41.html", "41 Day 41 41.1 Announcements", " 41 Day 41 41.1 Announcements "],
["day-42.html", "42 Day 42 42.1 Announcements", " 42 Day 42 42.1 Announcements "],
["day-43.html", "43 Day 43 43.1 Announcements", " 43 Day 43 43.1 Announcements "],
["references.html", "References", " References "]
]
